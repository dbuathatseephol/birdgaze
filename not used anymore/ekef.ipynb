{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIX_TSV = \"fix_testing.tsv\"\n",
    "TRIAL_TSV_1 = \"trialdata_testing_firstdisplayonly.tsv\"\n",
    "TRIAL_TSV_2 = \"trialdata_testing_seconddisplayonly.tsv\"\n",
    "PAIR_SELECTION = \"top_bottom_3_pairs_per_viewing.csv\"\n",
    "\n",
    "IMAGE_ROOT = \"Testing/testing_images\"\n",
    "OUTDIR = \"figure_b/qualitative_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc39356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading TSVs and rendering (FIXED version)…\n",
      "Rendering 72 pairs (sorted by cross-view fixation difference)…\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_001_609dc852d3214d75a3620ce4f02b0c9f__586e715638b04dc3a559c4ccc9b8abfa_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_002_552e4370bcaa4f4e89777ace9335d029__483fb8e0c1804956945f7dea8c54c1d2_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_003_6f50c2b038564cd19a0873bf396f1549__0c5a770e3f57463f84a9897407744692_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_004_5d8146740e0944548a526c666d002462__50bfbc370d1a41ee973ee21b33616e46_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_005_ad935455eb5f48a39c5f22eadb88a266__a961d9f2d5b94de584d705197202e8f2_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_006_c53935d4c6ce4db892ea81327e9cda0d__c5501af926ff4037b1a8d413fe67d89b_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_007_55f57e72bc294394b58f3a6e8ae792f6__52adf4992f234d47b56ff89c2cc055b7_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_008_c25e54bf6c9b4fe6be877578e6efd0a9__c4a8633be9c84993b5280b95c1195940_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_009_51fa967738384379b5637ecbfd8e0f8a__50e5fe33452444d58d5744ce8a95ed36_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_010_596853509efd4aa98591a034dbefb8b5__994060a5aa3f4f62a53be760e14f4ec7_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_011_9cab7bdcdca54367b3a074557d1d2410__34ad3c12373b45858d933969f9e726ac_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_012_4efa4de50430421885c96dbbc2bc3beb__804f479253e04e149591f7268bd6ffd5_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_013_7def261cee25419484ca00c98b82b6da__2692bf1be81349d8a74d7144999db852_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_014_15ef3e209a84417282278db5a1e3b412__0e68f17fae9949888cd2017b9b5a6754_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_015_29a6b28353c14c38b7dd2edc13c86a0b__2c4e653610eb497da50f7420cff5c3a2_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_016_6b94d252b54242748bfb2c611bd3368f__01f6d2022368434aacc21db2a6ba4fc9_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_017_16a382f4e5034c13b076392b53fd53a7__53d84ab5255f41ada85a51206648ad05_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_018_5f9ebd55e0b74c9cae01a255bad5ef27__33aae810e78046d2bb367344aeca8294_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_019_7cc37f0472f748599a44d47770b24467__2ce212ba33494a75a00b43500a31c475_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_020_1098187526484c62b20eb0f59faec6ef__0693816789364e83bc559e435816ec8f_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_021_8b20a875626e420fa0abf64466663c6e__0f4c34f941f243c5b5f19c25924a9202_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_022_b5350d028ab8412c913e07306b132aba__b198dffa9f9f4b93adafd24c1e4393a6_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_023_14c8f9858abc47e5b9a2f1e9c3ac2873__3e5475b4d63c4c01b7558818565a0784_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_024_705ae58bb7954d1ab4469ad98c10c380__628cbe14859049deb43a077a00f24e00_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_025_19fdd1aaa7244cc696ea94f64d80469d__57f6a02ebfaa4340af2adf4d066b0a6f_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_026_6f6a6066bee642aab2bc7f77c1fcccfc__2c9f67a45a7640b0b73200a1e2cea5b1_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_027_62e268e032da4efe8a767216a5a830d3__57ad557d462048a9aef0dcb67348e305_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_028_53b27750ff6b49b6bbcfbc312f8e5550__8f6a40194ecd4bdf9275a6c852de91a6_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_029_78765bd55e9a4ec7a9c5f5590a36b386__07150a02be164be4bd0e325cb2428430_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_030_573e9073f3ed44b094a3ffbbe8fed4db__571b534494664f83b7f796bde052bf49_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_031_8c2fde1879fd4c52adaacb2c10ab7cd5__2e6d7983fc32450c8b08a6d33217c889_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_032_8f8709d10e3b4a7d9132023c629b81f7__5592a6d794c24c73849b83794d5b6a4e_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_033_85ff0af221644b2db46d29632a358004__63e76187253f4a9f939f82e895fd5179_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_034_d2e3309962c645d1876005afbfc4d2ab__ce79ad2e750c46c3b2495c431fa63932_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_035_5ec0a218f01048b2b25396013c727f77__21dce860b4934055a44d34bfcea1ae4d_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_036_f39e8c35b562470eb229890c2c547210__e00efbb3124b43f684903b0230e16252_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_037_50ded7f0f1e449768df0d0de67569db6__49b0a237c6354943a210232b051cd486_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_038_15ae1a8affa04c35afde69fed22060b2__3f42b0faebd6452f96de9606b432ae11_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_039_21a57415248c4e4880835e441d6e00fd__1e014f55dac948ccb2e9053666e123b4_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_040_7f5e4bf12f9f4e159081162cf88a7fb7__5ee126f8ae6640758d5481e2689bdb81_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_041_d392002b67b041739cb8399b1a31488a__d91c5088371c4b0dab529bd3cb8083b1_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_042_dbc7fc54be7544718e0c9964f23f3005__dba5f9612a5644af81dea179dc1880aa_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_043_374cad12d807421b9e8295785d6bc44f__0260cc5cc34344afb50cfaeda417d65b_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_044_7e996257944a4eeda68a18d167963086__5dd0a3014e9849d690ad2d3572c3688d_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_045_3beb9f613c0e4e6c9fc926fa7885e8be__482b46b2a93947fda6c76da78d560578_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_046_7cbde8f25a374d519a8f44e9a94320f2__991ef840427e44b28485942a98ebb8d3_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_047_495cce2d2d5c4495bd7c9e6d830478f3__87c1a1bec35c47e089bac56e8131c919_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_048_002fac9e85104f41a7843820e9e87ee3__30e447f9321a4c74880915cc77f54950_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_049_740455db60c2427c95bed843f977d956__341480a2e5364dcd887d2fae629a5fa3_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_050_c5259043d52a4261b7b61362955ffda2__c797181a7ff0412cbdcf1a116f8b784a_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_051_0e68f17fae9949888cd2017b9b5a6754__1e014f55dac948ccb2e9053666e123b4_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_052_b8a9bb6df11f42ceb2b99b77c6042903__b5a412d4d3fd487ab2df85a841e15426_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_053_6f7c91efa2704bf49aab2ee6b564de26__33faf20644da4ac7a8c8826d8c3fc1b6_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_054_11b33ba71fc34d88b6a2fd4183ec3cea__1a8b5b4beb2844bfa7d6d7dd163a829f_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_055_74dc89c8de294e968df1fb3942338d1d__70f5a78a800c4d20b1b165a0b9ee4199_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_056_8a2d4ef79ab9493980b74219f92de94b__7bea9ca3f4854ce7911b3ed001e45ea2_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_057_2c4e653610eb497da50f7420cff5c3a2__3e263a25c12942b6a691c1e3f8b8c0d0_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_058_51d069ec5b064608ac23eaf223e1dcaa__8e11a5ad9cec4032bba9fe67be646e78_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_059_1549b032d34945b18a18aad52760ffd3__1496f9a5325d4793806a8c7029873437_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_060_8f37b5b2dd8845908ba518dabf9a181a__3e263a25c12942b6a691c1e3f8b8c0d0_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_061_2246c9472ff54add81dbb8b63b1531b5__1711eb0224a6428e928a21cc682463dd_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_062_35a74db7048c4de29b3c7ee6ae50a713__31d3f5bbd9d049bda8388be302ce4f89_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_063_85d267dfb76a48de97dc7157922b90f8__80bcb40a02e143a4aa8bd0d63240b954_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_064_f87e2a1fb05f440aa7f9746eec6c6289__f62a9d89afec4d79992d7ebe886b28f6_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_065_c76dd3c76e404ba6bd604487fc6450c5__ae0f35add10f4d9d80bbb8a553358e38_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_066_fec5ae6f72f445ea97f69ec64979ab06__fcd5dbb7acee4019a91dbc33d573c3cf_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_067_7287dfd9e4a7453b85dfccc581d234bf__5899d895f4aa4591a07d4b64a746e8b8_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_068_87ed7bc29ec646f0af4de81ded2685e3__79aae83c4ac946dc99cd85b6c84df966_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_069_62ebab2acc714ab082cf9c991a0f2698__9edd384ee5394760be897cdbef0c8e87_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_070_3c5319f397d94ec7aafaf2c06ef19a84__777bd2d5894e4d05848bfe271c70348a_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_071_3c08f572aa264552aeff7ed80dc13c80__36b578d3f4c94029af93664112058d1a_by_view_TSV.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/by_pair_TSV_all_fixed/pair_072_28ae8fcc3c214dc5b785225113b5d5ed__15dcb6731dda4a3fb0996108c8ef8d50_by_view_TSV.pdf\n",
      "\n",
      "Debugging Info - Sample fixation counts:\n",
      "\n",
      "Pair: 002fac9e85104f41a7843820e9e87ee3 | 30e447f9321a4c74880915cc77f54950\n",
      "  full: First=142 fix, Second=40 fix\n",
      "  central: First=133 fix, Second=67 fix\n",
      "  peripheral: First=182 fix, Second=54 fix\n",
      "\n",
      "Pair: 0e68f17fae9949888cd2017b9b5a6754 | 1e014f55dac948ccb2e9053666e123b4\n",
      "  full: First=149 fix, Second=45 fix\n",
      "  central: First=131 fix, Second=65 fix\n",
      "  peripheral: First=169 fix, Second=72 fix\n",
      "\n",
      "Pair: 1098187526484c62b20eb0f59faec6ef | 0693816789364e83bc559e435816ec8f\n",
      "  full: First=122 fix, Second=43 fix\n",
      "  central: First=173 fix, Second=70 fix\n",
      "  peripheral: First=151 fix, Second=54 fix\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "DATA_DIR = Path(\"data files\")           # <- your TSVs live here (per screenshots)\n",
    "IMG_DIR  = Path(\"Testing/testing_images\")\n",
    "OUT_DIR  = Path(\"figure_b/qualitative_analysis/by_pair_TSV_all_fixed\")\n",
    "\n",
    "SCREEN_SIZE = (1024, 768)\n",
    "BOX = (SCREEN_SIZE[0]//2-310, SCREEN_SIZE[1]//2-310,\n",
    "       SCREEN_SIZE[0]//2+310, SCREEN_SIZE[1]//2+310)  # l,t,r,b\n",
    "\n",
    "# File names are a bit inconsistent across runs; handle both spellings.\n",
    "TR_FIRST_CANDIDATES  = [\"trialdata_testing_firstdisplayonly.tsv\",  \"trial_data_testing_firstdisplayonly.tsv\"]\n",
    "TR_SECOND_CANDIDATES = [\"trialdata_testing_seconddisplayonly.tsv\", \"trial_data_testing_seconddisplayonly.tsv\"]\n",
    "FIX_FILE             = \"fix_testing.tsv\"\n",
    "\n",
    "# Robust subject id keys\n",
    "SUBJECT_KEYS = [\n",
    "    \"Session_Name_\", \"RECORDING_SESSION_LABEL\", \"subject\", \"subject_id\",\n",
    "    \"participant\", \"worker_id\", \"uid\", \"id\"\n",
    "]\n",
    "\n",
    "\n",
    "# =================== I/O HELPERS ===================\n",
    "def _read_tsv(path: Path) -> pd.DataFrame:\n",
    "    # Most of your TSVs are UTF-16, but some exports come UTF-8. Try both.\n",
    "    for enc in (\"utf-16\", \"utf-8\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=\"\\t\", encoding=enc, engine=\"python\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # final attempt: default pandas guess\n",
    "    return pd.read_csv(path, sep=\"\\t\", engine=\"python\")\n",
    "\n",
    "\n",
    "def _find_first_existing(base: Path, candidates: List[str]) -> Path:\n",
    "    for name in candidates:\n",
    "        p = base / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"None of {candidates} found under {base}\")\n",
    "\n",
    "\n",
    "def _norm_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Standardize the columns we rely on\n",
    "    ren = {}\n",
    "\n",
    "    # session id\n",
    "    if \"Session_Name_\" in df.columns:\n",
    "        pass\n",
    "    elif \"RECORDING_SESSION_LABEL\" in df.columns:\n",
    "        ren[\"RECORDING_SESSION_LABEL\"] = \"Session_Name_\"\n",
    "\n",
    "    # trial index\n",
    "    for cand in [\"Trial_Index_\", \"TRIAL_INDEX\", \"Trial_Index\", \"INDEX\"]:\n",
    "        if cand in df.columns:\n",
    "            ren[cand] = \"Trial_Index_\"\n",
    "            break\n",
    "\n",
    "    # fix index and coords (fix file only)\n",
    "    if \"CURRENT_FIX_INDEX\" in df.columns:\n",
    "        ren[\"CURRENT_FIX_INDEX\"] = \"fix_index\"\n",
    "    if \"CURRENT_FIX_X\" in df.columns:\n",
    "        ren[\"CURRENT_FIX_X\"] = \"fix_x\"\n",
    "    if \"CURRENT_FIX_Y\" in df.columns:\n",
    "        ren[\"CURRENT_FIX_Y\"] = \"fix_y\"\n",
    "\n",
    "    # filenames / meta\n",
    "    for k in [\"filename1\", \"filename2\", \"viewing\", \"ACC\", \"RESPONSE\",\n",
    "              \"FIXATION_COUNT\", \"correct_response\"]:\n",
    "        if k in df.columns:\n",
    "            continue\n",
    "        # try common variants (none needed right now beyond above)\n",
    "\n",
    "    df = df.rename(columns=ren)\n",
    "\n",
    "    # strong types\n",
    "    if \"Trial_Index_\" in df.columns:\n",
    "        df[\"Trial_Index_\"] = pd.to_numeric(df[\"Trial_Index_\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"fix_index\" in df.columns:\n",
    "        df[\"fix_index\"] = pd.to_numeric(df[\"fix_index\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _load_trials() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    p_first  = _find_first_existing(DATA_DIR, TR_FIRST_CANDIDATES)\n",
    "    p_second = _find_first_existing(DATA_DIR, TR_SECOND_CANDIDATES)\n",
    "    tr_first  = _norm_columns(_read_tsv(p_first))\n",
    "    tr_second = _norm_columns(_read_tsv(p_second))\n",
    "    return tr_first, tr_second\n",
    "\n",
    "\n",
    "def _load_fix() -> pd.DataFrame:\n",
    "    p_fix = DATA_DIR / FIX_FILE\n",
    "    if not p_fix.exists():\n",
    "        raise FileNotFoundError(p_fix)\n",
    "    return _norm_columns(_read_tsv(p_fix))\n",
    "\n",
    "\n",
    "def _subject_id(row: pd.Series) -> str:\n",
    "    for k in SUBJECT_KEYS:\n",
    "        if k in row and pd.notna(row[k]):\n",
    "            return str(row[k])\n",
    "    return \"?\"\n",
    "\n",
    "\n",
    "# =================== CORE LOGIC ===================\n",
    "def _unique_subjects(df: pd.DataFrame) -> int:\n",
    "    if df.empty: return 0\n",
    "    subj = df.apply(_subject_id, axis=1)\n",
    "    return subj.nunique()\n",
    "\n",
    "\n",
    "def _trial_label(df: pd.DataFrame) -> str:\n",
    "    if df.empty: return \"trials: n/a\"\n",
    "    nums = sorted(int(x) for x in df[\"Trial_Index_\"].dropna().unique())\n",
    "    if not nums: return \"trials: n/a\"\n",
    "    if len(nums) <= 8:\n",
    "        return \"trials \" + \", \".join(map(str, nums))\n",
    "    return f\"trials {', '.join(map(str, nums[:8]))} …\"\n",
    "\n",
    "\n",
    "def _group_trials(tr_first: pd.DataFrame, tr_second: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns dict keyed by (filename1, filename2) -> {view -> dict(first_df, second_df, acc, right, wrong)}\n",
    "    \"\"\"\n",
    "    out: Dict[Tuple[str, str], Dict[str, Dict[str, pd.DataFrame]]] = {}\n",
    "\n",
    "    # Keep only the fields we need\n",
    "    keep_cols = [\"Session_Name_\", \"Trial_Index_\", \"filename1\", \"filename2\",\n",
    "                 \"viewing\", \"ACC\", \"RESPONSE\", \"correct_response\", \"FIXATION_COUNT\"]\n",
    "    t1 = tr_first.copy()\n",
    "    t2 = tr_second.copy()\n",
    "    for t in (t1, t2):\n",
    "        for k in keep_cols:\n",
    "            if k not in t.columns:\n",
    "                t[k] = np.nan\n",
    "\n",
    "    # Pair id is strictly the two filenames\n",
    "    t1[\"pair\"] = list(zip(t1[\"filename1\"], t1[\"filename2\"]))\n",
    "    t2[\"pair\"] = list(zip(t2[\"filename1\"], t2[\"filename2\"]))\n",
    "\n",
    "    for pair, df_first in t1.groupby(\"pair\"):\n",
    "        df_second = t2[t2[\"pair\"] == pair]\n",
    "\n",
    "        views = {}\n",
    "        for view in [\"full\", \"central\", \"peripheral\"]:\n",
    "            f_view = df_first[df_first[\"viewing\"].astype(str).str.lower() == view]\n",
    "            s_view = df_second[df_second[\"viewing\"].astype(str).str.lower() == view]\n",
    "            if f_view.empty or s_view.empty:\n",
    "                continue\n",
    "\n",
    "            # acc/right/wrong computed from *first* display rows of that view\n",
    "            acc_flags = []\n",
    "            for _, r in f_view.iterrows():\n",
    "                acc = r.get(\"ACC\")\n",
    "                if pd.isna(acc):\n",
    "                    # fall back to RESPONSE ?= correct_response\n",
    "                    resp = str(r.get(\"RESPONSE\", \"\")).strip().lower()\n",
    "                    cor  = str(r.get(\"correct_response\", \"\")).strip().lower()\n",
    "                    if resp and cor:\n",
    "                        acc_flags.append(int(resp == cor))\n",
    "                else:\n",
    "                    try:\n",
    "                        acc_flags.append(int(bool(int(acc))))\n",
    "                    except Exception:\n",
    "                        acc_flags.append(int(bool(acc)))\n",
    "            if acc_flags:\n",
    "                acc = float(np.mean(acc_flags))\n",
    "                right = int(np.sum(acc_flags))\n",
    "                wrong = int(len(acc_flags) - right)\n",
    "            else:\n",
    "                acc = 0.0; right = 0; wrong = 0\n",
    "\n",
    "            views[view] = {\n",
    "                \"first_df\":  f_view.copy(),\n",
    "                \"second_df\": s_view.copy(),\n",
    "                \"acc\":       acc,\n",
    "                \"right\":     right,\n",
    "                \"wrong\":     wrong\n",
    "            }\n",
    "        if views:\n",
    "            out[pair] = views\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _collect_fixations_for_image(\n",
    "    fix: pd.DataFrame,\n",
    "    trials: pd.DataFrame,\n",
    "    which: str,  # \"first\" or \"second\"\n",
    "    first_trials_df: pd.DataFrame = None  # Pass the first display trials to get FIXATION_COUNT\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return only the fixations that occurred while the requested image was on-screen.\n",
    "    \n",
    "    For dual-image trials, fixations are numbered continuously:\n",
    "      - first image:  fix_index 0 to (FIXATION_COUNT-1)  \n",
    "      - second image: fix_index FIXATION_COUNT onwards\n",
    "    \n",
    "    FIXATION_COUNT comes from the first display TSV row for each trial.\n",
    "    \"\"\"\n",
    "    if trials.empty:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    l, t, r, b = BOX\n",
    "    xs_all, ys_all = [], []\n",
    "\n",
    "    # For each trial row, gather its fixations\n",
    "    for _, tr in trials.iterrows():\n",
    "        sess = tr[\"Session_Name_\"]\n",
    "        tidx = tr[\"Trial_Index_\"]\n",
    "        if pd.isna(sess) or pd.isna(tidx):\n",
    "            continue\n",
    "\n",
    "        # Get all fixations for this session/trial\n",
    "        fx = fix[(fix[\"Session_Name_\"] == sess) & (fix[\"Trial_Index_\"] == tidx)]\n",
    "        if fx.empty:\n",
    "            continue\n",
    "        \n",
    "        # Sort fixations by index to ensure proper ordering\n",
    "        fx = fx.sort_values('fix_index')\n",
    "        \n",
    "        # Get FIXATION_COUNT from the first display data for this trial\n",
    "        boundary = None\n",
    "        if which == \"first\":\n",
    "            # For first display, use FIXATION_COUNT directly from current row\n",
    "            if \"FIXATION_COUNT\" in tr and pd.notna(tr[\"FIXATION_COUNT\"]):\n",
    "                try:\n",
    "                    boundary = int(tr[\"FIXATION_COUNT\"])\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            # For second display, need to look up FIXATION_COUNT from first display data\n",
    "            if first_trials_df is not None:\n",
    "                first_row = first_trials_df[\n",
    "                    (first_trials_df[\"Session_Name_\"] == sess) & \n",
    "                    (first_trials_df[\"Trial_Index_\"] == tidx)\n",
    "                ]\n",
    "                if not first_row.empty and \"FIXATION_COUNT\" in first_row.columns:\n",
    "                    try:\n",
    "                        boundary = int(first_row.iloc[0][\"FIXATION_COUNT\"])\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # If we couldn't find boundary, try to infer it\n",
    "        if boundary is None or pd.isna(boundary):\n",
    "            # Use a heuristic: assume roughly equal fixations for each image\n",
    "            max_fix = fx[\"fix_index\"].max()\n",
    "            if pd.notna(max_fix):\n",
    "                boundary = int(max_fix / 2) + 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Select appropriate fixations based on which image\n",
    "        if which == \"first\":\n",
    "            # First image: fixations 0 to boundary-1\n",
    "            sel = fx[\"fix_index\"] < boundary\n",
    "        else:\n",
    "            # Second image: fixations from boundary onwards  \n",
    "            sel = fx[\"fix_index\"] >= boundary\n",
    "\n",
    "        xs = fx.loc[sel, \"fix_x\"].astype(float).values\n",
    "        ys = fx.loc[sel, \"fix_y\"].astype(float).values\n",
    "\n",
    "        # Keep only points inside the 620x620 image box\n",
    "        m = (xs >= l) & (xs <= r) & (ys >= t) & (ys <= b)\n",
    "        xs_all.extend(xs[m])\n",
    "        ys_all.extend(ys[m])\n",
    "\n",
    "    return np.array(xs_all), np.array(ys_all)\n",
    "\n",
    "\n",
    "def _imshow_bg(ax, image_path: Path):\n",
    "    l, t, r, b = BOX\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    ax.imshow(img, extent=(l, r, b, t))\n",
    "\n",
    "\n",
    "def _draw_panel(ax, img_path: Path, xs: np.ndarray, ys: np.ndarray, title: str):\n",
    "    _imshow_bg(ax, img_path)\n",
    "    if xs.size:\n",
    "        ax.scatter(xs, ys, s=30, c=\"#FF6B6B\", alpha=0.5,\n",
    "                   edgecolor=\"white\", linewidth=0.8, zorder=5)\n",
    "    ax.set_xlim(0, SCREEN_SIZE[0]); ax.set_ylim(SCREEN_SIZE[1], 0)\n",
    "    ax.set_xlabel(\"x (screen px)\"); ax.set_ylabel(\"y (screen px)\")\n",
    "    ax.set_title(title, fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "# =================== RENDER ALL ===================\n",
    "def render_all_pairs():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tr_first, tr_second = _load_trials()\n",
    "    fix = _load_fix()\n",
    "\n",
    "    groups = _group_trials(tr_first, tr_second)\n",
    "    if not groups:\n",
    "        print(\"No groups were formed; check your TSVs and column names.\")\n",
    "        return\n",
    "\n",
    "    # rank pairs by how different the fixation counts are across views (first+second sum)\n",
    "    diffs: List[Tuple[float, Tuple[str, str]]] = []\n",
    "    for (f1, f2), views in groups.items():\n",
    "        counts = []\n",
    "        for view, vdict in views.items():\n",
    "            # IMPORTANT: Pass first_df to the second image collection\n",
    "            xs1, ys1 = _collect_fixations_for_image(fix, vdict[\"first_df\"],  which=\"first\", first_trials_df=None)\n",
    "            xs2, ys2 = _collect_fixations_for_image(fix, vdict[\"second_df\"], which=\"second\", first_trials_df=vdict[\"first_df\"])\n",
    "            counts.append(xs1.size + xs2.size)\n",
    "        if counts:\n",
    "            diffs.append((float(np.std(counts)), (f1, f2)))\n",
    "\n",
    "    diffs.sort(reverse=True, key=lambda x: x[0])\n",
    "    print(f\"Rendering {len(diffs)} pairs (sorted by cross-view fixation difference)…\")\n",
    "\n",
    "    for rank, ((f1, f2_std), (f1_name, f2_name)) in enumerate(zip(diffs, [p for _, p in diffs]), 1):\n",
    "        f1, f2 = f1_name, f2_name\n",
    "        views = groups[(f1, f2)]\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        fig.suptitle(f\"Same Pair Across Views\\nPair: {f1} | {f2}\",\n",
    "                     fontsize=16, fontweight=\"bold\", y=0.98)\n",
    "\n",
    "        def col_title(view: str, which: str, acc: float, right: int, wrong: int,\n",
    "                      trials_df: pd.DataFrame, n_fix: int) -> str:\n",
    "            subj_n = _unique_subjects(trials_df)\n",
    "            who = \"First\" if which == \"first\" else \"Second\"\n",
    "            return (f\"{view.capitalize()} Viewing • Acc: {acc*100:.1f}% • {who}: {n_fix} fix\\n\"\n",
    "                    f\"Subjects: right={right} • wrong={wrong} • N={subj_n} subjects\")\n",
    "\n",
    "        col = 0\n",
    "        for view in [\"full\", \"central\", \"peripheral\"]:\n",
    "            if view not in views:\n",
    "                # keep the layout consistent; draw empty axes with \"n/a\"\n",
    "                for row in [0, 1]:\n",
    "                    ax = plt.subplot(2, 3, col + 1 + row*3)\n",
    "                    ax.axis(\"off\")\n",
    "                    ax.set_title(f\"{view.capitalize()} Viewing • n/a\", fontsize=11, fontweight=\"bold\")\n",
    "                col += 1\n",
    "                continue\n",
    "\n",
    "            v = views[view]\n",
    "            first_df  = v[\"first_df\"]\n",
    "            second_df = v[\"second_df\"]\n",
    "\n",
    "            # IMPORTANT: Pass first_df when collecting second image fixations\n",
    "            xs1, ys1 = _collect_fixations_for_image(fix, first_df,  which=\"first\", first_trials_df=None)\n",
    "            xs2, ys2 = _collect_fixations_for_image(fix, second_df, which=\"second\", first_trials_df=first_df)\n",
    "\n",
    "            # panels\n",
    "            ax1 = plt.subplot(2, 3, col + 1)\n",
    "            title1 = col_title(view, \"first\", v[\"acc\"], v[\"right\"], v[\"wrong\"], first_df,  xs1.size)\n",
    "            _draw_panel(ax1, IMG_DIR / f1, xs1, ys1, title1)\n",
    "\n",
    "            ax2 = plt.subplot(2, 3, col + 4)\n",
    "            title2 = col_title(view, \"second\", v[\"acc\"], v[\"right\"], v[\"wrong\"], second_df, xs2.size)\n",
    "            _draw_panel(ax2, IMG_DIR / f2, xs2, ys2, title2)\n",
    "\n",
    "            col += 1\n",
    "\n",
    "        out = OUT_DIR / f\"pair_{rank:03d}_{Path(f1).stem}__{Path(f2).stem}_by_view_TSV.pdf\"\n",
    "        fig.tight_layout(rect=[0, 0.02, 1, 0.95])\n",
    "        fig.savefig(out, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"  ✓ Saved: {out}\")\n",
    "\n",
    "    # Also print some debugging info\n",
    "    print(\"\\nDebugging Info - Sample fixation counts:\")\n",
    "    sample_count = 0\n",
    "    for (f1, f2), views in groups.items():\n",
    "        if sample_count >= 3:\n",
    "            break\n",
    "        print(f\"\\nPair: {Path(f1).stem} | {Path(f2).stem}\")\n",
    "        for view, vdict in views.items():\n",
    "            xs1, ys1 = _collect_fixations_for_image(fix, vdict[\"first_df\"], which=\"first\", first_trials_df=None)\n",
    "            xs2, ys2 = _collect_fixations_for_image(fix, vdict[\"second_df\"], which=\"second\", first_trials_df=vdict[\"first_df\"])\n",
    "            print(f\"  {view}: First={xs1.size} fix, Second={xs2.size} fix\")\n",
    "        sample_count += 1\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Reading TSVs and rendering (FIXED version)…\")\n",
    "    render_all_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78d852f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PATTERN ANALYSIS - SIMPLE VERSION\n",
      "============================================================\n",
      "Loading data files...\n",
      "  Loading trialdata_testing_firstdisplayonly.tsv...\n",
      "    Success with utf-16 encoding\n",
      "  Loading trialdata_testing_seconddisplayonly.tsv...\n",
      "    Success with utf-16 encoding\n",
      "  Loading fix_testing.tsv...\n",
      "    Success with utf-16 encoding\n",
      "\n",
      "Data shapes:\n",
      "  First display: (2304, 45)\n",
      "  Second display: (2304, 45)\n",
      "  Fixations: (45717, 34)\n",
      "\n",
      "Processing full viewing...\n",
      "  Found 768 trials\n",
      "  Calculated metrics for 768 trials\n",
      "  Top: 660 trials, Bottom: 108 trials\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_simple/pattern_analysis_full.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_simple/pattern_analysis_full.png\n",
      "\n",
      "Processing central viewing...\n",
      "  Found 768 trials\n",
      "  Calculated metrics for 768 trials\n",
      "  Top: 618 trials, Bottom: 150 trials\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_simple/pattern_analysis_central.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_simple/pattern_analysis_central.png\n",
      "\n",
      "Processing peripheral viewing...\n",
      "  Found 768 trials\n",
      "  Calculated metrics for 768 trials\n",
      "  Top: 649 trials, Bottom: 119 trials\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_simple/pattern_analysis_peripheral.pdf\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_simple/pattern_analysis_peripheral.png\n",
      "\n",
      "============================================================\n",
      "✓ Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pattern analysis visualization - Simplified version\n",
    "Generates comparative visualizations of eye-tracking patterns for top vs bottom performers\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "DATA_DIR = Path(\"data files\")\n",
    "OUT_DIR = Path(\"figure_b/qualitative_analysis/pattern_analysis_simple\")\n",
    "\n",
    "# Colors from your screenshots\n",
    "COLOR_TOP = \"#5cb85c\"     # Green\n",
    "COLOR_BOTTOM = \"#d9534f\"  # Red\n",
    "\n",
    "SCREEN_SIZE = (1024, 768)\n",
    "BOX = (202, 74, 822, 694)  # Image box coordinates\n",
    "\n",
    "\n",
    "# =================== DATA LOADING ===================\n",
    "def load_data_simple():\n",
    "    \"\"\"Load data with minimal processing\"\"\"\n",
    "    \n",
    "    print(\"Loading data files...\")\n",
    "    \n",
    "    # Load first display data\n",
    "    first_files = [\"trialdata_testing_firstdisplayonly.tsv\", \"trial_data_testing_firstdisplayonly.tsv\"]\n",
    "    for fname in first_files:\n",
    "        fpath = DATA_DIR / fname\n",
    "        if fpath.exists():\n",
    "            print(f\"  Loading {fname}...\")\n",
    "            # Try different encodings\n",
    "            for encoding in [\"utf-16\", \"utf-8\", \"latin1\"]:\n",
    "                try:\n",
    "                    first_df = pd.read_csv(fpath, sep=\"\\t\", encoding=encoding)\n",
    "                    print(f\"    Success with {encoding} encoding\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            break\n",
    "    \n",
    "    # Load second display data\n",
    "    second_files = [\"trialdata_testing_seconddisplayonly.tsv\", \"trial_data_testing_seconddisplayonly.tsv\"]\n",
    "    for fname in second_files:\n",
    "        fpath = DATA_DIR / fname\n",
    "        if fpath.exists():\n",
    "            print(f\"  Loading {fname}...\")\n",
    "            for encoding in [\"utf-16\", \"utf-8\", \"latin1\"]:\n",
    "                try:\n",
    "                    second_df = pd.read_csv(fpath, sep=\"\\t\", encoding=encoding)\n",
    "                    print(f\"    Success with {encoding} encoding\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            break\n",
    "    \n",
    "    # Load fixation data\n",
    "    fix_path = DATA_DIR / \"fix_testing.tsv\"\n",
    "    print(f\"  Loading fix_testing.tsv...\")\n",
    "    for encoding in [\"utf-16\", \"utf-8\", \"latin1\"]:\n",
    "        try:\n",
    "            fix_df = pd.read_csv(fix_path, sep=\"\\t\", encoding=encoding)\n",
    "            print(f\"    Success with {encoding} encoding\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"  First display: {first_df.shape}\")\n",
    "    print(f\"  Second display: {second_df.shape}\")\n",
    "    print(f\"  Fixations: {fix_df.shape}\")\n",
    "    \n",
    "    return first_df, second_df, fix_df\n",
    "\n",
    "\n",
    "def get_column_names(df):\n",
    "    \"\"\"Get standardized column names\"\"\"\n",
    "    \n",
    "    # Map different possible column names to standard names\n",
    "    mappings = {\n",
    "        \"Session_Name_\": [\"Session_Name_\", \"RECORDING_SESSION_LABEL\", \"session\", \"Session\"],\n",
    "        \"Trial_Index_\": [\"Trial_Index_\", \"TRIAL_INDEX\", \"Trial_Index\", \"trial_index\"],\n",
    "        \"fix_x\": [\"CURRENT_FIX_X\", \"fix_x\", \"FIX_X\", \"X\"],\n",
    "        \"fix_y\": [\"CURRENT_FIX_Y\", \"fix_y\", \"FIX_Y\", \"Y\"],\n",
    "        \"fix_index\": [\"CURRENT_FIX_INDEX\", \"fix_index\", \"FIX_INDEX\"],\n",
    "        \"viewing\": [\"viewing\", \"Viewing\", \"VIEWING\", \"condition\"],\n",
    "        \"ACC\": [\"ACC\", \"accuracy\", \"correct\", \"CORRECT\"],\n",
    "        \"FIXATION_COUNT\": [\"FIXATION_COUNT\", \"fixation_count\", \"n_fixations\"],\n",
    "        \"AVERAGE_FIXATION_DURATION\": [\"AVERAGE_FIXATION_DURATION\", \"avg_fix_duration\", \"mean_duration\"]\n",
    "    }\n",
    "    \n",
    "    col_map = {}\n",
    "    for standard_name, possible_names in mappings.items():\n",
    "        for col in df.columns:\n",
    "            if col in possible_names:\n",
    "                col_map[standard_name] = col\n",
    "                break\n",
    "    \n",
    "    return col_map\n",
    "\n",
    "\n",
    "def calculate_metrics_simple(first_df, second_df, fix_df, viewing_cond):\n",
    "    \"\"\"Calculate metrics for a viewing condition\"\"\"\n",
    "    \n",
    "    print(f\"\\nProcessing {viewing_cond} viewing...\")\n",
    "    \n",
    "    # Get column mappings\n",
    "    first_cols = get_column_names(first_df)\n",
    "    fix_cols = get_column_names(fix_df)\n",
    "    \n",
    "    # Filter first display by viewing condition\n",
    "    viewing_col = first_cols.get(\"viewing\", \"viewing\")\n",
    "    if viewing_col not in first_df.columns:\n",
    "        print(f\"  Warning: viewing column not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    mask = first_df[viewing_col].astype(str).str.lower() == viewing_cond.lower()\n",
    "    trials = first_df[mask].copy()\n",
    "    \n",
    "    if trials.empty:\n",
    "        print(f\"  No trials found for {viewing_cond}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"  Found {len(trials)} trials\")\n",
    "    \n",
    "    # Process each trial\n",
    "    metrics_list = []\n",
    "    sess_col = first_cols.get(\"Session_Name_\", \"Session_Name_\")\n",
    "    trial_col = first_cols.get(\"Trial_Index_\", \"Trial_Index_\")\n",
    "    acc_col = first_cols.get(\"ACC\", \"ACC\")\n",
    "    \n",
    "    fix_sess_col = fix_cols.get(\"Session_Name_\", \"Session_Name_\")\n",
    "    fix_trial_col = fix_cols.get(\"Trial_Index_\", \"Trial_Index_\")\n",
    "    fix_x_col = fix_cols.get(\"fix_x\", \"CURRENT_FIX_X\")\n",
    "    fix_y_col = fix_cols.get(\"fix_y\", \"CURRENT_FIX_Y\")\n",
    "    \n",
    "    for idx, trial in trials.iterrows():\n",
    "        try:\n",
    "            sess = trial[sess_col]\n",
    "            tidx = trial[trial_col]\n",
    "            \n",
    "            # Convert trial index to numeric if needed\n",
    "            try:\n",
    "                tidx = float(tidx)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # Get fixations\n",
    "            fix_mask = (fix_df[fix_sess_col] == sess)\n",
    "            \n",
    "            # Handle trial index comparison carefully\n",
    "            fix_trial_vals = pd.to_numeric(fix_df[fix_trial_col], errors='coerce')\n",
    "            fix_mask = fix_mask & (fix_trial_vals == tidx)\n",
    "            \n",
    "            trial_fix = fix_df[fix_mask]\n",
    "            \n",
    "            if trial_fix.empty:\n",
    "                continue\n",
    "            \n",
    "            # Get fixations in box\n",
    "            l, t, r, b = BOX\n",
    "            x_vals = pd.to_numeric(trial_fix[fix_x_col], errors='coerce')\n",
    "            y_vals = pd.to_numeric(trial_fix[fix_y_col], errors='coerce')\n",
    "            \n",
    "            in_box = (x_vals >= l) & (x_vals <= r) & (y_vals >= t) & (y_vals <= b)\n",
    "            x_in_box = x_vals[in_box]\n",
    "            y_in_box = y_vals[in_box]\n",
    "            \n",
    "            if len(x_in_box) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                \"ACC\": float(trial[acc_col]) if acc_col in trial.index and pd.notna(trial[acc_col]) else 0,\n",
    "                \"Number of\\nFixations\": len(x_in_box),\n",
    "                \"Gaze\\nDispersion (px)\": np.sqrt(x_in_box.std()**2 + y_in_box.std()**2) if len(x_in_box) > 1 else 0,\n",
    "                \"Center\\nBias (px)\": np.mean(np.sqrt((x_in_box - 512)**2 + (y_in_box - 384)**2)),\n",
    "            }\n",
    "            \n",
    "            # Coverage calculation\n",
    "            x_img = x_in_box - l\n",
    "            y_img = y_in_box - t\n",
    "            grid = 31  # 620/20\n",
    "            x_bins = (x_img // 20).astype(int)\n",
    "            y_bins = (y_img // 20).astype(int)\n",
    "            unique_cells = len(set(zip(x_bins, y_bins)))\n",
    "            metrics[\"Coverage\\n(%)\"] = 100 * unique_cells / (grid * grid)\n",
    "            \n",
    "            # Duration metrics if available\n",
    "            fc_col = first_cols.get(\"FIXATION_COUNT\", \"FIXATION_COUNT\")\n",
    "            dur_col = first_cols.get(\"AVERAGE_FIXATION_DURATION\", \"AVERAGE_FIXATION_DURATION\")\n",
    "            \n",
    "            if dur_col in trial.index and pd.notna(trial[dur_col]):\n",
    "                metrics[\"Average\\nDuration (ms)\"] = float(trial[dur_col])\n",
    "                if fc_col in trial.index and pd.notna(trial[fc_col]):\n",
    "                    metrics[\"Total\\nDuration (ms)\"] = float(trial[dur_col]) * float(trial[fc_col])\n",
    "                else:\n",
    "                    metrics[\"Total\\nDuration (ms)\"] = float(trial[dur_col]) * len(x_in_box)\n",
    "            else:\n",
    "                metrics[\"Average\\nDuration (ms)\"] = np.nan\n",
    "                metrics[\"Total\\nDuration (ms)\"] = np.nan\n",
    "            \n",
    "            metrics_list.append(metrics)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    print(f\"  Calculated metrics for {len(df)} trials\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_visualization(metrics_df, viewing_cond):\n",
    "    \"\"\"Create the pattern analysis visualization\"\"\"\n",
    "    \n",
    "    if metrics_df.empty or len(metrics_df) < 4:\n",
    "        print(f\"  Insufficient data for visualization\")\n",
    "        return None\n",
    "    \n",
    "    # Split by accuracy\n",
    "    top_df = metrics_df[metrics_df[\"ACC\"] == 1]\n",
    "    bottom_df = metrics_df[metrics_df[\"ACC\"] == 0]\n",
    "    \n",
    "    # If not enough, use median split\n",
    "    if len(top_df) < 2 or len(bottom_df) < 2:\n",
    "        median_acc = metrics_df[\"ACC\"].median()\n",
    "        top_df = metrics_df[metrics_df[\"ACC\"] >= median_acc]\n",
    "        bottom_df = metrics_df[metrics_df[\"ACC\"] < median_acc]\n",
    "    \n",
    "    print(f\"  Top: {len(top_df)} trials, Bottom: {len(bottom_df)} trials\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    fig.suptitle(f\"{viewing_cond.upper()} Viewing - Pattern Analysis\\n\"\n",
    "                 \"Values that Explain the Overall Pattern\",\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Create subplots\n",
    "    metrics_to_plot = [\n",
    "        (\"Number of\\nFixations\", 1),\n",
    "        (\"Average\\nDuration (ms)\", 2),\n",
    "        (\"Gaze\\nDispersion (px)\", 3),\n",
    "        (\"Center\\nBias (px)\", 4),\n",
    "        (\"Coverage\\n(%)\", 5),\n",
    "        (\"Total\\nDuration (ms)\", 6)\n",
    "    ]\n",
    "    \n",
    "    for metric_name, pos in metrics_to_plot:\n",
    "        ax = plt.subplot(2, 3, pos)\n",
    "        \n",
    "        if metric_name not in metrics_df.columns:\n",
    "            ax.text(0.5, 0.5, \"N/A\", ha='center', va='center')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "        \n",
    "        # Get values\n",
    "        top_vals = top_df[metric_name].dropna()\n",
    "        bottom_vals = bottom_df[metric_name].dropna()\n",
    "        \n",
    "        if len(top_vals) == 0 or len(bottom_vals) == 0:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha='center', va='center')\n",
    "            continue\n",
    "        \n",
    "        # Stats\n",
    "        top_mean = top_vals.mean()\n",
    "        bottom_mean = bottom_vals.mean()\n",
    "        top_std = top_vals.std() if len(top_vals) > 1 else 0\n",
    "        bottom_std = bottom_vals.std() if len(bottom_vals) > 1 else 0\n",
    "        \n",
    "        # Delta\n",
    "        if top_mean != 0:\n",
    "            delta = 100 * (bottom_mean - top_mean) / top_mean\n",
    "        else:\n",
    "            delta = 0\n",
    "        \n",
    "        # Plot bars\n",
    "        x = [0.5, 1.5]\n",
    "        bars = ax.bar(x, [top_mean, bottom_mean],\n",
    "                      yerr=[top_std, bottom_std],\n",
    "                      width=0.6,\n",
    "                      color=[COLOR_TOP, COLOR_BOTTOM],\n",
    "                      edgecolor='black',\n",
    "                      capsize=5,\n",
    "                      alpha=0.9)\n",
    "        \n",
    "        # Labels on bars\n",
    "        for bar, val in zip(bars, [top_mean, bottom_mean]):\n",
    "            h = bar.get_height()\n",
    "            label = f'{val:.0f}' if val > 100 else f'{val:.1f}'\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, h/2,\n",
    "                   label, ha='center', va='center',\n",
    "                   fontsize=10, fontweight='bold', color='white')\n",
    "        \n",
    "        # Title\n",
    "        ax.set_title(f\"Δ = {delta:+.0f} ({delta:+.1f}%)\", fontsize=10)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(['Top\\nTrials', 'Bottom\\nTrials'])\n",
    "        ax.set_ylabel(metric_name, fontsize=10)\n",
    "        ax.yaxis.grid(True, alpha=0.3)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_ylim(bottom=0)\n",
    "    \n",
    "    # Add text box\n",
    "    acc_top = 100 * top_df[\"ACC\"].mean()\n",
    "    acc_bottom = 100 * bottom_df[\"ACC\"].mean()\n",
    "    \n",
    "    text = f\"Key Patterns for {viewing_cond.upper()} viewing:\\n\"\n",
    "    text += f\"• Top trials (avg accuracy: {acc_top:.1f}%) show most difference in fixation patterns\\n\"\n",
    "    text += f\"• Bottom trials (avg accuracy: {acc_bottom:.1f}%) demonstrate inefficient scanning\\n\"\n",
    "    text += f\"• Success rate difference: {acc_top - acc_bottom:.1f}%\\n\"\n",
    "    text += \"• These metrics explain the overall pattern between successful and unsuccessful trials\"\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    fig.text(0.5, 0.01, text, ha='center', fontsize=10, bbox=props)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "    return fig\n",
    "\n",
    "\n",
    "# =================== MAIN ===================\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    \n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PATTERN ANALYSIS - SIMPLE VERSION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    first_df, second_df, fix_df = load_data_simple()\n",
    "    \n",
    "    # Process each viewing condition\n",
    "    for viewing in [\"full\", \"central\", \"peripheral\"]:\n",
    "        try:\n",
    "            # Calculate metrics\n",
    "            metrics_df = calculate_metrics_simple(first_df, second_df, fix_df, viewing)\n",
    "            \n",
    "            if metrics_df.empty:\n",
    "                continue\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = create_visualization(metrics_df, viewing)\n",
    "            \n",
    "            if fig:\n",
    "                # Save\n",
    "                out_pdf = OUT_DIR / f\"pattern_analysis_{viewing}.pdf\"\n",
    "                out_png = OUT_DIR / f\"pattern_analysis_{viewing}.png\"\n",
    "                \n",
    "                fig.savefig(out_pdf, dpi=150, bbox_inches='tight')\n",
    "                fig.savefig(out_png, dpi=150, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "                \n",
    "                print(f\"  ✓ Saved: {out_pdf}\")\n",
    "                print(f\"  ✓ Saved: {out_png}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {viewing}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ Complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95da1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STANDARDIZED PATTERN ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "  Loaded with utf-16 encoding\n",
      "\n",
      "Processing full viewing...\n",
      "  Calculated metrics for 768 trials\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_standardized/summary_full_corrected.csv\n",
      "\n",
      "Processing central viewing...\n",
      "  Calculated metrics for 767 trials\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_standardized/summary_central_corrected.csv\n",
      "\n",
      "Processing peripheral viewing...\n",
      "  Calculated metrics for 768 trials\n",
      "  ✓ Saved: figure_b/qualitative_analysis/pattern_analysis_standardized/summary_peripheral_corrected.csv\n",
      "\n",
      "✓ Saved corrected visualization:\n",
      "  figure_b/qualitative_analysis/pattern_analysis_standardized/standardized_effect_sizes_all_conditions_corrected.pdf\n",
      "  figure_b/qualitative_analysis/pattern_analysis_standardized/standardized_effect_sizes_all_conditions_corrected.png\n",
      "\n",
      "============================================================\n",
      "TOP DISCRIMINATIVE METRICS (Avg |Cohen's d|):\n",
      "============================================================\n",
      " 1. Image Coverage (%)        | 0.243\n",
      " 2. Spatial Entropy           | 0.220\n",
      " 3. Unique Areas              | 0.196\n",
      " 4. Scan Path Length          | 0.191\n",
      " 5. Fixation Count            | 0.188\n",
      " 6. Scan Path Efficiency      | 0.151\n",
      " 7. Horizontal Spread         | 0.141\n",
      " 8. Avg Saccade Amplitude     | 0.114\n",
      " 9. Gaze Dispersion (px)      | 0.108\n",
      "10. Avg Duration (ms)         | 0.106\n",
      "\n",
      "============================================================\n",
      "✓ Analysis complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pattern Analysis Summary with Standardized Axes and Clear Significance Indicators\n",
    "Creates effect size visualizations with consistent scales across all viewing conditions\n",
    "CORRECTED VERSION: Green = Correct trials, Red = Incorrect trials\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Config\n",
    "DATA_DIR = Path(\"data files\")\n",
    "OUT_DIR = Path(\"figure_b/qualitative_analysis/pattern_analysis_standardized\")\n",
    "\n",
    "# Colors - FIXED: Intuitive color scheme\n",
    "COLOR_CORRECT = \"#5cb85c\"    # Green - for correct responses\n",
    "COLOR_INCORRECT = \"#d9534f\"  # Red - for incorrect responses\n",
    "\n",
    "\n",
    "def calculate_all_metrics(first_df, fix_df, viewing_condition):\n",
    "    \"\"\"Calculate comprehensive metrics for all trials\"\"\"\n",
    "    \n",
    "    # Filter by viewing condition\n",
    "    mask = first_df[\"viewing\"].astype(str).str.lower() == viewing_condition.lower()\n",
    "    trials = first_df[mask].copy()\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    for idx, trial in trials.iterrows():\n",
    "        try:\n",
    "            # Get session and trial info\n",
    "            sess = trial.get(\"Session_Name_\", trial.get(\"RECORDING_SESSION_LABEL\", \"\"))\n",
    "            tidx = pd.to_numeric(trial.get(\"Trial_Index_\", trial.get(\"TRIAL_INDEX\", -1)), errors='coerce')\n",
    "            \n",
    "            if pd.isna(tidx):\n",
    "                continue\n",
    "            \n",
    "            # Get fixations\n",
    "            fix_mask = fix_df.get(\"Session_Name_\", fix_df.get(\"RECORDING_SESSION_LABEL\", \"\")) == sess\n",
    "            trial_fix_idx = pd.to_numeric(fix_df.get(\"Trial_Index_\", fix_df.get(\"TRIAL_INDEX\", 0)), errors='coerce')\n",
    "            fix_mask = fix_mask & (trial_fix_idx == tidx)\n",
    "            \n",
    "            trial_fix = fix_df[fix_mask].copy()\n",
    "            \n",
    "            if trial_fix.empty:\n",
    "                continue\n",
    "            \n",
    "            # Get coordinates\n",
    "            x_col = \"CURRENT_FIX_X\" if \"CURRENT_FIX_X\" in trial_fix.columns else \"fix_x\"\n",
    "            y_col = \"CURRENT_FIX_Y\" if \"CURRENT_FIX_Y\" in trial_fix.columns else \"fix_y\"\n",
    "            \n",
    "            x = pd.to_numeric(trial_fix[x_col], errors='coerce').dropna().values\n",
    "            y = pd.to_numeric(trial_fix[y_col], errors='coerce').dropna().values\n",
    "            \n",
    "            if len(x) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Image box\n",
    "            BOX = (202, 74, 822, 694)\n",
    "            l, t, r, b = BOX\n",
    "            \n",
    "            # Filter to box\n",
    "            in_box = (x >= l) & (x <= r) & (y >= t) & (y <= b)\n",
    "            x = x[in_box]\n",
    "            y = y[in_box]\n",
    "            \n",
    "            if len(x) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Convert to image coords\n",
    "            x_img = x - l\n",
    "            y_img = y - t\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                \"ACC\": float(trial.get(\"ACC\", 0)),\n",
    "                \"Fixation Count\": len(x),\n",
    "                \"Gaze Dispersion (px)\": np.sqrt(x_img.std()**2 + y_img.std()**2),\n",
    "                \"Center Bias (px)\": np.mean(np.sqrt((x_img - 310)**2 + (y_img - 310)**2)),\n",
    "                \"Image Coverage (%)\": calculate_coverage(x_img, y_img),\n",
    "                \"Quadrant Coverage\": count_quadrants(x_img, y_img),\n",
    "                \"Periphery Ratio (%)\": 100 * np.sum(np.sqrt((x_img - 310)**2 + (y_img - 310)**2) > 200) / len(x),\n",
    "                \"Scan Path Length\": calculate_path_length(x, y),\n",
    "                \"Scan Path Efficiency\": calculate_path_efficiency(x, y),\n",
    "                \"Avg Saccade Amplitude\": np.mean(np.sqrt(np.diff(x)**2 + np.diff(y)**2)),\n",
    "                \"Revisit Rate (%)\": calculate_revisit_rate(x_img, y_img),\n",
    "                \"Unique Areas\": count_unique_areas(x_img, y_img),\n",
    "                \"Spatial Entropy\": calculate_spatial_entropy(x_img, y_img),\n",
    "                \"Horizontal Spread\": x_img.std(),\n",
    "                \"Vertical Spread\": y_img.std(),\n",
    "                \"Avg Duration (ms)\": trial.get(\"AVERAGE_FIXATION_DURATION\", np.nan),\n",
    "                \"Total Duration (ms)\": trial.get(\"FIXATION_COUNT\", len(x)) * trial.get(\"AVERAGE_FIXATION_DURATION\", np.nan),\n",
    "            }\n",
    "            \n",
    "            metrics_list.append(metrics)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(metrics_list)\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def calculate_coverage(x, y, grid_size=31):\n",
    "    x_bins = (x // 20).astype(int)\n",
    "    y_bins = (y // 20).astype(int)\n",
    "    unique_cells = len(set(zip(x_bins, y_bins)))\n",
    "    return 100 * unique_cells / (grid_size * grid_size)\n",
    "\n",
    "\n",
    "def count_quadrants(x, y):\n",
    "    quadrants = set()\n",
    "    for xi, yi in zip(x, y):\n",
    "        q = 0\n",
    "        if xi >= 310: q += 1\n",
    "        if yi >= 310: q += 2\n",
    "        quadrants.add(q)\n",
    "    return len(quadrants)\n",
    "\n",
    "\n",
    "def calculate_path_length(x, y):\n",
    "    if len(x) < 2:\n",
    "        return 0\n",
    "    return np.sum(np.sqrt(np.diff(x)**2 + np.diff(y)**2))\n",
    "\n",
    "\n",
    "def calculate_path_efficiency(x, y):\n",
    "    if len(x) < 3:\n",
    "        return 1.0\n",
    "    try:\n",
    "        from scipy.spatial import ConvexHull\n",
    "        points = np.column_stack((x, y))\n",
    "        hull = ConvexHull(points)\n",
    "        hull_perimeter = hull.area\n",
    "        actual_path = calculate_path_length(x, y)\n",
    "        return hull_perimeter / actual_path if actual_path > 0 else 1.0\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def calculate_revisit_rate(x, y, grid_size=50):\n",
    "    cells = [(int(xi//grid_size), int(yi//grid_size)) for xi, yi in zip(x, y)]\n",
    "    total_visits = len(cells)\n",
    "    unique_visits = len(set(cells))\n",
    "    return 100 * (1 - unique_visits/total_visits) if total_visits > 0 else 0\n",
    "\n",
    "\n",
    "def count_unique_areas(x, y, grid_size=50):\n",
    "    cells = [(int(xi//grid_size), int(yi//grid_size)) for xi, yi in zip(x, y)]\n",
    "    return len(set(cells))\n",
    "\n",
    "\n",
    "def calculate_spatial_entropy(x, y, bins=10):\n",
    "    hist, _, _ = np.histogram2d(x, y, bins=bins)\n",
    "    hist = hist.flatten()\n",
    "    hist = hist[hist > 0]\n",
    "    if len(hist) == 0:\n",
    "        return 0\n",
    "    hist_norm = hist / hist.sum()\n",
    "    return -np.sum(hist_norm * np.log(hist_norm + 1e-10))\n",
    "\n",
    "\n",
    "def calculate_effect_sizes(metrics_df):\n",
    "    \"\"\"Calculate effect sizes and significance\"\"\"\n",
    "    \n",
    "    # Split by performance - RENAMED FOR CLARITY\n",
    "    correct_df = metrics_df[metrics_df[\"ACC\"] == 1]\n",
    "    incorrect_df = metrics_df[metrics_df[\"ACC\"] == 0]\n",
    "    \n",
    "    # Fallback if not enough samples\n",
    "    if len(correct_df) < 3 or len(incorrect_df) < 3:\n",
    "        median_acc = metrics_df[\"ACC\"].median()\n",
    "        correct_df = metrics_df[metrics_df[\"ACC\"] >= median_acc]\n",
    "        incorrect_df = metrics_df[metrics_df[\"ACC\"] < median_acc]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for col in metrics_df.columns:\n",
    "        if col == \"ACC\":\n",
    "            continue\n",
    "        \n",
    "        correct_vals = correct_df[col].dropna()\n",
    "        incorrect_vals = incorrect_df[col].dropna()\n",
    "        \n",
    "        if len(correct_vals) < 2 or len(incorrect_vals) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Statistics\n",
    "        correct_mean = correct_vals.mean()\n",
    "        incorrect_mean = incorrect_vals.mean()\n",
    "        correct_std = correct_vals.std()\n",
    "        incorrect_std = incorrect_vals.std()\n",
    "        \n",
    "        # Cohen's d - positive when incorrect > correct\n",
    "        pooled_std = np.sqrt((correct_std**2 + incorrect_std**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            cohens_d = (incorrect_mean - correct_mean) / pooled_std\n",
    "        else:\n",
    "            cohens_d = 0\n",
    "        \n",
    "        # Percentage change\n",
    "        if correct_mean != 0:\n",
    "            pct_change = 100 * (incorrect_mean - correct_mean) / correct_mean\n",
    "        else:\n",
    "            pct_change = 0\n",
    "        \n",
    "        # Statistical test\n",
    "        t_stat, p_value = stats.ttest_ind(correct_vals, incorrect_vals)\n",
    "        \n",
    "        # Significance level\n",
    "        if p_value < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif p_value < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            sig = \"*\"\n",
    "        else:\n",
    "            sig = \"\"\n",
    "        \n",
    "        results.append({\n",
    "            \"Metric\": col,\n",
    "            \"Correct Mean\": correct_mean,\n",
    "            \"Incorrect Mean\": incorrect_mean,\n",
    "            \"% Change\": pct_change,\n",
    "            \"Cohen's d\": cohens_d,\n",
    "            \"p-value\": p_value,\n",
    "            \"Significance\": sig\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def create_standardized_visualization(all_summaries):\n",
    "    \"\"\"Create visualization with standardized axes across all conditions\"\"\"\n",
    "    \n",
    "    # Create figure with subplots for each viewing condition\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    fig.suptitle(\"Metrics that Best Explain Performance Differences Across Viewing Conditions\\n\"\n",
    "                 \"Effect Sizes (Cohen's d) with Statistical Significance\",\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Find global min/max for standardized axes\n",
    "    all_cohens_d = []\n",
    "    all_pct_change = []\n",
    "    for summary in all_summaries:\n",
    "        all_cohens_d.extend(summary[\"Cohen's d\"].values)\n",
    "        all_pct_change.extend(summary[\"% Change\"].values)\n",
    "    \n",
    "    # Set standardized ranges\n",
    "    d_min, d_max = -0.3, 0.3  # As requested\n",
    "    pct_min, pct_max = -10, 15  # Reasonable range for percentage\n",
    "    \n",
    "    viewing_conditions = [\"full\", \"central\", \"peripheral\"]\n",
    "    \n",
    "    for idx, (viewing, summary_df) in enumerate(zip(viewing_conditions, all_summaries)):\n",
    "        # Sort by absolute effect size\n",
    "        summary_df = summary_df.sort_values(\"Cohen's d\", key=abs, ascending=False).head(12)\n",
    "        \n",
    "        # Left plot: Effect sizes\n",
    "        ax1 = plt.subplot(3, 2, idx*2 + 1)\n",
    "        \n",
    "        # FIXED COLOR ASSIGNMENT: Red when incorrect > correct (d > 0), Green when correct > incorrect (d < 0)\n",
    "        colors = [COLOR_INCORRECT if d > 0 else COLOR_CORRECT for d in summary_df[\"Cohen's d\"]]\n",
    "        bars = ax1.barh(range(len(summary_df)), summary_df[\"Cohen's d\"], \n",
    "                        color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Add significance markers\n",
    "        for i, (d, sig) in enumerate(zip(summary_df[\"Cohen's d\"], summary_df[\"Significance\"])):\n",
    "            if sig:\n",
    "                x_pos = d + 0.01 if d > 0 else d - 0.01\n",
    "                ax1.text(x_pos, i, sig, va='center', ha='left' if d > 0 else 'right',\n",
    "                        fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax1.set_yticks(range(len(summary_df)))\n",
    "        ax1.set_yticklabels(summary_df[\"Metric\"], fontsize=9)\n",
    "        ax1.set_xlabel(\"Cohen's d (Effect Size)\", fontsize=10)\n",
    "        ax1.set_title(f\"{viewing.upper()} Viewing - Effect Sizes\", fontsize=11, fontweight='bold')\n",
    "        ax1.set_xlim(d_min, d_max)\n",
    "        ax1.axvline(0, color='black', linewidth=0.5)\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Right plot: Percentage changes\n",
    "        ax2 = plt.subplot(3, 2, idx*2 + 2)\n",
    "        \n",
    "        # Same color logic for percentage changes\n",
    "        colors = [COLOR_INCORRECT if d > 0 else COLOR_CORRECT for d in summary_df[\"% Change\"]]\n",
    "        bars = ax2.barh(range(len(summary_df)), summary_df[\"% Change\"], \n",
    "                        color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax2.set_yticks(range(len(summary_df)))\n",
    "        ax2.set_yticklabels(summary_df[\"Metric\"], fontsize=9)\n",
    "        ax2.set_xlabel(\"Percentage Change (%)\", fontsize=10)\n",
    "        ax2.set_title(f\"{viewing.upper()} Viewing - Relative Differences\", fontsize=11, fontweight='bold')\n",
    "        ax2.set_xlim(pct_min, pct_max)\n",
    "        ax2.axvline(0, color='black', linewidth=0.5)\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add legend for significance levels\n",
    "    legend_text = \"Significance: *** p<0.001, ** p<0.01, * p<0.05\"\n",
    "    fig.text(0.5, 0.02, legend_text, ha='center', fontsize=10, \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # FIXED COLOR LEGEND - Clear and intuitive\n",
    "    green_patch = mpatches.Patch(color=COLOR_CORRECT, alpha=0.7, label='Higher in Correct Trials')\n",
    "    red_patch = mpatches.Patch(color=COLOR_INCORRECT, alpha=0.7, label='Higher in Incorrect Trials')\n",
    "    fig.legend(handles=[green_patch, red_patch], loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    return fig\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate standardized summary analysis\"\"\"\n",
    "    \n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STANDARDIZED PATTERN ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    for encoding in [\"utf-16\", \"utf-8\", \"latin1\"]:\n",
    "        try:\n",
    "            first_df = pd.read_csv(DATA_DIR / \"trialdata_testing_firstdisplayonly.tsv\", \n",
    "                                  sep=\"\\t\", encoding=encoding)\n",
    "            fix_df = pd.read_csv(DATA_DIR / \"fix_testing.tsv\",\n",
    "                                sep=\"\\t\", encoding=encoding)\n",
    "            print(f\"  Loaded with {encoding} encoding\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Process each viewing condition\n",
    "    all_summaries = []\n",
    "    \n",
    "    for viewing in [\"full\", \"central\", \"peripheral\"]:\n",
    "        print(f\"\\nProcessing {viewing} viewing...\")\n",
    "        \n",
    "        try:\n",
    "            # Calculate metrics\n",
    "            metrics_df = calculate_all_metrics(first_df, fix_df, viewing)\n",
    "            \n",
    "            if metrics_df.empty:\n",
    "                print(f\"  No data for {viewing}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Calculated metrics for {len(metrics_df)} trials\")\n",
    "            \n",
    "            # Calculate effect sizes\n",
    "            summary_df = calculate_effect_sizes(metrics_df)\n",
    "            all_summaries.append(summary_df)\n",
    "            \n",
    "            # Save individual summaries with updated column names\n",
    "            table_path = OUT_DIR / f\"summary_{viewing}_corrected.csv\"\n",
    "            summary_df.to_csv(table_path, index=False)\n",
    "            print(f\"  ✓ Saved: {table_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create combined standardized visualization\n",
    "    if len(all_summaries) == 3:\n",
    "        fig = create_standardized_visualization(all_summaries)\n",
    "        \n",
    "        # Save combined figure\n",
    "        out_pdf = OUT_DIR / \"standardized_effect_sizes_all_conditions_corrected.pdf\"\n",
    "        out_png = OUT_DIR / \"standardized_effect_sizes_all_conditions_corrected.png\"\n",
    "        \n",
    "        fig.savefig(out_pdf, dpi=300, bbox_inches='tight')\n",
    "        fig.savefig(out_png, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"\\n✓ Saved corrected visualization:\")\n",
    "        print(f\"  {out_pdf}\")\n",
    "        print(f\"  {out_png}\")\n",
    "    \n",
    "    # Print top metrics across all conditions\n",
    "    if all_summaries:\n",
    "        combined = pd.concat(all_summaries, ignore_index=True)\n",
    "        avg_effects = combined.groupby(\"Metric\")[\"Cohen's d\"].apply(lambda x: np.mean(np.abs(x))).sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TOP DISCRIMINATIVE METRICS (Avg |Cohen's d|):\")\n",
    "        print(\"=\"*60)\n",
    "        for i, (metric, effect) in enumerate(avg_effects.head(10).items(), 1):\n",
    "            print(f\"{i:2d}. {metric:25s} | {effect:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ Analysis complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f865c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PATTERN ANALYSIS VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Loading sample data...\n",
      "  ✓ Loaded with utf-16 encoding\n",
      "\n",
      "================================================================================\n",
      "UNDERSTANDING ACC VALUES\n",
      "================================================================================\n",
      "\n",
      "ACC column unique values: [1 0]\n",
      "ACC = 1 means: CORRECT trial\n",
      "ACC = 0 means: INCORRECT trial\n",
      "\n",
      "Correct trials (ACC=1): 85\n",
      "Incorrect trials (ACC=0): 15\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE CALCULATION\n",
      "================================================================================\n",
      "\n",
      "Metric: FIXATION_COUNT\n",
      "  Correct trials mean:   13.96\n",
      "  Incorrect trials mean: 14.73\n",
      "  Difference: 0.77\n",
      "\n",
      "  Cohen's d = (incorrect_mean - correct_mean) / pooled_std\n",
      "  Cohen's d = (14.73 - 13.96) / 2.91\n",
      "  Cohen's d = 0.265\n",
      "\n",
      "================================================================================\n",
      "COLOR CODING LOGIC\n",
      "================================================================================\n",
      "\n",
      "Cohen's d = 0.265\n",
      "  → Positive Cohen's d means: INCORRECT trials have HIGHER values\n",
      "  → Color should be: RED (incorrect trials are higher)\n",
      "  → Interpretation: This metric is ELEVATED in incorrect trials\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION OF SCRIPT LOGIC\n",
      "================================================================================\n",
      "\n",
      "Your script does:\n",
      "  1. cohens_d = (incorrect_mean - correct_mean) / pooled_std\n",
      "  2. If d > 0: Use RED (incorrect > correct)\n",
      "  3. If d < 0: Use GREEN (correct > incorrect)\n",
      "\n",
      "✓ This is CORRECT!\n",
      "\n",
      "Color interpretation:\n",
      "  🔴 RED bars   = Metric is higher when trials are INCORRECT\n",
      "  🟢 GREEN bars = Metric is higher when trials are CORRECT\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE INTERPRETATIONS\n",
      "================================================================================\n",
      "\n",
      "If you see:\n",
      "\n",
      "  RED bar for 'Fixation Count' (d > 0):\n",
      "    → People made MORE fixations on INCORRECT trials\n",
      "    → Suggests difficulty/confusion\n",
      "\n",
      "  GREEN bar for 'Scan Path Efficiency' (d < 0):\n",
      "    → People had HIGHER efficiency on CORRECT trials\n",
      "    → Suggests more systematic scanning when correct\n",
      "\n",
      "  RED bar for 'Center Bias' (d > 0):\n",
      "    → People looked MORE at center on INCORRECT trials\n",
      "    → Suggests insufficient exploration\n",
      "\n",
      "================================================================================\n",
      "FINAL CHECK\n",
      "================================================================================\n",
      "\n",
      "The script color logic:\n",
      "  colors = [COLOR_INCORRECT if d > 0 else COLOR_CORRECT for d in ...] \n",
      "  where COLOR_INCORRECT = RED, COLOR_CORRECT = GREEN\n",
      "\n",
      "✓ This is CORRECT and INTUITIVE!\n",
      "\n",
      "✓ Red = bad (higher in incorrect trials)\n",
      "✓ Green = good (higher in correct trials)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Your pattern analysis script is CORRECT if:\n",
      "  1. ACC = 1 means correct trials ✓\n",
      "  2. ACC = 0 means incorrect trials ✓\n",
      "  3. Cohen's d = (incorrect_mean - correct_mean) / pooled_std ✓\n",
      "  4. RED (d > 0) means metric is higher in incorrect trials ✓\n",
      "  5. GREEN (d < 0) means metric is higher in correct trials ✓\n",
      "\n",
      "✓ All logic appears CORRECT!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verification script to check if the pattern analysis colors and calculations are correct\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Config\n",
    "DATA_DIR = Path(\"data files\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PATTERN ANALYSIS VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load a sample of data to verify the logic\n",
    "print(\"\\nLoading sample data...\")\n",
    "for encoding in [\"utf-16\", \"utf-8\", \"latin1\"]:\n",
    "    try:\n",
    "        first_df = pd.read_csv(DATA_DIR / \"trialdata_testing_firstdisplayonly.tsv\", \n",
    "                              sep=\"\\t\", encoding=encoding, nrows=100)\n",
    "        print(f\"  ✓ Loaded with {encoding} encoding\")\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Check what ACC values look like\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNDERSTANDING ACC VALUES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nACC column unique values: {first_df['ACC'].unique()}\")\n",
    "print(f\"ACC = 1 means: CORRECT trial\")\n",
    "print(f\"ACC = 0 means: INCORRECT trial\")\n",
    "\n",
    "# Simulate the splitting logic from your script\n",
    "correct_df = first_df[first_df[\"ACC\"] == 1]\n",
    "incorrect_df = first_df[first_df[\"ACC\"] == 0]\n",
    "\n",
    "print(f\"\\nCorrect trials (ACC=1): {len(correct_df)}\")\n",
    "print(f\"Incorrect trials (ACC=0): {len(incorrect_df)}\")\n",
    "\n",
    "# Simulate a metric calculation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Let's say \"Fixation Count\" for example\n",
    "if \"FIXATION_COUNT\" in first_df.columns:\n",
    "    metric_name = \"FIXATION_COUNT\"\n",
    "    correct_vals = correct_df[metric_name].dropna()\n",
    "    incorrect_vals = incorrect_df[metric_name].dropna()\n",
    "    \n",
    "    correct_mean = correct_vals.mean()\n",
    "    incorrect_mean = incorrect_vals.mean()\n",
    "    \n",
    "    print(f\"\\nMetric: {metric_name}\")\n",
    "    print(f\"  Correct trials mean:   {correct_mean:.2f}\")\n",
    "    print(f\"  Incorrect trials mean: {incorrect_mean:.2f}\")\n",
    "    print(f\"  Difference: {incorrect_mean - correct_mean:.2f}\")\n",
    "    \n",
    "    # Cohen's d calculation\n",
    "    correct_std = correct_vals.std()\n",
    "    incorrect_std = incorrect_vals.std()\n",
    "    pooled_std = np.sqrt((correct_std**2 + incorrect_std**2) / 2)\n",
    "    cohens_d = (incorrect_mean - correct_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  Cohen's d = (incorrect_mean - correct_mean) / pooled_std\")\n",
    "    print(f\"  Cohen's d = ({incorrect_mean:.2f} - {correct_mean:.2f}) / {pooled_std:.2f}\")\n",
    "    print(f\"  Cohen's d = {cohens_d:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLOR CODING LOGIC\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nCohen's d = {cohens_d:.3f}\")\n",
    "    \n",
    "    if cohens_d > 0:\n",
    "        print(f\"  → Positive Cohen's d means: INCORRECT trials have HIGHER values\")\n",
    "        print(f\"  → Color should be: RED (incorrect trials are higher)\")\n",
    "        print(f\"  → Interpretation: This metric is ELEVATED in incorrect trials\")\n",
    "    else:\n",
    "        print(f\"  → Negative Cohen's d means: CORRECT trials have HIGHER values\")\n",
    "        print(f\"  → Color should be: GREEN (correct trials are higher)\")\n",
    "        print(f\"  → Interpretation: This metric is ELEVATED in correct trials\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFICATION OF SCRIPT LOGIC\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nYour script does:\")\n",
    "    print(\"  1. cohens_d = (incorrect_mean - correct_mean) / pooled_std\")\n",
    "    print(\"  2. If d > 0: Use RED (incorrect > correct)\")\n",
    "    print(\"  3. If d < 0: Use GREEN (correct > incorrect)\")\n",
    "    \n",
    "    print(\"\\n✓ This is CORRECT!\")\n",
    "    print(\"\\nColor interpretation:\")\n",
    "    print(\"  🔴 RED bars   = Metric is higher when trials are INCORRECT\")\n",
    "    print(\"  🟢 GREEN bars = Metric is higher when trials are CORRECT\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE INTERPRETATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nIf you see:\")\n",
    "    print(\"\\n  RED bar for 'Fixation Count' (d > 0):\")\n",
    "    print(\"    → People made MORE fixations on INCORRECT trials\")\n",
    "    print(\"    → Suggests difficulty/confusion\")\n",
    "    \n",
    "    print(\"\\n  GREEN bar for 'Scan Path Efficiency' (d < 0):\")\n",
    "    print(\"    → People had HIGHER efficiency on CORRECT trials\")\n",
    "    print(\"    → Suggests more systematic scanning when correct\")\n",
    "    \n",
    "    print(\"\\n  RED bar for 'Center Bias' (d > 0):\")\n",
    "    print(\"    → People looked MORE at center on INCORRECT trials\")\n",
    "    print(\"    → Suggests insufficient exploration\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nThe script color logic:\")\n",
    "    print(\"  colors = [COLOR_INCORRECT if d > 0 else COLOR_CORRECT for d in ...] \")\n",
    "    print(\"  where COLOR_INCORRECT = RED, COLOR_CORRECT = GREEN\")\n",
    "    print(\"\\n✓ This is CORRECT and INTUITIVE!\")\n",
    "    print(\"\\n✓ Red = bad (higher in incorrect trials)\")\n",
    "    print(\"✓ Green = good (higher in correct trials)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nFIXATION_COUNT not found, checking available columns...\")\n",
    "    print(f\"Available: {first_df.columns.tolist()[:20]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYour pattern analysis script is CORRECT if:\")\n",
    "print(\"  1. ACC = 1 means correct trials ✓\")\n",
    "print(\"  2. ACC = 0 means incorrect trials ✓\")\n",
    "print(\"  3. Cohen's d = (incorrect_mean - correct_mean) / pooled_std ✓\")\n",
    "print(\"  4. RED (d > 0) means metric is higher in incorrect trials ✓\")\n",
    "print(\"  5. GREEN (d < 0) means metric is higher in correct trials ✓\")\n",
    "print(\"\\n✓ All logic appears CORRECT!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2527010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIXATION VISUALIZATION ACCURACY VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Loading sample data...\n",
      "  ✓ Loaded with utf-16 encoding\n",
      "\n",
      "================================================================================\n",
      "CHECKING ACCURACY CALCULATION IN YOUR SCRIPT\n",
      "================================================================================\n",
      "\n",
      "Your script does this for each trial:\n",
      "\n",
      "    acc = r.get(\"ACC\")\n",
      "    if pd.isna(acc):\n",
      "        # fall back to RESPONSE ?= correct_response\n",
      "        resp = str(r.get(\"RESPONSE\", \"\")).strip().lower()\n",
      "        cor  = str(r.get(\"correct_response\", \"\")).strip().lower()\n",
      "        if resp and cor:\n",
      "            acc_flags.append(int(resp == cor))\n",
      "    else:\n",
      "        try:\n",
      "            acc_flags.append(int(bool(int(acc))))\n",
      "        except Exception:\n",
      "            acc_flags.append(int(bool(acc)))\n",
      "\n",
      "\n",
      "Let's test this logic with sample data:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Trial 0:\n",
      "  ACC: 1\n",
      "  RESPONSE: 6, correct_response: 6\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 1:\n",
      "  ACC: 1\n",
      "  RESPONSE: 6, correct_response: 6\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 2:\n",
      "  ACC: 1\n",
      "  RESPONSE: 7, correct_response: 7\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 3:\n",
      "  ACC: 1\n",
      "  RESPONSE: 6, correct_response: 6\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 4:\n",
      "  ACC: 1\n",
      "  RESPONSE: 7, correct_response: 7\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 5:\n",
      "  ACC: 0\n",
      "  RESPONSE: 6, correct_response: 7\n",
      "  Computed accuracy: 0 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 6:\n",
      "  ACC: 1\n",
      "  RESPONSE: 6, correct_response: 6\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 7:\n",
      "  ACC: 0\n",
      "  RESPONSE: 6, correct_response: 7\n",
      "  Computed accuracy: 0 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 8:\n",
      "  ACC: 1\n",
      "  RESPONSE: 6, correct_response: 6\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "Trial 9:\n",
      "  ACC: 1\n",
      "  RESPONSE: 7, correct_response: 7\n",
      "  Computed accuracy: 1 (using ACC column (converted to int/bool/int))\n",
      "\n",
      "================================================================================\n",
      "POTENTIAL ISSUES\n",
      "================================================================================\n",
      "\n",
      "1. ACC Column Usage:\n",
      "   Your script uses: int(bool(int(acc)))\n",
      "   This means:\n",
      "     - ACC = 0 → int(bool(int(0))) = int(bool(0)) = int(False) = 0 ✓\n",
      "     - ACC = 1 → int(bool(int(1))) = int(bool(1)) = int(True) = 1 ✓\n",
      "   ✓ This is CORRECT for binary ACC values\n",
      "\n",
      "2. Response Code Fallback:\n",
      "   If ACC is missing, it uses: int(RESPONSE == correct_response)\n",
      "   Question: Are RESPONSE and correct_response the same type?\n",
      "\n",
      "   RESPONSE unique values: [6 7]\n",
      "   correct_response unique values: [6 7]\n",
      "\n",
      "   RESPONSE as numeric: [6 7]\n",
      "   correct_response as numeric: [6 7]\n",
      "\n",
      "3. String Comparison Issue:\n",
      "   Your script converts both to strings and compares:\n",
      "     resp = str(r.get('RESPONSE', '')).strip().lower()\n",
      "     cor = str(r.get('correct_response', '')).strip().lower()\n",
      "     acc = int(resp == cor)\n",
      "\n",
      "   ⚠ POTENTIAL PROBLEM:\n",
      "     - If RESPONSE = 6 (number) and correct_response = 6 (number)\n",
      "     - str(6) = '6' and str(6) = '6'\n",
      "     - '6' == '6' → True ✓\n",
      "\n",
      "     - If RESPONSE = 6.0 (float) and correct_response = 6 (int)\n",
      "     - str(6.0) = '6.0' and str(6) = '6'\n",
      "     - '6.0' == '6' → False ❌ MISMATCH!\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "To be safe, compare as NUMBERS, not strings:\n",
      "\n",
      "    # Instead of:\n",
      "    resp = str(r.get(\"RESPONSE\", \"\")).strip().lower()\n",
      "    cor = str(r.get(\"correct_response\", \"\")).strip().lower()\n",
      "    if resp and cor:\n",
      "        acc_flags.append(int(resp == cor))\n",
      "    \n",
      "    # Use:\n",
      "    resp = pd.to_numeric(r.get(\"RESPONSE\"), errors='coerce')\n",
      "    cor = pd.to_numeric(r.get(\"correct_response\"), errors='coerce')\n",
      "    if pd.notna(resp) and pd.notna(cor):\n",
      "        acc_flags.append(int(resp == cor))\n",
      "\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION WITH ACTUAL DATA\n",
      "================================================================================\n",
      "\n",
      "✓ No mismatches found! String comparison works correctly for your data.\n",
      "\n",
      "================================================================================\n",
      "FINAL VERDICT\n",
      "================================================================================\n",
      "\n",
      "Your fixation visualization script's accuracy calculation:\n",
      "  1. Uses ACC column when available ✓\n",
      "  2. Falls back to RESPONSE == correct_response ✓\n",
      "  3. Uses string comparison for fallback ⚠ (could be issue with floats)\n",
      "\n",
      "If ACC column exists and is correct, the script is fine.\n",
      "If you rely on the RESPONSE fallback, consider using numeric comparison.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verification script to check if the fixation visualization script\n",
    "calculates accuracy correctly\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Config\n",
    "DATA_DIR = Path(\"data files\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FIXATION VISUALIZATION ACCURACY VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load sample data\n",
    "print(\"\\nLoading sample data...\")\n",
    "for encoding in [\"utf-16\", \"utf-8\", \"latin1\"]:\n",
    "    try:\n",
    "        tr_first = pd.read_csv(DATA_DIR / \"trialdata_testing_firstdisplayonly.tsv\", \n",
    "                              sep=\"\\t\", encoding=encoding, nrows=50)\n",
    "        print(f\"  ✓ Loaded with {encoding} encoding\")\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING ACCURACY CALCULATION IN YOUR SCRIPT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nYour script does this for each trial:\")\n",
    "print(\"\"\"\n",
    "    acc = r.get(\"ACC\")\n",
    "    if pd.isna(acc):\n",
    "        # fall back to RESPONSE ?= correct_response\n",
    "        resp = str(r.get(\"RESPONSE\", \"\")).strip().lower()\n",
    "        cor  = str(r.get(\"correct_response\", \"\")).strip().lower()\n",
    "        if resp and cor:\n",
    "            acc_flags.append(int(resp == cor))\n",
    "    else:\n",
    "        try:\n",
    "            acc_flags.append(int(bool(int(acc))))\n",
    "        except Exception:\n",
    "            acc_flags.append(int(bool(acc)))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nLet's test this logic with sample data:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Test the logic\n",
    "sample_trials = tr_first.head(10)\n",
    "\n",
    "for idx, row in sample_trials.iterrows():\n",
    "    acc = row.get(\"ACC\")\n",
    "    response = row.get(\"RESPONSE\")\n",
    "    correct_response = row.get(\"correct_response\")\n",
    "    \n",
    "    # Replicate the script's logic\n",
    "    if pd.isna(acc):\n",
    "        resp = str(response).strip().lower() if pd.notna(response) else \"\"\n",
    "        cor = str(correct_response).strip().lower() if pd.notna(correct_response) else \"\"\n",
    "        if resp and cor:\n",
    "            computed_acc = int(resp == cor)\n",
    "            method = \"RESPONSE == correct_response\"\n",
    "        else:\n",
    "            computed_acc = None\n",
    "            method = \"Missing data\"\n",
    "    else:\n",
    "        try:\n",
    "            computed_acc = int(bool(int(acc)))\n",
    "            method = \"ACC column (converted to int/bool/int)\"\n",
    "        except:\n",
    "            computed_acc = int(bool(acc))\n",
    "            method = \"ACC column (converted to bool/int)\"\n",
    "    \n",
    "    print(f\"\\nTrial {idx}:\")\n",
    "    print(f\"  ACC: {acc}\")\n",
    "    print(f\"  RESPONSE: {response}, correct_response: {correct_response}\")\n",
    "    print(f\"  Computed accuracy: {computed_acc} (using {method})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POTENTIAL ISSUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. ACC Column Usage:\")\n",
    "print(\"   Your script uses: int(bool(int(acc)))\")\n",
    "print(\"   This means:\")\n",
    "print(\"     - ACC = 0 → int(bool(int(0))) = int(bool(0)) = int(False) = 0 ✓\")\n",
    "print(\"     - ACC = 1 → int(bool(int(1))) = int(bool(1)) = int(True) = 1 ✓\")\n",
    "print(\"   ✓ This is CORRECT for binary ACC values\")\n",
    "\n",
    "print(\"\\n2. Response Code Fallback:\")\n",
    "print(\"   If ACC is missing, it uses: int(RESPONSE == correct_response)\")\n",
    "print(\"   Question: Are RESPONSE and correct_response the same type?\")\n",
    "\n",
    "# Check if RESPONSE and correct_response have same type/values\n",
    "if \"RESPONSE\" in tr_first.columns and \"correct_response\" in tr_first.columns:\n",
    "    resp_values = tr_first[\"RESPONSE\"].dropna().unique()\n",
    "    cor_values = tr_first[\"correct_response\"].dropna().unique()\n",
    "    \n",
    "    print(f\"\\n   RESPONSE unique values: {resp_values}\")\n",
    "    print(f\"   correct_response unique values: {cor_values}\")\n",
    "    \n",
    "    # Check if they're numeric\n",
    "    try:\n",
    "        resp_numeric = pd.to_numeric(tr_first[\"RESPONSE\"], errors='coerce').dropna().unique()\n",
    "        cor_numeric = pd.to_numeric(tr_first[\"correct_response\"], errors='coerce').dropna().unique()\n",
    "        print(f\"\\n   RESPONSE as numeric: {resp_numeric}\")\n",
    "        print(f\"   correct_response as numeric: {cor_numeric}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n3. String Comparison Issue:\")\n",
    "print(\"   Your script converts both to strings and compares:\")\n",
    "print(\"     resp = str(r.get('RESPONSE', '')).strip().lower()\")\n",
    "print(\"     cor = str(r.get('correct_response', '')).strip().lower()\")\n",
    "print(\"     acc = int(resp == cor)\")\n",
    "print(\"\\n   ⚠ POTENTIAL PROBLEM:\")\n",
    "print(\"     - If RESPONSE = 6 (number) and correct_response = 6 (number)\")\n",
    "print(\"     - str(6) = '6' and str(6) = '6'\")\n",
    "print(\"     - '6' == '6' → True ✓\")\n",
    "print(\"\\n     - If RESPONSE = 6.0 (float) and correct_response = 6 (int)\")\n",
    "print(\"     - str(6.0) = '6.0' and str(6) = '6'\")\n",
    "print(\"     - '6.0' == '6' → False ❌ MISMATCH!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTo be safe, compare as NUMBERS, not strings:\")\n",
    "print(\"\"\"\n",
    "    # Instead of:\n",
    "    resp = str(r.get(\"RESPONSE\", \"\")).strip().lower()\n",
    "    cor = str(r.get(\"correct_response\", \"\")).strip().lower()\n",
    "    if resp and cor:\n",
    "        acc_flags.append(int(resp == cor))\n",
    "    \n",
    "    # Use:\n",
    "    resp = pd.to_numeric(r.get(\"RESPONSE\"), errors='coerce')\n",
    "    cor = pd.to_numeric(r.get(\"correct_response\"), errors='coerce')\n",
    "    if pd.notna(resp) and pd.notna(cor):\n",
    "        acc_flags.append(int(resp == cor))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION WITH ACTUAL DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if there's actual mismatch\n",
    "if \"ACC\" in tr_first.columns and \"RESPONSE\" in tr_first.columns and \"correct_response\" in tr_first.columns:\n",
    "    # Compare string method vs numeric method\n",
    "    mismatches = []\n",
    "    \n",
    "    for idx, row in tr_first.iterrows():\n",
    "        if pd.notna(row.get(\"RESPONSE\")) and pd.notna(row.get(\"correct_response\")):\n",
    "            # String method (your script)\n",
    "            resp_str = str(row.get(\"RESPONSE\")).strip().lower()\n",
    "            cor_str = str(row.get(\"correct_response\")).strip().lower()\n",
    "            acc_string = int(resp_str == cor_str)\n",
    "            \n",
    "            # Numeric method (recommended)\n",
    "            resp_num = pd.to_numeric(row.get(\"RESPONSE\"), errors='coerce')\n",
    "            cor_num = pd.to_numeric(row.get(\"correct_response\"), errors='coerce')\n",
    "            acc_numeric = int(resp_num == cor_num) if pd.notna(resp_num) and pd.notna(cor_num) else None\n",
    "            \n",
    "            if acc_string != acc_numeric:\n",
    "                mismatches.append({\n",
    "                    \"row\": idx,\n",
    "                    \"RESPONSE\": row.get(\"RESPONSE\"),\n",
    "                    \"correct_response\": row.get(\"correct_response\"),\n",
    "                    \"string_acc\": acc_string,\n",
    "                    \"numeric_acc\": acc_numeric\n",
    "                })\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"\\n⚠ Found {len(mismatches)} potential mismatches between string and numeric comparison!\")\n",
    "        print(\"\\nSample mismatches:\")\n",
    "        for m in mismatches[:5]:\n",
    "            print(f\"  Row {m['row']}: RESPONSE={m['RESPONSE']}, correct={m['correct_response']}\")\n",
    "            print(f\"    String method: {m['string_acc']}, Numeric method: {m['numeric_acc']}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No mismatches found! String comparison works correctly for your data.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL VERDICT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nYour fixation visualization script's accuracy calculation:\")\n",
    "print(\"  1. Uses ACC column when available ✓\")\n",
    "print(\"  2. Falls back to RESPONSE == correct_response ✓\")\n",
    "print(\"  3. Uses string comparison for fallback ⚠ (could be issue with floats)\")\n",
    "print(\"\\nIf ACC column exists and is correct, the script is fine.\")\n",
    "print(\"If you rely on the RESPONSE fallback, consider using numeric comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdgaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
