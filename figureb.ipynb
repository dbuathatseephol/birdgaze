{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb73f0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUALITATIVE TRIAL ANALYSIS\n",
      "Comparing Top vs Bottom Performing Trials\n",
      "============================================================\n",
      "âœ“ Loaded Training 1: 1364 trials\n",
      "âœ“ Loaded Training 2: 2320 trials\n",
      "âœ“ Loaded Testing: 2304 trials\n",
      "\n",
      "ðŸ“Š Generating qualitative comparison figures...\n",
      "Note: Actual bird images will be displayed as backgrounds\n",
      "\n",
      "ðŸ“Š Analyzing full viewing condition...\n",
      "  âœ“ Saved: qualitative_analysis/qualitative_full_viewing.pdf\n",
      "  âœ“ Metrics saved: qualitative_analysis/metrics_full_viewing.pdf\n",
      "\n",
      "ðŸ“Š Analyzing central viewing condition...\n",
      "  âœ“ Saved: qualitative_analysis/qualitative_central_viewing.pdf\n",
      "  âœ“ Metrics saved: qualitative_analysis/metrics_central_viewing.pdf\n",
      "\n",
      "ðŸ“Š Analyzing peripheral viewing condition...\n",
      "  âœ“ Saved: qualitative_analysis/qualitative_peripheral_viewing.pdf\n",
      "  âœ“ Metrics saved: qualitative_analysis/metrics_peripheral_viewing.pdf\n",
      "\n",
      "ðŸ“ˆ Creating overall pattern summary...\n",
      "\n",
      "âœ“ Overall summary saved: qualitative_analysis/overall_pattern_summary.png\n",
      "\n",
      "============================================================\n",
      "âœ… Analysis complete!\n",
      "ðŸ“ All outputs saved to: qualitative_analysis/\n",
      "\n",
      "The figures will show:\n",
      "â€¢ Actual bird images as backgrounds\n",
      "â€¢ Coral-colored fixation points overlaid\n",
      "â€¢ Jet colormap heatmaps (semi-transparent)\n",
      "â€¢ No viewing condition masks\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class QualitativeTrialAnalyzer:\n",
    "    \"\"\"Analyze and visualize top vs bottom performing trials for each viewing condition\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: Path = Path(\".\")):\n",
    "        self.data_path = data_path\n",
    "        self.screen_size = (1024, 768)\n",
    "        self.image_size = 620\n",
    "        \n",
    "        # Image positioning on screen\n",
    "        self.box = (\n",
    "            self.screen_size[0] // 2 - 310,  # left\n",
    "            self.screen_size[1] // 2 - 310,  # top\n",
    "            self.screen_size[0] // 2 + 310,  # right\n",
    "            self.screen_size[1] // 2 + 310   # bottom\n",
    "        )\n",
    "        \n",
    "        # Load all datasets\n",
    "        self.load_all_data()\n",
    "        \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load data from all phases\"\"\"\n",
    "        self.datasets = {}\n",
    "        \n",
    "        # Define dataset paths - matching your existing structure\n",
    "        dataset_configs = {\n",
    "            \"Training 1\": {\n",
    "                \"json\": \"Training 1/training1.json\",\n",
    "                \"images\": \"Training 1/training1_images\",\n",
    "                \"dual_images\": False\n",
    "            },\n",
    "            \"Training 2\": {\n",
    "                \"json\": \"Training 2/training2.json\",\n",
    "                \"images\": \"Training 2/training2_images\",\n",
    "                \"dual_images\": True\n",
    "            },\n",
    "            \"Testing\": {\n",
    "                \"json\": \"Testing/testing.json\",\n",
    "                \"images\": \"Testing/testing_images\",\n",
    "                \"dual_images\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for name, config in dataset_configs.items():\n",
    "            json_path = self.data_path / config[\"json\"]\n",
    "            if json_path.exists():\n",
    "                with open(json_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                self.datasets[name] = {\n",
    "                    \"data\": data,\n",
    "                    \"image_folder\": self.data_path / config[\"images\"],\n",
    "                    \"dual_images\": config[\"dual_images\"]\n",
    "                }\n",
    "                print(f\"âœ“ Loaded {name}: {len(data)} trials\")\n",
    "            else:\n",
    "                print(f\"âš  {name} not found at {json_path}\")\n",
    "    \n",
    "    def load_image(self, image_folder: Path, image_name: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Load an image file and return as numpy array\"\"\"\n",
    "        image_path = image_folder / image_name\n",
    "        if image_path.exists():\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            return np.array(img)\n",
    "        else:\n",
    "            print(f\"âš  Image not found: {image_path}\")\n",
    "            return None\n",
    "    \n",
    "    def get_trial_performance(self, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Calculate performance metrics for each unique trial configuration\"\"\"\n",
    "        if dataset_name not in self.datasets:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        data = self.datasets[dataset_name][\"data\"]\n",
    "        \n",
    "        # Group trials by unique configurations\n",
    "        if dataset_name == \"Training 1\":\n",
    "            # Group by single image\n",
    "            trial_groups = {}\n",
    "            for trial in data:\n",
    "                img = trial.get(\"first_image\")\n",
    "                if img:\n",
    "                    if img not in trial_groups:\n",
    "                        trial_groups[img] = []\n",
    "                    trial_groups[img].append(trial)\n",
    "            \n",
    "            performance = []\n",
    "            for img, trials in trial_groups.items():\n",
    "                correct = sum(1 for t in trials if self.is_correct(t))\n",
    "                total = len(trials)\n",
    "                performance.append({\n",
    "                    \"trial_id\": img,\n",
    "                    \"first_image\": img,\n",
    "                    \"accuracy\": correct / total if total > 0 else 0,\n",
    "                    \"n_responses\": total,\n",
    "                    \"correct_count\": correct,\n",
    "                    \"trials\": trials\n",
    "                })\n",
    "                \n",
    "        elif dataset_name in [\"Training 2\", \"Testing\"]:\n",
    "            # Group by image pair (and viewing condition for Testing)\n",
    "            trial_groups = {}\n",
    "            for trial in data:\n",
    "                first = trial.get(\"first_image\")\n",
    "                second = trial.get(\"second_image\")\n",
    "                \n",
    "                if dataset_name == \"Testing\":\n",
    "                    viewing = trial.get(\"viewing_condition\", \"unknown\")\n",
    "                    key = (first, second, viewing)\n",
    "                else:\n",
    "                    key = (first, second)\n",
    "                \n",
    "                if first and second:\n",
    "                    if key not in trial_groups:\n",
    "                        trial_groups[key] = []\n",
    "                    trial_groups[key].append(trial)\n",
    "            \n",
    "            performance = []\n",
    "            for key, trials in trial_groups.items():\n",
    "                correct = sum(1 for t in trials if self.is_correct(t))\n",
    "                total = len(trials)\n",
    "                \n",
    "                entry = {\n",
    "                    \"trial_id\": str(key),\n",
    "                    \"first_image\": key[0],\n",
    "                    \"second_image\": key[1],\n",
    "                    \"accuracy\": correct / total if total > 0 else 0,\n",
    "                    \"n_responses\": total,\n",
    "                    \"correct_count\": correct,\n",
    "                    \"trials\": trials\n",
    "                }\n",
    "                \n",
    "                if dataset_name == \"Testing\" and len(key) > 2:\n",
    "                    entry[\"viewing_condition\"] = key[2]\n",
    "                \n",
    "                performance.append(entry)\n",
    "        \n",
    "        return pd.DataFrame(performance)\n",
    "    \n",
    "    def is_correct(self, trial: Dict) -> bool:\n",
    "        \"\"\"Determine if a trial response was correct\"\"\"\n",
    "        # Check for explicit accuracy flag\n",
    "        if \"acc\" in trial:\n",
    "            v = trial[\"acc\"]\n",
    "            if isinstance(v, bool):\n",
    "                return v\n",
    "            if isinstance(v, (int, float)):\n",
    "                return bool(v)\n",
    "            if isinstance(v, str):\n",
    "                sv = str(v).strip().lower()\n",
    "                if sv in {\"1\", \"true\", \"correct\", \"right\"}:\n",
    "                    return True\n",
    "                if sv in {\"0\", \"false\", \"incorrect\", \"wrong\"}:\n",
    "                    return False\n",
    "        \n",
    "        # Fallback: compare answer to correct response\n",
    "        answer = str(trial.get(\"subj_answer\", \"\")).strip().lower()\n",
    "        correct = str(trial.get(\"correct_response\", \"\")).strip().lower()\n",
    "        if answer and correct:\n",
    "            return answer == correct\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_top_bottom_trials(self, dataset_name: str, viewing_condition: Optional[str] = None, n: int = 3):\n",
    "        \"\"\"Get top and bottom n trial configurations by success rate\"\"\"\n",
    "        df = self.get_trial_performance(dataset_name)\n",
    "        \n",
    "        if df.empty:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # Filter by viewing condition if specified\n",
    "        if viewing_condition and \"viewing_condition\" in df.columns:\n",
    "            df = df[df[\"viewing_condition\"] == viewing_condition]\n",
    "        \n",
    "        # Sort by accuracy\n",
    "        df = df.sort_values(\"accuracy\", ascending=False)\n",
    "        \n",
    "        top = df.head(n)\n",
    "        bottom = df.tail(n)\n",
    "        \n",
    "        return top, bottom\n",
    "    \n",
    "    def collect_fixations(self, trials: List[Dict], image_key: str = \"first_image\") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Collect all fixations for given trials\"\"\"\n",
    "        l, t, r, b = self.box\n",
    "        xs_all, ys_all, durations_all = [], [], []\n",
    "        \n",
    "        for trial in trials:\n",
    "            xs = np.asarray(trial.get(\"fix_x\", []), dtype=float)\n",
    "            ys = np.asarray(trial.get(\"fix_y\", []), dtype=float)\n",
    "            durations = np.asarray(trial.get(\"fix_dur\", np.ones_like(xs) * 100), dtype=float)\n",
    "            \n",
    "            # Handle image order for dual-image trials\n",
    "            order = np.asarray(trial.get(\"fix_index\", np.arange(1, len(xs) + 1)), dtype=float)\n",
    "            idx = trial.get(\"test_image_fixation_idx\")\n",
    "            \n",
    "            if idx is not None:\n",
    "                if image_key == \"first_image\":\n",
    "                    mask = order < idx\n",
    "                else:\n",
    "                    mask = order >= idx\n",
    "                xs = xs[mask]\n",
    "                ys = ys[mask]\n",
    "                durations = durations[mask]\n",
    "            \n",
    "            # Filter to image boundaries\n",
    "            mask = (xs >= l) & (xs <= r) & (ys >= t) & (ys <= b)\n",
    "            xs_all.extend(xs[mask])\n",
    "            ys_all.extend(ys[mask])\n",
    "            durations_all.extend(durations[mask])\n",
    "        \n",
    "        return np.array(xs_all), np.array(ys_all), np.array(durations_all)\n",
    "    \n",
    "    def create_heatmap(self, xs, ys, durations=None):\n",
    "        \"\"\"Create a fixation density heatmap\"\"\"\n",
    "        if len(xs) == 0:\n",
    "            return np.zeros((150, 150))\n",
    "        \n",
    "        l, t, r, b = self.box\n",
    "        \n",
    "        # Create 2D histogram\n",
    "        H, xedges, yedges = np.histogram2d(\n",
    "            xs, ys, \n",
    "            bins=[np.linspace(l, r, 150), np.linspace(t, b, 150)],\n",
    "            weights=durations if durations is not None else None\n",
    "        )\n",
    "        \n",
    "        # Apply gaussian smoothing\n",
    "        H = gaussian_filter(H, sigma=15)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    def calculate_pattern_metrics(self, xs, ys, durations):\n",
    "        \"\"\"Calculate metrics that explain fixation patterns\"\"\"\n",
    "        if len(xs) == 0:\n",
    "            return {\n",
    "                'n_fixations': 0,\n",
    "                'total_duration': 0,\n",
    "                'avg_duration': 0,\n",
    "                'spatial_spread': 0,\n",
    "                'center_bias': 0,\n",
    "                'coverage': 0\n",
    "            }\n",
    "        \n",
    "        # Convert to image coordinates\n",
    "        l, t, r, b = self.box\n",
    "        xs_img = xs - l\n",
    "        ys_img = ys - t\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'n_fixations': len(xs),\n",
    "            'total_duration': np.sum(durations) if durations is not None else len(xs) * 100,\n",
    "            'avg_duration': np.mean(durations) if durations is not None else 100,\n",
    "            'spatial_spread': np.std(xs_img) + np.std(ys_img),\n",
    "            'center_bias': np.mean(np.sqrt((xs_img - 310)**2 + (ys_img - 310)**2)),\n",
    "            'coverage': len(set([(int(x/20), int(y/20)) for x, y in zip(xs_img, ys_img)])) / 961  # 31x31 grid\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_trial_with_image(self, ax, trial_config, position: str, dataset_name: str, \n",
    "                              image_folder: Path, image_key: str = \"first_image\", \n",
    "                              show_image_num: int = 1):\n",
    "        \"\"\"Plot heatmap and fixations with actual image background\"\"\"\n",
    "        trials = trial_config[\"trials\"]\n",
    "        \n",
    "        # Determine which image to show\n",
    "        if image_key == \"first_image\":\n",
    "            image_name = trial_config[\"first_image\"]\n",
    "        else:\n",
    "            image_name = trial_config.get(\"second_image\", trial_config[\"first_image\"])\n",
    "        \n",
    "        # Load and display the image\n",
    "        l, t, r, b = self.box\n",
    "        img = self.load_image(image_folder, image_name)\n",
    "        if img is not None:\n",
    "            ax.imshow(img, extent=(l, r, b, t), aspect='auto')\n",
    "        \n",
    "        # Collect fixations\n",
    "        xs, ys, durations = self.collect_fixations(trials, image_key)\n",
    "        \n",
    "        # Create and overlay heatmap\n",
    "        if len(xs) > 0:\n",
    "            H = self.create_heatmap(xs, ys, durations)\n",
    "            extent = [l, r, b, t]\n",
    "            im = ax.imshow(H.T, extent=extent, origin=\"upper\", cmap=\"jet\", \n",
    "                          alpha=0.6, interpolation=\"bilinear\")\n",
    "            \n",
    "            # Add fixation points (coral color as in your original code)\n",
    "            ax.scatter(xs, ys, s=30, c='#FF6B6B', alpha=0.5, \n",
    "                      edgecolors='white', linewidth=0.8)\n",
    "        \n",
    "        # Add image boundary\n",
    "        rect = patches.Rectangle((l, t), r-l, b-t, linewidth=2, \n",
    "                                edgecolor='black', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_pattern_metrics(xs, ys, durations)\n",
    "        \n",
    "        # Set title\n",
    "        color = 'green' if position == 'top' else 'red'\n",
    "        \n",
    "        if dataset_name == \"Training 1\":\n",
    "            title = f\"{image_name[:30]}\"\n",
    "        else:\n",
    "            title = f\"Image {show_image_num}: {image_name[:20]}\"\n",
    "        \n",
    "        title += f\"\\nAcc: {trial_config['accuracy']:.1%} (N={trial_config['n_responses']})\"\n",
    "        title += f\"\\nFix: {metrics['n_fixations']} | Spread: {metrics['spatial_spread']:.0f}px\"\n",
    "        \n",
    "        ax.set_title(title, fontsize=9, color=color)\n",
    "        \n",
    "        ax.set_xlim(0, self.screen_size[0])\n",
    "        ax.set_ylim(self.screen_size[1], 0)\n",
    "        ax.set_xlabel(\"x (screen px)\", fontsize=8)\n",
    "        ax.set_ylabel(\"y (screen px)\", fontsize=8)\n",
    "        ax.tick_params(labelsize=7)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def create_qualitative_figures(self, output_dir: Path = Path(\"qualitative_analysis\")):\n",
    "        \"\"\"Create comprehensive qualitative comparison figures\"\"\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Focus on Testing dataset with viewing conditions\n",
    "        if \"Testing\" not in self.datasets:\n",
    "            print(\"âš  Testing dataset not found!\")\n",
    "            return\n",
    "        \n",
    "        dataset_info = self.datasets[\"Testing\"]\n",
    "        image_folder = dataset_info[\"image_folder\"]\n",
    "        \n",
    "        viewing_conditions = ['full', 'central', 'peripheral']\n",
    "        \n",
    "        for condition in viewing_conditions:\n",
    "            print(f\"\\nðŸ“Š Analyzing {condition} viewing condition...\")\n",
    "            \n",
    "            # Get top and bottom trials\n",
    "            top_trials, bottom_trials = self.get_top_bottom_trials(\"Testing\", condition, n=3)\n",
    "            \n",
    "            if top_trials.empty or bottom_trials.empty:\n",
    "                print(f\"  âš  Not enough data for {condition}\")\n",
    "                continue\n",
    "            \n",
    "            # Create figure for this viewing condition\n",
    "            fig = plt.figure(figsize=(20, 14))\n",
    "            fig.suptitle(f'Testing Phase - {condition.upper()} Viewing Condition\\n'\n",
    "                        f'Top 3 Trials (High Accuracy) vs Bottom 3 Trials (Low Accuracy)',\n",
    "                        fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Create grid: 2 rows (top/bottom), 6 columns (3 trials Ã— 2 images each)\n",
    "            gs = GridSpec(2, 6, figure=fig, hspace=0.3, wspace=0.15)\n",
    "            \n",
    "            all_metrics = {'top': [], 'bottom': []}\n",
    "            \n",
    "            # Plot top trials\n",
    "            for i, (_, trial) in enumerate(top_trials.iterrows()):\n",
    "                # First image\n",
    "                ax1 = fig.add_subplot(gs[0, i*2])\n",
    "                m1 = self.plot_trial_with_image(ax1, trial, 'top', \"Testing\", \n",
    "                                                image_folder, \"first_image\", \n",
    "                                                show_image_num=1)\n",
    "                \n",
    "                # Second image\n",
    "                ax2 = fig.add_subplot(gs[0, i*2 + 1])\n",
    "                m2 = self.plot_trial_with_image(ax2, trial, 'top', \"Testing\", \n",
    "                                                image_folder, \"second_image\", \n",
    "                                                show_image_num=2)\n",
    "                \n",
    "                all_metrics['top'].append({'first': m1, 'second': m2})\n",
    "            \n",
    "            # Plot bottom trials\n",
    "            for i, (_, trial) in enumerate(bottom_trials.iterrows()):\n",
    "                # First image\n",
    "                ax1 = fig.add_subplot(gs[1, i*2])\n",
    "                m1 = self.plot_trial_with_image(ax1, trial, 'bottom', \"Testing\", \n",
    "                                                image_folder, \"first_image\", \n",
    "                                                show_image_num=1)\n",
    "                \n",
    "                # Second image\n",
    "                ax2 = fig.add_subplot(gs[1, i*2 + 1])\n",
    "                m2 = self.plot_trial_with_image(ax2, trial, 'bottom', \"Testing\", \n",
    "                                                image_folder, \"second_image\", \n",
    "                                                show_image_num=2)\n",
    "                \n",
    "                all_metrics['bottom'].append({'first': m1, 'second': m2})\n",
    "            \n",
    "            # Add colorbar for heatmaps\n",
    "            cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "            sm = plt.cm.ScalarMappable(cmap='jet', norm=plt.Normalize(vmin=0, vmax=1))\n",
    "            sm.set_array([])\n",
    "            cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "            cbar.set_label('Fixation Density', fontsize=10)\n",
    "            \n",
    "            # Save figure\n",
    "            output_file = output_dir / f\"qualitative_{condition}_viewing.pdf\"\n",
    "            plt.savefig(output_file, bbox_inches='tight', dpi=150)\n",
    "            plt.close()\n",
    "            print(f\"  âœ“ Saved: {output_file}\")\n",
    "            \n",
    "            # Create metrics summary\n",
    "            self.create_metrics_summary(condition, all_metrics, top_trials, bottom_trials, output_dir)\n",
    "    \n",
    "    def create_metrics_summary(self, condition: str, metrics_data: Dict, \n",
    "                              top_trials: pd.DataFrame, bottom_trials: pd.DataFrame, \n",
    "                              output_dir: Path):\n",
    "        \"\"\"Create a summary figure showing pattern metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle(f'{condition.upper()} Viewing - Pattern Analysis\\n'\n",
    "                    f'Values that Explain the Overall Pattern',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Extract metrics for comparison\n",
    "        metric_names = ['n_fixations', 'avg_duration', 'spatial_spread', \n",
    "                       'center_bias', 'coverage', 'total_duration']\n",
    "        metric_labels = ['Number of\\nFixations', 'Average\\nDuration (ms)', \n",
    "                        'Spatial\\nSpread (px)', 'Center\\nBias (px)', \n",
    "                        'Coverage\\n(%)', 'Total\\nDuration (ms)']\n",
    "        \n",
    "        for idx, (metric, label) in enumerate(zip(metric_names, metric_labels)):\n",
    "            ax = axes[idx // 3, idx % 3]\n",
    "            \n",
    "            # Collect values\n",
    "            top_vals = []\n",
    "            bottom_vals = []\n",
    "            \n",
    "            for trial_metrics in metrics_data['top']:\n",
    "                val1 = trial_metrics['first'].get(metric, 0)\n",
    "                val2 = trial_metrics['second'].get(metric, 0)\n",
    "                top_vals.extend([val1, val2])\n",
    "            \n",
    "            for trial_metrics in metrics_data['bottom']:\n",
    "                val1 = trial_metrics['first'].get(metric, 0)\n",
    "                val2 = trial_metrics['second'].get(metric, 0)\n",
    "                bottom_vals.extend([val1, val2])\n",
    "            \n",
    "            if metric == 'coverage':\n",
    "                top_vals = [v * 100 for v in top_vals]\n",
    "                bottom_vals = [v * 100 for v in bottom_vals]\n",
    "            \n",
    "            # Create bar plot\n",
    "            x = ['Top\\nTrials', 'Bottom\\nTrials']\n",
    "            y = [np.mean(top_vals) if top_vals else 0, \n",
    "                 np.mean(bottom_vals) if bottom_vals else 0]\n",
    "            err = [np.std(top_vals) if top_vals else 0, \n",
    "                   np.std(bottom_vals) if bottom_vals else 0]\n",
    "            \n",
    "            bars = ax.bar(x, y, yerr=err, color=['green', 'red'], \n",
    "                         alpha=0.7, capsize=10)\n",
    "            \n",
    "            ax.set_ylabel(label, fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, val in zip(bars, y):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{val:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            # Add difference annotation\n",
    "            if y[1] != 0:\n",
    "                diff = y[0] - y[1]\n",
    "                diff_pct = (diff / y[1]) * 100\n",
    "                ax.set_title(f'Î” = {diff:.0f} ({diff_pct:+.0f}%)', fontsize=9)\n",
    "        \n",
    "        # Add summary text\n",
    "        avg_acc_top = top_trials['accuracy'].mean()\n",
    "        avg_acc_bottom = bottom_trials['accuracy'].mean()\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "Key Patterns for {condition.upper()} viewing:\n",
    "â€¢ Top trials (avg accuracy: {avg_acc_top:.1%}) show most difference in fixation patterns\n",
    "â€¢ Bottom trials (avg accuracy: {avg_acc_bottom:.1%}) demonstrate inefficient scanning\n",
    "â€¢ Success rate difference: {(avg_acc_top - avg_acc_bottom):.1%}\n",
    "â€¢ These metrics explain the overall pattern between successful and unsuccessful trials\n",
    "        \"\"\"\n",
    "        \n",
    "        fig.text(0.5, 0.02, summary, ha='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.08, 1, 0.96])\n",
    "        \n",
    "        output_file = output_dir / f\"metrics_{condition}_viewing.pdf\"\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"  âœ“ Metrics saved: {output_file}\")\n",
    "    \n",
    "    def create_overall_summary(self, output_dir: Path = Path(\"qualitative_analysis\")):\n",
    "        \"\"\"Create an overall summary across all conditions\"\"\"\n",
    "        if \"Testing\" not in self.datasets:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        fig.suptitle('Pattern Discovery: Key Differences Between Top and Bottom Trials',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        viewing_conditions = ['full', 'central', 'peripheral']\n",
    "        metrics_to_show = ['n_fixations', 'spatial_spread', 'center_bias', 'coverage']\n",
    "        metric_labels = ['Fixation Count', 'Spatial Spread (px)', 'Center Bias (px)', 'Coverage (%)']\n",
    "        \n",
    "        for i, condition in enumerate(viewing_conditions):\n",
    "            # Get trials\n",
    "            top_trials, bottom_trials = self.get_top_bottom_trials(\"Testing\", condition, n=3)\n",
    "            \n",
    "            if top_trials.empty or bottom_trials.empty:\n",
    "                continue\n",
    "            \n",
    "            for j, (metric, label) in enumerate(zip(metrics_to_show, metric_labels)):\n",
    "                ax = axes[i, j]\n",
    "                \n",
    "                # Collect metrics for all trials\n",
    "                top_vals = []\n",
    "                bottom_vals = []\n",
    "                \n",
    "                for _, trial in top_trials.iterrows():\n",
    "                    xs1, ys1, dur1 = self.collect_fixations(trial[\"trials\"], \"first_image\")\n",
    "                    xs2, ys2, dur2 = self.collect_fixations(trial[\"trials\"], \"second_image\")\n",
    "                    \n",
    "                    m1 = self.calculate_pattern_metrics(xs1, ys1, dur1)\n",
    "                    m2 = self.calculate_pattern_metrics(xs2, ys2, dur2)\n",
    "                    \n",
    "                    val = (m1[metric] + m2[metric]) / 2\n",
    "                    if metric == 'coverage':\n",
    "                        val *= 100\n",
    "                    top_vals.append(val)\n",
    "                \n",
    "                for _, trial in bottom_trials.iterrows():\n",
    "                    xs1, ys1, dur1 = self.collect_fixations(trial[\"trials\"], \"first_image\")\n",
    "                    xs2, ys2, dur2 = self.collect_fixations(trial[\"trials\"], \"second_image\")\n",
    "                    \n",
    "                    m1 = self.calculate_pattern_metrics(xs1, ys1, dur1)\n",
    "                    m2 = self.calculate_pattern_metrics(xs2, ys2, dur2)\n",
    "                    \n",
    "                    val = (m1[metric] + m2[metric]) / 2\n",
    "                    if metric == 'coverage':\n",
    "                        val *= 100\n",
    "                    bottom_vals.append(val)\n",
    "                \n",
    "                # Create box plot\n",
    "                bp = ax.boxplot([top_vals, bottom_vals], \n",
    "                               tick_labels=['Top', 'Bottom'],\n",
    "                               patch_artist=True)\n",
    "                \n",
    "                # Color the boxes\n",
    "                bp['boxes'][0].set_facecolor('lightgreen')\n",
    "                bp['boxes'][1].set_facecolor('lightcoral')\n",
    "                \n",
    "                if i == 0:\n",
    "                    ax.set_title(label, fontsize=10, fontweight='bold')\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(f'{condition.capitalize()}', fontsize=10, fontweight='bold')\n",
    "                \n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.tick_params(labelsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_file = output_dir / \"overall_pattern_summary.png\"\n",
    "        fig.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"\\nâœ“ Overall summary saved: {output_file}\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUALITATIVE TRIAL ANALYSIS\")\n",
    "    print(\"Comparing Top vs Bottom Performing Trials\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = QualitativeTrialAnalyzer()\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"qualitative_analysis\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Generating qualitative comparison figures...\")\n",
    "    print(\"Note: Actual bird images will be displayed as backgrounds\")\n",
    "    analyzer.create_qualitative_figures(output_dir)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Creating overall pattern summary...\")\n",
    "    analyzer.create_overall_summary(output_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… Analysis complete!\")\n",
    "    print(f\"ðŸ“ All outputs saved to: {output_dir}/\")\n",
    "    print(\"\\nThe figures will show:\")\n",
    "    print(\"â€¢ Actual bird images as backgrounds\")\n",
    "    print(\"â€¢ Coral-colored fixation points overlaid\")\n",
    "    print(\"â€¢ Jet colormap heatmaps (semi-transparent)\")\n",
    "    print(\"â€¢ No viewing condition masks\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c8111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUALITATIVE TRIAL ANALYSIS\n",
      "Comparing Top vs Bottom Performing Trials\n",
      "============================================================\n",
      "âœ“ Loaded Training 1: 1364 trials\n",
      "âœ“ Loaded Training 2: 2320 trials\n",
      "âœ“ Loaded Testing: 2304 trials\n",
      "\n",
      "ðŸ“Š Generating qualitative comparison figures...\n",
      "Note: Actual bird images will be displayed as backgrounds\n",
      "\n",
      "ðŸ“Š Analyzing full viewing condition...\n",
      "  âœ“ Saved: qualitative_analysis/qualitative_full_viewing.pdf\n",
      "  âœ“ Metrics saved: qualitative_analysis/metrics_full_viewing.pdf\n",
      "\n",
      "ðŸ“Š Analyzing central viewing condition...\n",
      "  âœ“ Saved: qualitative_analysis/qualitative_central_viewing.pdf\n",
      "  âœ“ Metrics saved: qualitative_analysis/metrics_central_viewing.pdf\n",
      "\n",
      "ðŸ“Š Analyzing peripheral viewing condition...\n",
      "  âœ“ Saved: qualitative_analysis/qualitative_peripheral_viewing.pdf\n",
      "  âœ“ Metrics saved: qualitative_analysis/metrics_peripheral_viewing.pdf\n",
      "\n",
      "ðŸ“ˆ Creating overall pattern summary...\n",
      "\n",
      "âœ“ Overall summary saved: qualitative_analysis/overall_pattern_summary.png\n",
      "\n",
      "============================================================\n",
      "âœ… Analysis complete!\n",
      "ðŸ“ All outputs saved to: qualitative_analysis/\n",
      "\n",
      "The figures will show:\n",
      "â€¢ Trial numbers in titles (not image names)\n",
      "â€¢ Coral-colored fixation points overlaid\n",
      "â€¢ Jet colormap heatmaps (semi-transparent)\n",
      "â€¢ Gaze Dispersion in place of Spread\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# file: qualitative_trial_analyzer.py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class QualitativeTrialAnalyzer:\n",
    "    \"\"\"Analyze and visualize top vs bottom performing trials for each viewing condition\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: Path = Path(\".\")):\n",
    "        self.data_path = data_path\n",
    "        self.screen_size = (1024, 768)\n",
    "        self.image_size = 620\n",
    "        \n",
    "        # Image positioning on screen\n",
    "        self.box = (\n",
    "            self.screen_size[0] // 2 - 310,  # left\n",
    "            self.screen_size[1] // 2 - 310,  # top\n",
    "            self.screen_size[0] // 2 + 310,  # right\n",
    "            self.screen_size[1] // 2 + 310   # bottom\n",
    "        )\n",
    "        \n",
    "        # Load all datasets\n",
    "        self.load_all_data()\n",
    "    \n",
    "    # ---------- helpers for trial labeling ----------\n",
    "    @staticmethod\n",
    "    def extract_trial_number(trial: Dict) -> Optional[int]:\n",
    "        \"\"\"Best-effort extraction of a numeric trial index.\"\"\"\n",
    "        # Why: datasets often vary key names; we normalize without requiring schema changes.\n",
    "        candidate_keys = [\n",
    "            \"trial_index\", \"trial_num\", \"trial_number\", \"trial\",\n",
    "            \"trialId\", \"trial_id\", \"idx\", \"index\"\n",
    "        ]\n",
    "        for k in candidate_keys:\n",
    "            if k in trial:\n",
    "                v = trial[k]\n",
    "                # Normalize strings like \"12\", \"trial_12\"\n",
    "                if isinstance(v, str):\n",
    "                    digits = ''.join(ch for ch in v if ch.isdigit())\n",
    "                    if digits.isdigit():\n",
    "                        return int(digits)\n",
    "                elif isinstance(v, (int, float)) and not np.isnan(v):\n",
    "                    return int(v)\n",
    "        # As a fallback, try position if present (non-standard)\n",
    "        if \"__position__\" in trial and isinstance(trial[\"__position__\"], int):\n",
    "            return int(trial[\"__position__\"])\n",
    "        return None\n",
    "\n",
    "    def format_trial_label(self, trials: List[Dict]) -> str:\n",
    "        \"\"\"Generate a compact human label like 'Trial 12' or 'Trials 5, 9, 11â€¦'.\"\"\"\n",
    "        nums = []\n",
    "        for t in trials:\n",
    "            n = self.extract_trial_number(t)\n",
    "            if n is not None:\n",
    "                nums.append(n)\n",
    "        nums = sorted(set(nums))\n",
    "        if not nums:\n",
    "            return \"Trial (id unavailable)\"\n",
    "        if len(nums) == 1:\n",
    "            return f\"Trial {nums[0]}\"\n",
    "        # cap to avoid long titles\n",
    "        head = nums[:5]\n",
    "        suffix = \"â€¦\" if len(nums) > 5 else \"\"\n",
    "        return \"Trials \" + \", \".join(str(x) for x in head) + suffix\n",
    "\n",
    "    # ---------- data loading ----------\n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load data from all phases\"\"\"\n",
    "        self.datasets = {}\n",
    "        \n",
    "        dataset_configs = {\n",
    "            \"Training 1\": {\n",
    "                \"json\": \"Training 1/training1.json\",\n",
    "                \"images\": \"Training 1/training1_images\",\n",
    "                \"dual_images\": False\n",
    "            },\n",
    "            \"Training 2\": {\n",
    "                \"json\": \"Training 2/training2.json\",\n",
    "                \"images\": \"Training 2/training2_images\",\n",
    "                \"dual_images\": True\n",
    "            },\n",
    "            \"Testing\": {\n",
    "                \"json\": \"Testing/testing.json\",\n",
    "                \"images\": \"Testing/testing_images\",\n",
    "                \"dual_images\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for name, config in dataset_configs.items():\n",
    "            json_path = self.data_path / config[\"json\"]\n",
    "            if json_path.exists():\n",
    "                with open(json_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                # Optionally attach position for fallback labeling\n",
    "                for i, tr in enumerate(data):\n",
    "                    tr.setdefault(\"__position__\", i + 1)\n",
    "                self.datasets[name] = {\n",
    "                    \"data\": data,\n",
    "                    \"image_folder\": self.data_path / config[\"images\"],\n",
    "                    \"dual_images\": config[\"dual_images\"]\n",
    "                }\n",
    "                print(f\"âœ“ Loaded {name}: {len(data)} trials\")\n",
    "            else:\n",
    "                print(f\"âš  {name} not found at {json_path}\")\n",
    "    \n",
    "    def load_image(self, image_folder: Path, image_name: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Load an image file and return as numpy array\"\"\"\n",
    "        image_path = image_folder / image_name\n",
    "        if image_path.exists():\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            return np.array(img)\n",
    "        else:\n",
    "            print(f\"âš  Image not found: {image_path}\")\n",
    "            return None\n",
    "    \n",
    "    def get_trial_performance(self, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Calculate performance metrics for each unique trial configuration\"\"\"\n",
    "        if dataset_name not in self.datasets:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        data = self.datasets[dataset_name][\"data\"]\n",
    "        \n",
    "        if dataset_name == \"Training 1\":\n",
    "            trial_groups = {}\n",
    "            for trial in data:\n",
    "                img = trial.get(\"first_image\")\n",
    "                if img:\n",
    "                    trial_groups.setdefault(img, []).append(trial)\n",
    "            \n",
    "            performance = []\n",
    "            for img, trials in trial_groups.items():\n",
    "                correct = sum(1 for t in trials if self.is_correct(t))\n",
    "                total = len(trials)\n",
    "                performance.append({\n",
    "                    \"trial_id\": img,\n",
    "                    \"first_image\": img,\n",
    "                    \"accuracy\": correct / total if total > 0 else 0,\n",
    "                    \"n_responses\": total,\n",
    "                    \"correct_count\": correct,\n",
    "                    \"trials\": trials\n",
    "                })\n",
    "                \n",
    "        elif dataset_name in [\"Training 2\", \"Testing\"]:\n",
    "            trial_groups = {}\n",
    "            for trial in data:\n",
    "                first = trial.get(\"first_image\")\n",
    "                second = trial.get(\"second_image\")\n",
    "                if dataset_name == \"Testing\":\n",
    "                    viewing = trial.get(\"viewing_condition\", \"unknown\")\n",
    "                    key = (first, second, viewing)\n",
    "                else:\n",
    "                    key = (first, second)\n",
    "                if first and second:\n",
    "                    trial_groups.setdefault(key, []).append(trial)\n",
    "            \n",
    "            performance = []\n",
    "            for key, trials in trial_groups.items():\n",
    "                correct = sum(1 for t in trials if self.is_correct(t))\n",
    "                total = len(trials)\n",
    "                entry = {\n",
    "                    \"trial_id\": str(key),\n",
    "                    \"first_image\": key[0],\n",
    "                    \"second_image\": key[1],\n",
    "                    \"accuracy\": correct / total if total > 0 else 0,\n",
    "                    \"n_responses\": total,\n",
    "                    \"correct_count\": correct,\n",
    "                    \"trials\": trials\n",
    "                }\n",
    "                if dataset_name == \"Testing\" and len(key) > 2:\n",
    "                    entry[\"viewing_condition\"] = key[2]\n",
    "                performance.append(entry)\n",
    "        \n",
    "        return pd.DataFrame(performance)\n",
    "    \n",
    "    def is_correct(self, trial: Dict) -> bool:\n",
    "        \"\"\"Determine if a trial response was correct\"\"\"\n",
    "        if \"acc\" in trial:\n",
    "            v = trial[\"acc\"]\n",
    "            if isinstance(v, bool):\n",
    "                return v\n",
    "            if isinstance(v, (int, float)):\n",
    "                return bool(v)\n",
    "            if isinstance(v, str):\n",
    "                sv = str(v).strip().lower()\n",
    "                if sv in {\"1\", \"true\", \"correct\", \"right\"}:\n",
    "                    return True\n",
    "                if sv in {\"0\", \"false\", \"incorrect\", \"wrong\"}:\n",
    "                    return False\n",
    "        \n",
    "        answer = str(trial.get(\"subj_answer\", \"\")).strip().lower()\n",
    "        correct = str(trial.get(\"correct_response\", \"\")).strip().lower()\n",
    "        if answer and correct:\n",
    "            return answer == correct\n",
    "        return False\n",
    "    \n",
    "    def get_top_bottom_trials(self, dataset_name: str, viewing_condition: Optional[str] = None, n: int = 3):\n",
    "        \"\"\"Get top and bottom n trial configurations by success rate\"\"\"\n",
    "        df = self.get_trial_performance(dataset_name)\n",
    "        if df.empty:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        if viewing_condition and \"viewing_condition\" in df.columns:\n",
    "            df = df[df[\"viewing_condition\"] == viewing_condition]\n",
    "        df = df.sort_values(\"accuracy\", ascending=False)\n",
    "        top = df.head(n)\n",
    "        bottom = df.tail(n)\n",
    "        return top, bottom\n",
    "    \n",
    "    def collect_fixations(self, trials: List[Dict], image_key: str = \"first_image\") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Collect all fixations for given trials\"\"\"\n",
    "        l, t, r, b = self.box\n",
    "        xs_all, ys_all, durations_all = [], [], []\n",
    "        \n",
    "        for trial in trials:\n",
    "            xs = np.asarray(trial.get(\"fix_x\", []), dtype=float)\n",
    "            ys = np.asarray(trial.get(\"fix_y\", []), dtype=float)\n",
    "            durations = np.asarray(trial.get(\"fix_dur\", np.ones_like(xs) * 100), dtype=float)\n",
    "            \n",
    "            order = np.asarray(trial.get(\"fix_index\", np.arange(1, len(xs) + 1)), dtype=float)\n",
    "            idx = trial.get(\"test_image_fixation_idx\")\n",
    "            \n",
    "            if idx is not None:\n",
    "                if image_key == \"first_image\":\n",
    "                    mask = order < idx\n",
    "                else:\n",
    "                    mask = order >= idx\n",
    "                xs = xs[mask]\n",
    "                ys = ys[mask]\n",
    "                durations = durations[mask]\n",
    "            \n",
    "            mask = (xs >= l) & (xs <= r) & (ys >= t) & (ys <= b)\n",
    "            xs_all.extend(xs[mask])\n",
    "            ys_all.extend(ys[mask])\n",
    "            durations_all.extend(durations[mask])\n",
    "        \n",
    "        return np.array(xs_all), np.array(ys_all), np.array(durations_all)\n",
    "    \n",
    "    def create_heatmap(self, xs, ys, durations=None):\n",
    "        \"\"\"Create a fixation density heatmap\"\"\"\n",
    "        if len(xs) == 0:\n",
    "            return np.zeros((150, 150))\n",
    "        \n",
    "        l, t, r, b = self.box\n",
    "        H, xedges, yedges = np.histogram2d(\n",
    "            xs, ys, \n",
    "            bins=[np.linspace(l, r, 150), np.linspace(t, b, 150)],\n",
    "            weights=durations if durations is not None else None\n",
    "        )\n",
    "        H = gaussian_filter(H, sigma=15)\n",
    "        return H\n",
    "    \n",
    "    def calculate_pattern_metrics(self, xs, ys, durations):\n",
    "        \"\"\"Calculate metrics that explain fixation patterns\"\"\"\n",
    "        if len(xs) == 0:\n",
    "            return {\n",
    "                'n_fixations': 0,\n",
    "                'total_duration': 0,\n",
    "                'avg_duration': 0,\n",
    "                'spatial_spread': 0,\n",
    "                'center_bias': 0,\n",
    "                'coverage': 0\n",
    "            }\n",
    "        \n",
    "        l, t, r, b = self.box\n",
    "        xs_img = xs - l\n",
    "        ys_img = ys - t\n",
    "        \n",
    "        metrics = {\n",
    "            'n_fixations': len(xs),\n",
    "            'total_duration': np.sum(durations) if durations is not None else len(xs) * 100,\n",
    "            'avg_duration': np.mean(durations) if durations is not None else 100,\n",
    "            'spatial_spread': np.std(xs_img) + np.std(ys_img),\n",
    "            'center_bias': np.mean(np.sqrt((xs_img - 310)**2 + (ys_img - 310)**2)),\n",
    "            'coverage': len(set([(int(x/20), int(y/20)) for x, y in zip(xs_img, ys_img)])) / 961\n",
    "        }\n",
    "        return metrics\n",
    "    \n",
    "    def plot_trial_with_image(self, ax, trial_config, position: str, dataset_name: str, \n",
    "                              image_folder: Path, image_key: str = \"first_image\", \n",
    "                              show_image_num: int = 1):\n",
    "        \"\"\"Plot heatmap and fixations with actual image background\"\"\"\n",
    "        trials = trial_config[\"trials\"]\n",
    "        \n",
    "        # Determine which image to show (background only)\n",
    "        if image_key == \"first_image\":\n",
    "            image_name = trial_config[\"first_image\"]\n",
    "        else:\n",
    "            image_name = trial_config.get(\"second_image\", trial_config[\"first_image\"])\n",
    "        \n",
    "        l, t, r, b = self.box\n",
    "        img = self.load_image(image_folder, image_name)\n",
    "        if img is not None:\n",
    "            ax.imshow(img, extent=(l, r, b, t), aspect='auto')\n",
    "        \n",
    "        xs, ys, durations = self.collect_fixations(trials, image_key)\n",
    "        \n",
    "        if len(xs) > 0:\n",
    "            H = self.create_heatmap(xs, ys, durations)\n",
    "            extent = [l, r, b, t]\n",
    "            im = ax.imshow(H.T, extent=extent, origin=\"upper\", cmap=\"jet\", \n",
    "                           alpha=0.6, interpolation=\"bilinear\")\n",
    "            ax.scatter(xs, ys, s=30, c='#FF6B6B', alpha=0.5, \n",
    "                       edgecolors='white', linewidth=0.8)\n",
    "        \n",
    "        rect = patches.Rectangle((l, t), r-l, b-t, linewidth=2, \n",
    "                                 edgecolor='black', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        metrics = self.calculate_pattern_metrics(xs, ys, durations)\n",
    "        \n",
    "        color = 'green' if position == 'top' else 'red'\n",
    "        trial_label = self.format_trial_label(trials)\n",
    "\n",
    "        # Title now highlights the trial(s), not the image name; keep which image slot this is.\n",
    "        title = f\"{trial_label} Â· Image {show_image_num}\\n\"\n",
    "        title += f\"Acc: {trial_config['accuracy']:.1%} (N={trial_config['n_responses']})\\n\"\n",
    "        title += f\"Fix: {metrics['n_fixations']} | Gaze Dispersion: {metrics['spatial_spread']:.0f}px\"\n",
    "        \n",
    "        ax.set_title(title, fontsize=9, color=color)\n",
    "        \n",
    "        ax.set_xlim(0, self.screen_size[0])\n",
    "        ax.set_ylim(self.screen_size[1], 0)\n",
    "        ax.set_xlabel(\"x (screen px)\", fontsize=8)\n",
    "        ax.set_ylabel(\"y (screen px)\", fontsize=8)\n",
    "        ax.tick_params(labelsize=7)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def create_qualitative_figures(self, output_dir: Path = Path(\"qualitative_analysis\")):\n",
    "        \"\"\"Create comprehensive qualitative comparison figures\"\"\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if \"Testing\" not in self.datasets:\n",
    "            print(\"âš  Testing dataset not found!\")\n",
    "            return\n",
    "        \n",
    "        dataset_info = self.datasets[\"Testing\"]\n",
    "        image_folder = dataset_info[\"image_folder\"]\n",
    "        \n",
    "        viewing_conditions = ['full', 'central', 'peripheral']\n",
    "        \n",
    "        for condition in viewing_conditions:\n",
    "            print(f\"\\nðŸ“Š Analyzing {condition} viewing condition...\")\n",
    "            \n",
    "            top_trials, bottom_trials = self.get_top_bottom_trials(\"Testing\", condition, n=3)\n",
    "            \n",
    "            if top_trials.empty or bottom_trials.empty:\n",
    "                print(f\"  âš  Not enough data for {condition}\")\n",
    "                continue\n",
    "            \n",
    "            fig = plt.figure(figsize=(20, 14))\n",
    "            fig.suptitle(\n",
    "                f'Testing Phase - {condition.upper()} Viewing Condition\\n'\n",
    "                f'Top 3 Trials (High Accuracy) vs Bottom 3 Trials (Low Accuracy)',\n",
    "                fontsize=16, fontweight='bold'\n",
    "            )\n",
    "            \n",
    "            gs = GridSpec(2, 6, figure=fig, hspace=0.3, wspace=0.15)\n",
    "            all_metrics = {'top': [], 'bottom': []}\n",
    "            \n",
    "            for i, (_, trial) in enumerate(top_trials.iterrows()):\n",
    "                ax1 = fig.add_subplot(gs[0, i*2])\n",
    "                m1 = self.plot_trial_with_image(ax1, trial, 'top', \"Testing\", \n",
    "                                                image_folder, \"first_image\", \n",
    "                                                show_image_num=1)\n",
    "                ax2 = fig.add_subplot(gs[0, i*2 + 1])\n",
    "                m2 = self.plot_trial_with_image(ax2, trial, 'top', \"Testing\", \n",
    "                                                image_folder, \"second_image\", \n",
    "                                                show_image_num=2)\n",
    "                all_metrics['top'].append({'first': m1, 'second': m2})\n",
    "            \n",
    "            for i, (_, trial) in enumerate(bottom_trials.iterrows()):\n",
    "                ax1 = fig.add_subplot(gs[1, i*2])\n",
    "                m1 = self.plot_trial_with_image(ax1, trial, 'bottom', \"Testing\", \n",
    "                                                image_folder, \"first_image\", \n",
    "                                                show_image_num=1)\n",
    "                ax2 = fig.add_subplot(gs[1, i*2 + 1])\n",
    "                m2 = self.plot_trial_with_image(ax2, trial, 'bottom', \"Testing\", \n",
    "                                                image_folder, \"second_image\", \n",
    "                                                show_image_num=2)\n",
    "                all_metrics['bottom'].append({'first': m1, 'second': m2})\n",
    "            \n",
    "            cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "            sm = plt.cm.ScalarMappable(cmap='jet', norm=plt.Normalize(vmin=0, vmax=1))\n",
    "            sm.set_array([])\n",
    "            cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "            cbar.set_label('Fixation Density', fontsize=10)\n",
    "            \n",
    "            output_file = output_dir / f\"qualitative_{condition}_viewing.pdf\"\n",
    "            plt.savefig(output_file, bbox_inches='tight', dpi=150)\n",
    "            plt.close()\n",
    "            print(f\"  âœ“ Saved: {output_file}\")\n",
    "            \n",
    "            self.create_metrics_summary(condition, all_metrics, top_trials, bottom_trials, output_dir)\n",
    "    \n",
    "    def create_metrics_summary(self, condition: str, metrics_data: Dict, \n",
    "                               top_trials: pd.DataFrame, bottom_trials: pd.DataFrame, \n",
    "                               output_dir: Path):\n",
    "        \"\"\"Create a summary figure showing pattern metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle(\n",
    "            f'{condition.upper()} Viewing - Pattern Analysis\\n'\n",
    "            f'Values that Explain the Overall Pattern',\n",
    "            fontsize=14, fontweight='bold'\n",
    "        )\n",
    "        \n",
    "        metric_names = ['n_fixations', 'avg_duration', 'spatial_spread', \n",
    "                        'center_bias', 'coverage', 'total_duration']\n",
    "        metric_labels = ['Number of\\nFixations', 'Average\\nDuration (ms)', \n",
    "                         'Gaze\\nDispersion (px)', 'Center\\nBias (px)', \n",
    "                         'Coverage\\n(%)', 'Total\\nDuration (ms)']\n",
    "        \n",
    "        for idx, (metric, label) in enumerate(zip(metric_names, metric_labels)):\n",
    "            ax = axes[idx // 3, idx % 3]\n",
    "            top_vals = []\n",
    "            bottom_vals = []\n",
    "            \n",
    "            for trial_metrics in metrics_data['top']:\n",
    "                val1 = trial_metrics['first'].get(metric, 0)\n",
    "                val2 = trial_metrics['second'].get(metric, 0)\n",
    "                top_vals.extend([val1, val2])\n",
    "            for trial_metrics in metrics_data['bottom']:\n",
    "                val1 = trial_metrics['first'].get(metric, 0)\n",
    "                val2 = trial_metrics['second'].get(metric, 0)\n",
    "                bottom_vals.extend([val1, val2])\n",
    "            \n",
    "            if metric == 'coverage':\n",
    "                top_vals = [v * 100 for v in top_vals]\n",
    "                bottom_vals = [v * 100 for v in bottom_vals]\n",
    "            \n",
    "            x = ['Top\\nTrials', 'Bottom\\nTrials']\n",
    "            y = [np.mean(top_vals) if top_vals else 0, \n",
    "                 np.mean(bottom_vals) if bottom_vals else 0]\n",
    "            err = [np.std(top_vals) if top_vals else 0, \n",
    "                   np.std(bottom_vals) if bottom_vals else 0]\n",
    "            \n",
    "            bars = ax.bar(x, y, yerr=err, color=['green', 'red'], \n",
    "                          alpha=0.7, capsize=10)\n",
    "            \n",
    "            ax.set_ylabel(label, fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            for bar, val in zip(bars, y):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{val:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            if y[1] != 0:\n",
    "                diff = y[0] - y[1]\n",
    "                diff_pct = (diff / y[1]) * 100\n",
    "                ax.set_title(f'Î” = {diff:.0f} ({diff_pct:+.0f}%)', fontsize=9)\n",
    "        \n",
    "        avg_acc_top = top_trials['accuracy'].mean()\n",
    "        avg_acc_bottom = bottom_trials['accuracy'].mean()\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "Key Patterns for {condition.upper()} viewing:\n",
    "â€¢ Top trials (avg accuracy: {avg_acc_top:.1%}) show most difference in fixation patterns\n",
    "â€¢ Bottom trials (avg accuracy: {avg_acc_bottom:.1%}) demonstrate inefficient scanning\n",
    "â€¢ Success rate difference: {(avg_acc_top - avg_acc_bottom):.1%}\n",
    "â€¢ These metrics explain the overall pattern between successful and unsuccessful trials\n",
    "        \"\"\"\n",
    "        \n",
    "        fig.text(0.5, 0.02, summary, ha='center', fontsize=10,\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.08, 1, 0.96])\n",
    "        \n",
    "        output_file = output_dir / f\"metrics_{condition}_viewing.pdf\"\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"  âœ“ Metrics saved: {output_file}\")\n",
    "    \n",
    "    def create_overall_summary(self, output_dir: Path = Path(\"qualitative_analysis\")):\n",
    "        \"\"\"Create an overall summary across all conditions\"\"\"\n",
    "        if \"Testing\" not in self.datasets:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        fig.suptitle('Pattern Discovery: Key Differences Between Top and Bottom Trials',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        \n",
    "        viewing_conditions = ['full', 'central', 'peripheral']\n",
    "        metrics_to_show = ['n_fixations', 'spatial_spread', 'center_bias', 'coverage']\n",
    "        metric_labels = ['Fixation Count', 'Gaze Dispersion (px)', 'Center Bias (px)', 'Coverage (%)']\n",
    "        \n",
    "        for i, condition in enumerate(viewing_conditions):\n",
    "            top_trials, bottom_trials = self.get_top_bottom_trials(\"Testing\", condition, n=3)\n",
    "            if top_trials.empty or bottom_trials.empty:\n",
    "                continue\n",
    "            \n",
    "            for j, (metric, label) in enumerate(zip(metrics_to_show, metric_labels)):\n",
    "                ax = axes[i, j]\n",
    "                top_vals = []\n",
    "                bottom_vals = []\n",
    "                \n",
    "                for _, trial in top_trials.iterrows():\n",
    "                    xs1, ys1, dur1 = self.collect_fixations(trial[\"trials\"], \"first_image\")\n",
    "                    xs2, ys2, dur2 = self.collect_fixations(trial[\"trials\"], \"second_image\")\n",
    "                    m1 = self.calculate_pattern_metrics(xs1, ys1, dur1)\n",
    "                    m2 = self.calculate_pattern_metrics(xs2, ys2, dur2)\n",
    "                    val = (m1[metric] + m2[metric]) / 2\n",
    "                    if metric == 'coverage': val *= 100\n",
    "                    top_vals.append(val)\n",
    "                \n",
    "                for _, trial in bottom_trials.iterrows():\n",
    "                    xs1, ys1, dur1 = self.collect_fixations(trial[\"trials\"], \"first_image\")\n",
    "                    xs2, ys2, dur2 = self.collect_fixations(trial[\"trials\"], \"second_image\")\n",
    "                    m1 = self.calculate_pattern_metrics(xs1, ys1, dur1)\n",
    "                    m2 = self.calculate_pattern_metrics(xs2, ys2, dur2)\n",
    "                    val = (m1[metric] + m2[metric]) / 2\n",
    "                    if metric == 'coverage': val *= 100\n",
    "                    bottom_vals.append(val)\n",
    "                \n",
    "                bp = ax.boxplot([top_vals, bottom_vals], \n",
    "                                tick_labels=['Top', 'Bottom'],\n",
    "                                patch_artist=True)\n",
    "                bp['boxes'][0].set_facecolor('lightgreen')\n",
    "                bp['boxes'][1].set_facecolor('lightcoral')\n",
    "                \n",
    "                if i == 0:\n",
    "                    ax.set_title(label, fontsize=10, fontweight='bold')\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(f'{condition.capitalize()}', fontsize=10, fontweight='bold')\n",
    "                \n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.tick_params(labelsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_file = output_dir / \"overall_pattern_summary.png\"\n",
    "        fig.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"\\nâœ“ Overall summary saved: {output_file}\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUALITATIVE TRIAL ANALYSIS\")\n",
    "    print(\"Comparing Top vs Bottom Performing Trials\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    analyzer = QualitativeTrialAnalyzer()\n",
    "    \n",
    "    output_dir = Path(\"qualitative_analysis\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Generating qualitative comparison figures...\")\n",
    "    print(\"Note: Actual bird images will be displayed as backgrounds\")\n",
    "    analyzer.create_qualitative_figures(output_dir)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Creating overall pattern summary...\")\n",
    "    analyzer.create_overall_summary(output_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… Analysis complete!\")\n",
    "    print(f\"ðŸ“ All outputs saved to: {output_dir}/\")\n",
    "    print(\"\\nThe figures will show:\")\n",
    "    print(\"â€¢ Trial numbers in titles (not image names)\")\n",
    "    print(\"â€¢ Coral-colored fixation points overlaid\")\n",
    "    print(\"â€¢ Jet colormap heatmaps (semi-transparent)\")\n",
    "    print(\"â€¢ Gaze Dispersion in place of Spread\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86059cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Saved: fdm_outputs_correctness/top_bottom_by_condition_sheets/full/qualitative_top_bottom_full.pdf\n",
      "  âœ“ Saved: fdm_outputs_correctness/top_bottom_by_condition_sheets/central/qualitative_top_bottom_central.pdf\n",
      "  âœ“ Saved: fdm_outputs_correctness/top_bottom_by_condition_sheets/peripheral/qualitative_top_bottom_peripheral.pdf\n",
      "\n",
      "Done. Sheets in: /Users/daisybuathatseephol/Documents/three_json_output/fdm_outputs_correctness/top_bottom_by_condition_sheets\n"
     ]
    }
   ],
   "source": [
    "# file: fdm_top_bottom_condition_sheet.py\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# ===== CONFIG =====\n",
    "SCREEN_SIZE = (1024, 768)\n",
    "VIEWINGS = (\"full\", \"central\", \"peripheral\")\n",
    "\n",
    "# ===== UTIL =====\n",
    "def ensure_dir(p: Path) -> Path:\n",
    "    p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def sanitize(name: str, maxlen: int = 80) -> str:\n",
    "    name = re.sub(r\"[^\\w\\-]+\", \"-\", str(name))\n",
    "    name = re.sub(r\"-+\", \"-\", name).strip(\"-\")\n",
    "    return name[:maxlen]\n",
    "\n",
    "def _norm(v) -> Optional[str]:\n",
    "    if v is None: return None\n",
    "    s = str(v).strip()\n",
    "    return s if s else None\n",
    "\n",
    "def _norm_lower(v) -> Optional[str]:\n",
    "    s = _norm(v)\n",
    "    return s.lower() if s is not None else None\n",
    "\n",
    "def norm_viewing(v: object) -> Optional[str]:\n",
    "    s = _norm_lower(v)\n",
    "    if s is None: return None\n",
    "    aliases = {\n",
    "        \"f\": \"full\", \"fullfield\": \"full\",\n",
    "        \"c\": \"central\", \"center\": \"central\",\n",
    "        \"p\": \"peripheral\", \"peri\": \"peripheral\", \"periph\": \"peripheral\",\n",
    "    }\n",
    "    s = aliases.get(s, s)\n",
    "    return s if s in VIEWINGS else None\n",
    "\n",
    "# ===== TRIAL ID EXTRACTION =====\n",
    "TRIAL_KEYS = (\n",
    "    \"trial_index\",\"trial_num\",\"trial_number\",\"trial\",\n",
    "    \"trialId\",\"trial_id\",\"idx\",\"index\"\n",
    ")\n",
    "\n",
    "def extract_trial_number(tr: Dict) -> Optional[int]:\n",
    "    for k in TRIAL_KEYS:\n",
    "        if k in tr:\n",
    "            v = tr[k]\n",
    "            if isinstance(v, (int, float)) and not np.isnan(v):\n",
    "                return int(v)\n",
    "            if isinstance(v, str):\n",
    "                digits = \"\".join(ch for ch in v if ch.isdigit())\n",
    "                if digits.isdigit():\n",
    "                    return int(digits)\n",
    "    if \"__position__\" in tr and isinstance(tr[\"__position__\"], int):\n",
    "        return int(tr[\"__position__\"])\n",
    "    return None\n",
    "\n",
    "def format_trials_label(trials: List[Dict]) -> str:\n",
    "    nums = []\n",
    "    for t in trials:\n",
    "        n = extract_trial_number(t)\n",
    "        if n is not None:\n",
    "            nums.append(n)\n",
    "    nums = sorted(set(nums))\n",
    "    if not nums:\n",
    "        return \"Trial (?)\"\n",
    "    if len(nums) == 1:\n",
    "        return f\"Trial {nums[0]}\"\n",
    "    head = nums[:5]\n",
    "    suffix = \"â€¦\" if len(nums) > 5 else \"\"\n",
    "    return \"Trials \" + \", \".join(str(x) for x in head) + suffix\n",
    "\n",
    "# ===== CORRECTNESS =====\n",
    "def infer_correctness(t: Dict) -> Optional[bool]:\n",
    "    if \"acc\" in t:\n",
    "        v = t[\"acc\"]\n",
    "        if isinstance(v, bool): return v\n",
    "        if isinstance(v, (int, float)): return bool(v)\n",
    "        if isinstance(v, str):\n",
    "            sv = _norm_lower(v)\n",
    "            if sv in {\"1\",\"true\",\"correct\",\"right\"}:  return True\n",
    "            if sv in {\"0\",\"false\",\"incorrect\",\"wrong\"}: return False\n",
    "    ans = _norm_lower(t.get(\"subj_answer\"))\n",
    "    gt  = _norm_lower(t.get(\"correct_response\"))\n",
    "    if ans is not None and gt is not None:\n",
    "        return ans == gt\n",
    "    return None\n",
    "\n",
    "# ===== GROUPING =====\n",
    "def group_trials(trials: List[Dict]) -> Dict[Tuple[str,str,str], List[Dict]]:\n",
    "    groups: Dict[Tuple[str,str,str], List[Dict]] = defaultdict(list)\n",
    "    for t in trials:\n",
    "        f = _norm(t.get(\"first_image\"))\n",
    "        s = _norm(t.get(\"second_image\"))\n",
    "        v = norm_viewing(t.get(\"viewing_condition\"))\n",
    "        if not (f and s and v): continue\n",
    "        groups[(f, s, v)].append(t)\n",
    "    return groups\n",
    "\n",
    "def summarize_group(trs: List[Dict]) -> Dict[str, float | int]:\n",
    "    right = wrong = total = 0\n",
    "    for t in trs:\n",
    "        flag = infer_correctness(t)\n",
    "        if flag is True:  right += 1; total += 1\n",
    "        elif flag is False: wrong += 1; total += 1\n",
    "    acc = (right / total) if total else 0.0\n",
    "    effect = abs(right - wrong)\n",
    "    return {\"right\": right, \"wrong\": wrong, \"total\": total, \"acc\": acc, \"effect\": effect}\n",
    "\n",
    "# ===== FIXATIONS + HEATMAP (combined panel) =====\n",
    "def _box(screen_size: Tuple[int,int] = SCREEN_SIZE):\n",
    "    w, h = screen_size\n",
    "    return (w//2 - 310, h//2 - 310, w//2 + 310, h//2 + 310)\n",
    "\n",
    "def _imshow_bg(ax, image_folder: Path, img_name: str):\n",
    "    l, t, r, b = _box()\n",
    "    img = Image.open(image_folder / img_name).convert(\"RGB\")\n",
    "    ax.imshow(img, extent=(l, r, b, t), aspect=\"auto\")\n",
    "\n",
    "def _collect_fixations(trials: List[Dict], image_key: str):\n",
    "    l, t, r, b = _box()\n",
    "    xs_all, ys_all = [], []\n",
    "    for tr in trials:\n",
    "        xs = np.asarray(tr.get(\"fix_x\", []), dtype=float)\n",
    "        ys = np.asarray(tr.get(\"fix_y\", []), dtype=float)\n",
    "        order = np.asarray(tr.get(\"fix_index\", np.arange(1, len(xs) + 1)), dtype=float)\n",
    "        idx = tr.get(\"test_image_fixation_idx\")\n",
    "        if idx is not None:\n",
    "            mask = order < idx if image_key == \"first_image\" else order >= idx\n",
    "            xs, ys = xs[mask], ys[mask]\n",
    "        m = (xs>=l) & (xs<=r) & (ys>=t) & (ys<=b)\n",
    "        xs_all.extend(xs[m]); ys_all.extend(ys[m])\n",
    "    return np.array(xs_all), np.array(ys_all)\n",
    "\n",
    "def plot_panel(ax, trials_subset: List[Dict], image_folder: Path, img: str, image_key: str):\n",
    "    _imshow_bg(ax, image_folder, img)\n",
    "    xs, ys = _collect_fixations(trials_subset, image_key)\n",
    "    if xs.size:\n",
    "        l, t, r, b = _box()\n",
    "        H, xe, ye = np.histogram2d(xs, ys, bins=[np.linspace(l, r, 150), np.linspace(t, b, 150)])\n",
    "        H = gaussian_filter(H, sigma=15)\n",
    "        extent = [xe[0], xe[-1], ye[-1], ye[0]]\n",
    "        ax.imshow(H.T, extent=extent, origin=\"upper\", cmap=\"jet\", alpha=0.6, interpolation=\"bilinear\")\n",
    "        ax.scatter(xs, ys, s=28, c=\"#FF6B6B\", alpha=0.55, edgecolors=\"white\", linewidth=0.8)\n",
    "    ax.set_xlim(0, SCREEN_SIZE[0]); ax.set_ylim(SCREEN_SIZE[1], 0)\n",
    "    ax.set_xlabel(\"x (screen px)\"); ax.set_ylabel(\"y (screen px)\")\n",
    "\n",
    "# ===== SHEET BUILDER =====\n",
    "def build_condition_sheet(viewing: str,\n",
    "                          picks_top: List[Tuple[Tuple[str,str,str], Dict]],\n",
    "                          picks_bottom: List[Tuple[Tuple[str,str,str], Dict]],\n",
    "                          groups: Dict[Tuple[str,str,str], List[Dict]],\n",
    "                          image_folder: Path,\n",
    "                          out_path: Path):\n",
    "    fig = plt.figure(figsize=(20, 12), dpi=150)\n",
    "    gs = GridSpec(2, 6, figure=fig, hspace=0.35, wspace=0.15)  # equal boxes\n",
    "\n",
    "    rows = [(\"Top 3 (High Accuracy)\", picks_top, \"green\"), (\"Bottom 3 (Low Accuracy)\", picks_bottom, \"red\")]\n",
    "    for row_idx, (row_title, picks, color) in enumerate(rows):\n",
    "        for i in range(3):\n",
    "            if i >= len(picks):\n",
    "                ax_blank1 = fig.add_subplot(gs[row_idx, i*2]); ax_blank1.axis(\"off\")\n",
    "                ax_blank2 = fig.add_subplot(gs[row_idx, i*2+1]); ax_blank2.axis(\"off\")\n",
    "                continue\n",
    "            (first, second, _), stats = picks[i]\n",
    "            trs = groups[(first, second, viewing)]\n",
    "            trial_label = format_trials_label(trs)\n",
    "            subset_all = trs  # overlay both correctness types together\n",
    "\n",
    "            ax1 = fig.add_subplot(gs[row_idx, i*2])\n",
    "            plot_panel(ax1, subset_all, image_folder, first, \"first_image\")\n",
    "            ax1.set_title(\n",
    "                f\"{trial_label} Â· Image 1\\nAcc {stats['acc']:.1%} Â· right={stats['right']} Â· wrong={stats['wrong']}\",\n",
    "                fontsize=9, color=color\n",
    "            )\n",
    "\n",
    "            ax2 = fig.add_subplot(gs[row_idx, i*2 + 1])\n",
    "            plot_panel(ax2, subset_all, image_folder, second, \"second_image\")\n",
    "            ax2.set_title(\n",
    "                f\"{trial_label} Â· Image 2\\nAcc {stats['acc']:.1%} Â· right={stats['right']} Â· wrong={stats['wrong']}\",\n",
    "                fontsize=9, color=color\n",
    "            )\n",
    "\n",
    "    cax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"jet\", norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cax)\n",
    "    cbar.set_label(\"Fixation Density\", fontsize=10)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Testing Phase â€“ {viewing.upper()} Viewing\\nTop 3 Trials vs Bottom 3 Trials (heatmap + fixations overlaid)\",\n",
    "        fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"  âœ“ Saved: {out_path}\")\n",
    "\n",
    "# ===== SELECTION =====\n",
    "def pick_top_bottom_per_view(groups: Dict[Tuple[str,str,str], List[Dict]], k: int, min_n: int):\n",
    "    by_view: Dict[str, List[Tuple[Tuple[str,str,str], Dict]]] = {v: [] for v in VIEWINGS}\n",
    "    for key, trs in groups.items():\n",
    "        _, _, v = key\n",
    "        if v not in VIEWINGS: continue\n",
    "        stats = summarize_group(trs)\n",
    "        if stats[\"total\"] < min_n:  # filter low N\n",
    "            continue\n",
    "        by_view[v].append((key, stats))\n",
    "\n",
    "    picks: Dict[str, Dict[str, List[Tuple[Tuple[str,str,str], Dict]]]] = {}\n",
    "    for v in VIEWINGS:\n",
    "        lst = by_view[v]\n",
    "        if not lst:\n",
    "            picks[v] = {\"top\": [], \"bottom\": []}\n",
    "            continue\n",
    "        top_sorted = sorted(lst, key=lambda x: (x[1][\"acc\"], x[1][\"effect\"], x[1][\"total\"]), reverse=True)\n",
    "        bot_sorted = sorted(lst, key=lambda x: (x[1][\"acc\"], x[1][\"effect\"], x[1][\"total\"]), reverse=False)\n",
    "        picks[v] = {\"top\": top_sorted[:k], \"bottom\": bot_sorted[:k]}\n",
    "    return picks\n",
    "\n",
    "# ===== RUN/CLI =====\n",
    "def run(json_path: str = \"Testing/testing.json\",\n",
    "        images: str = \"Testing/testing_images\",\n",
    "        k: int = 3,\n",
    "        min_n: int = 5,\n",
    "        outdir: str = \"fdm_outputs_correctness/top_bottom_by_condition_sheets\") -> None:\n",
    "    json_path = Path(json_path); img_folder = Path(images); out_root = ensure_dir(Path(outdir))\n",
    "    with json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        trials = json.load(f)\n",
    "    # provide stable fallback indexing for trial labels\n",
    "    for i, tr in enumerate(trials, start=1):\n",
    "        tr.setdefault(\"__position__\", i)\n",
    "\n",
    "    groups = group_trials(trials)\n",
    "    picks = pick_top_bottom_per_view(groups, k=k, min_n=min_n)\n",
    "\n",
    "    for viewing in VIEWINGS:\n",
    "        view_out = ensure_dir(out_root / viewing)\n",
    "        top_picks = picks[viewing][\"top\"]\n",
    "        bot_picks = picks[viewing][\"bottom\"]\n",
    "        out = view_out / f\"qualitative_top_bottom_{sanitize(viewing)}.pdf\"\n",
    "        build_condition_sheet(viewing, top_picks, bot_picks, groups, img_folder, out)\n",
    "\n",
    "    print(f\"\\nDone. Sheets in: {out_root.resolve()}\")\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"One-page sheets: Top/Bottom 3 per viewing (heatmap+fixations; shows trial IDs)\")\n",
    "    p.add_argument(\"--json\", default=\"Testing/testing.json\")\n",
    "    p.add_argument(\"--images\", default=\"Testing/testing_images\")\n",
    "    p.add_argument(\"--k\", type=int, default=3)\n",
    "    p.add_argument(\"--min-n\", type=int, default=5)\n",
    "    p.add_argument(\"--outdir\", default=\"fdm_outputs_correctness/top_bottom_by_condition_sheets\")\n",
    "    p.add_argument(\"-f\", \"--f\", help=argparse.SUPPRESS)  # swallow Jupyter -f\n",
    "    args, _ = p.parse_known_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    run(json_path=args.json, images=args.images, k=args.k, min_n=args.min_n, outdir=args.outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9860ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Ignoring unknown CLI args: ['--f=/Users/daisybuathatseephol/Library/Jupyter/runtime/kernel-v3f3702a772b0e7063e8c6aabbccc98f0eb003b113.json']\n",
      "No pairs found in JSON/TSV inputs that match expected conditions.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/birdgaze/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# scripts/detect_same_pairs_across_conditions.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Detect identical (unordered) image pairs appearing across viewing conditions (full/central/peripheral).\n",
    "\n",
    "Inputs searched:\n",
    "- JSON: --json (default: Testing/testing.json)\n",
    "- TSV/CSV: recursively scan folders whose name you pass via --scan-tsv-dir (repeatable).\n",
    "  If none exist or none match, auto-fallback to scan --data-root recursively.\n",
    "\n",
    "Understands TSV headers you shared: filename1, filename2, viewing, TRIAL_INDEX/INDEX.\n",
    "Notebook-safe (ignores unknown CLI args) and includes a helper run_in_notebook(...).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- constants ----------------\n",
    "\n",
    "Pair = Tuple[str, str]\n",
    "Triple = Tuple[str, str, str]\n",
    "VALID_CONDITIONS = {\"full\", \"central\", \"peripheral\"}\n",
    "\n",
    "TRIAL_KEY_CANDIDATES = [\n",
    "    \"trial_index\", \"trial_num\", \"trial_number\", \"trial\",\n",
    "    \"trialid\", \"trial_id\", \"idx\", \"index\"\n",
    "]\n",
    "FIRST_COL_CANDS = [\n",
    "    \"first_image\", \"image_first\", \"first\", \"img1\", \"image1\",\n",
    "    \"left_image\", \"left\", \"filename1\"\n",
    "]\n",
    "SECOND_COL_CANDS = [\n",
    "    \"second_image\", \"image_second\", \"second\", \"img2\", \"image2\",\n",
    "    \"right_image\", \"right\", \"filename2\"\n",
    "]\n",
    "COND_COL_CANDS = [\"viewing_condition\", \"condition\", \"view\", \"viewing\"]\n",
    "\n",
    "\n",
    "# ---------------- utils ----------------\n",
    "\n",
    "def normalize_token(s: str, strip_ext: bool) -> str:\n",
    "    t = \" \".join(str(s).strip().lower().split())\n",
    "    if strip_ext:\n",
    "        p = Path(t)\n",
    "        if p.suffix:\n",
    "            t = p.with_suffix(\"\").as_posix()\n",
    "    return t\n",
    "\n",
    "\n",
    "def unordered_pair(a: str, b: str) -> Pair:\n",
    "    return tuple(sorted((a, b)))  # type: ignore[return-value]\n",
    "\n",
    "\n",
    "def pick_first_present(cols: List[str], cands: List[str]) -> Optional[str]:\n",
    "    for c in cands:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r\"\\s+\", \"_\", c.strip().lower()) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_any_tsv(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=\"\\t\", dtype=str, encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return pd.read_csv(path, dtype=str, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def infer_condition_from_name(name: str) -> Optional[str]:\n",
    "    s = name.lower()\n",
    "    for c in VALID_CONDITIONS:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------------- loaders ----------------\n",
    "\n",
    "def load_trials_json(json_path: Path, verbose: bool = False) -> List[Triple]:\n",
    "    if not json_path.exists():\n",
    "        if verbose:\n",
    "            print(f\"[verbose] JSON not found: {json_path}\")\n",
    "        return []\n",
    "    with json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    out: List[Triple] = []\n",
    "    for t in data if isinstance(data, list) else []:\n",
    "        a, b = t.get(\"first_image\"), t.get(\"second_image\")\n",
    "        cond = str(t.get(\"viewing_condition\", \"\")).strip().lower()\n",
    "        if a and b and cond in VALID_CONDITIONS:\n",
    "            out.append((str(a), str(b), cond))\n",
    "    if verbose:\n",
    "        print(f\"[verbose] JSON pairs: {len(out)} from {json_path}\")\n",
    "        if out[:3]:\n",
    "            print(f\"[verbose] JSON sample: {out[:3]}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_pairs_direct(df: pd.DataFrame, source_name: str, verbose: bool) -> List[Triple]:\n",
    "    \"\"\"When both image columns exist in a single TSV/CSV.\"\"\"\n",
    "    df = normalize_columns(df)\n",
    "    cols = df.columns.tolist()\n",
    "    c_first = pick_first_present(cols, FIRST_COL_CANDS)\n",
    "    c_second = pick_first_present(cols, SECOND_COL_CANDS)\n",
    "    c_cond = pick_first_present(cols, COND_COL_CANDS)\n",
    "    if verbose:\n",
    "        print(f\"[verbose] {source_name}: first={c_first} second={c_second} cond={c_cond or '(filename hint)'}\")\n",
    "    if not (c_first and c_second):\n",
    "        return []\n",
    "    out: List[Triple] = []\n",
    "    take = df[[c_first, c_second] + ([c_cond] if c_cond else [])]\n",
    "    fname_cond = infer_condition_from_name(source_name)\n",
    "    for _, row in take.iterrows():\n",
    "        a, b = row.get(c_first), row.get(c_second)\n",
    "        if not (isinstance(a, str) and isinstance(b, str)):\n",
    "            continue\n",
    "        cond = (str(row.get(c_cond, \"\")).strip().lower() if c_cond else \"\") or (fname_cond or \"\")\n",
    "        if cond in VALID_CONDITIONS:\n",
    "            out.append((a, b, cond))\n",
    "    if verbose:\n",
    "        print(f\"[verbose] {source_name}: extracted {len(out)} triples\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def scan_tsvs(scan_roots: List[Path], glob_pattern: str, data_root: Path, verbose: bool) -> List[Path]:\n",
    "    \"\"\"Return files matching *glob_pattern*.(tsv|csv); auto-fallback to data_root if no hits.\"\"\"\n",
    "    candidates: List[Path] = []\n",
    "    for root in scan_roots:\n",
    "        if root.exists():\n",
    "            hits = [p for p in root.rglob(\"*\") if p.is_file()\n",
    "                    and glob_pattern.lower() in p.name.lower()\n",
    "                    and p.suffix.lower() in (\".tsv\", \".csv\")]\n",
    "            if verbose:\n",
    "                print(f\"[verbose] Scanning {root} -> {len(hits)} file(s)\")\n",
    "            candidates.extend(hits)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"[verbose] Scan root not found: {root}\")\n",
    "    if not candidates:\n",
    "        # fallback: scan entire data_root\n",
    "        hits = [p for p in data_root.rglob(\"*\") if p.is_file()\n",
    "                and glob_pattern.lower() in p.name.lower()\n",
    "                and p.suffix.lower() in (\".tsv\", \".csv\")]\n",
    "        if verbose:\n",
    "            print(f\"[verbose] Fallback scan {data_root} -> {len(hits)} file(s)\")\n",
    "        candidates.extend(hits)\n",
    "    # de-duplicate while preserving order\n",
    "    seen = set()\n",
    "    uniq: List[Path] = []\n",
    "    for p in candidates:\n",
    "        rp = p.resolve()\n",
    "        if rp not in seen:\n",
    "            seen.add(rp)\n",
    "            uniq.append(rp)\n",
    "    if verbose and uniq[:10]:\n",
    "        print(\"[verbose] First few TSV/CSV files:\")\n",
    "        for p in uniq[:10]:\n",
    "            print(f\"         - {p}\")\n",
    "    return uniq\n",
    "\n",
    "\n",
    "def load_pairs_from_tsvs(paths: List[Path], verbose: bool) -> List[Triple]:\n",
    "    triples: List[Triple] = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = read_any_tsv(p)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"[verbose] Failed reading {p}: {e}\")\n",
    "            continue\n",
    "        triples.extend(extract_pairs_direct(df, p.name, verbose))\n",
    "    if verbose:\n",
    "        print(f\"[verbose] Total TSV-derived triples: {len(triples)}\")\n",
    "    return triples\n",
    "\n",
    "\n",
    "# ---------------- index & reports ----------------\n",
    "\n",
    "def build_index(triples: List[Triple], strip_ext: bool):\n",
    "    pair_to_conditions: Dict[Pair, set] = defaultdict(set)\n",
    "    pair_to_counts: Dict[Pair, Counter] = defaultdict(Counter)\n",
    "    for a_raw, b_raw, cond in triples:\n",
    "        if cond not in VALID_CONDITIONS:\n",
    "            continue\n",
    "        a = normalize_token(a_raw, strip_ext)\n",
    "        b = normalize_token(b_raw, strip_ext)\n",
    "        if not a or not b:\n",
    "            continue\n",
    "        p = unordered_pair(a, b)\n",
    "        pair_to_conditions[p].add(cond)\n",
    "        pair_to_counts[p][cond] += 1\n",
    "    return pair_to_conditions, pair_to_counts\n",
    "\n",
    "\n",
    "def write_pairs_csv(path: Path, rows: Iterable[Pair]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"first_image\", \"second_image\"])\n",
    "        for a, b in sorted(rows):\n",
    "            w.writerow([a, b])\n",
    "\n",
    "\n",
    "def write_master_csv(path: Path, pair_to_conditions, pair_to_counts) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"first_image\",\"second_image\",\n",
    "            \"in_full\",\"in_central\",\"in_peripheral\",\"n_conditions\",\n",
    "            \"count_full\",\"count_central\",\"count_peripheral\",\n",
    "        ])\n",
    "        for a, b in sorted(pair_to_conditions.keys()):\n",
    "            conds = pair_to_conditions[(a, b)]\n",
    "            cnts = pair_to_counts[(a, b)]\n",
    "            w.writerow([\n",
    "                a, b,\n",
    "                int(\"full\" in conds), int(\"central\" in conds), int(\"peripheral\" in conds),\n",
    "                len(conds),\n",
    "                cnts.get(\"full\", 0), cnts.get(\"central\", 0), cnts.get(\"peripheral\", 0),\n",
    "            ])\n",
    "\n",
    "\n",
    "def summarize_and_write(outdir: Path, pair_to_conditions, pair_to_counts) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for (a, b), conds in pair_to_conditions.items():\n",
    "        cnts = pair_to_counts[(a, b)]\n",
    "        rows.append({\n",
    "            \"first_image\": a, \"second_image\": b,\n",
    "            \"in_full\": int(\"full\" in conds),\n",
    "            \"in_central\": int(\"central\" in conds),\n",
    "            \"in_peripheral\": int(\"peripheral\" in conds),\n",
    "            \"n_conditions\": len(conds),\n",
    "            \"count_full\": cnts.get(\"full\", 0),\n",
    "            \"count_central\": cnts.get(\"central\", 0),\n",
    "            \"count_peripheral\": cnts.get(\"peripheral\", 0),\n",
    "        })\n",
    "    master = pd.DataFrame(rows).sort_values([\"n_conditions\",\"first_image\",\"second_image\"],\n",
    "                                            ascending=[False,True,True])\n",
    "\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    write_master_csv(outdir / \"pairs_master.csv\", pair_to_conditions, pair_to_counts)\n",
    "\n",
    "    in2 = [(r[\"first_image\"], r[\"second_image\"]) for _, r in master[master[\"n_conditions\"] == 2].iterrows()]\n",
    "    in3 = [(r[\"first_image\"], r[\"second_image\"]) for _, r in master[master[\"n_conditions\"] == 3].iterrows()]\n",
    "    u_full = [(r[\"first_image\"], r[\"second_image\"]) for _, r in master[(master[\"n_conditions\"] == 1) & (master[\"in_full\"] == 1)].iterrows()]\n",
    "    u_central = [(r[\"first_image\"], r[\"second_image\"]) for _, r in master[(master[\"n_conditions\"] == 1) & (master[\"in_central\"] == 1)].iterrows()]\n",
    "    u_periph = [(r[\"first_image\"], r[\"second_image\"]) for _, r in master[(master[\"n_conditions\"] == 1) & (master[\"in_peripheral\"] == 1)].iterrows()]\n",
    "\n",
    "    write_pairs_csv(outdir / \"pairs_in_2_conditions.csv\", in2)\n",
    "    write_pairs_csv(outdir / \"pairs_in_3_conditions.csv\", in3)\n",
    "    write_pairs_csv(outdir / \"unique_full.csv\", u_full)\n",
    "    write_pairs_csv(outdir / \"unique_central.csv\", u_central)\n",
    "    write_pairs_csv(outdir / \"unique_peripheral.csv\", u_periph)\n",
    "\n",
    "    print(\"=== Pair Overlap Summary ===\")\n",
    "    print(f\"Total unique unordered pairs: {len(master)}\")\n",
    "    print(f\"Pairs in exactly 2 conditions: {len(in2)}\")\n",
    "    print(f\"Pairs in all 3 conditions:     {len(in3)}\")\n",
    "    print(f\"Unique to FULL / CENTRAL / PERIPHERAL: {len(u_full)} / {len(u_central)} / {len(u_periph)}\")\n",
    "    print(f\"Reports saved to: {outdir.resolve()}\")\n",
    "    return master\n",
    "\n",
    "\n",
    "# ---------------- CLI ----------------\n",
    "\n",
    "def build_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Detect same image pairs across viewing conditions from JSON and *testing*.tsv/.csv files.\",\n",
    "        allow_abbrev=False,\n",
    "    )\n",
    "    p.add_argument(\"--data-root\", type=Path, default=Path(\".\"), help=\"Project root.\")\n",
    "    p.add_argument(\"--json\", type=str, default=\"Testing/testing.json\", help=\"Relative to --data-root.\")\n",
    "    p.add_argument(\n",
    "        \"--scan-tsv-dir\", type=Path, nargs=\"*\", default=[Path(\"THREE_JSON_OUTPUT/data/three_json_output\")],\n",
    "        help=\"One or more folders to recursively scan. If none found or folder missing, falls back to scanning --data-root.\"\n",
    "    )\n",
    "    p.add_argument(\"--tsv-glob\", type=str, default=\"testing\", help=\"Substring to match in filenames (default: 'testing').\")\n",
    "    p.add_argument(\"--out\", type=Path, default=Path(\"pair_overlap_reports\"), help=\"Output directory.\")\n",
    "    p.add_argument(\"--strip-ext\", action=\"store_true\", help=\"Match regardless of file extension.\")\n",
    "    p.add_argument(\"--verbose\", action=\"store_true\", help=\"Print diagnostic info.\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def main(argv: Optional[Sequence[str]] = None) -> int:\n",
    "    ap = build_argparser()\n",
    "    args, unknown = ap.parse_known_args(argv)  # ignore notebook-injected args like --f=...\n",
    "    if unknown:\n",
    "        print(f\"[info] Ignoring unknown CLI args: {unknown}\")\n",
    "\n",
    "    data_root = args.data_root.resolve()\n",
    "    json_path = (data_root / args.json).resolve()\n",
    "\n",
    "    # TSV scan\n",
    "    scan_roots = [data_root / d for d in args.scan_tsv_dir] if args.scan_tsv_dir else [data_root]\n",
    "    tsv_paths = scan_tsvs(scan_roots, args.tsv_glob, data_root, verbose=args.verbose)\n",
    "\n",
    "    # Load\n",
    "    triples: List[Triple] = []\n",
    "    triples += load_trials_json(json_path, verbose=args.verbose)\n",
    "    triples += load_pairs_from_tsvs(tsv_paths, verbose=args.verbose)\n",
    "\n",
    "    if args.verbose:\n",
    "        print(f\"[verbose] Total triples gathered: {len(triples)}\")\n",
    "\n",
    "    if not triples:\n",
    "        print(\"No pairs found in JSON/TSV inputs that match expected conditions.\")\n",
    "        return 0\n",
    "\n",
    "    pair_to_conditions, pair_to_counts = build_index(triples, args.strip_ext)\n",
    "    summarize_and_write(args.out, pair_to_conditions, pair_to_counts)\n",
    "    return 0\n",
    "\n",
    "\n",
    "# ---------------- Notebook helper ----------------\n",
    "\n",
    "def run_in_notebook(\n",
    "    data_root: str | Path = \".\",\n",
    "    json_rel: str = \"Testing/testing.json\",\n",
    "    scan_dirs: list[str | Path] | None = None,\n",
    "    tsv_glob: str = \"testing\",\n",
    "    outdir: str | Path = \"pair_overlap_reports\",\n",
    "    strip_ext: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "        from scripts.detect_same_pairs_across_conditions import run_in_notebook\n",
    "        df = run_in_notebook(data_root=\".\", scan_dirs=[\"THREE_JSON_OUTPUT\"], verbose=True)\n",
    "    \"\"\"\n",
    "    root = Path(data_root).resolve()\n",
    "    dirs = [root / Path(d) for d in (scan_dirs or [\"THREE_JSON_OUTPUT/data/three_json_output\"])]\n",
    "    tsv_paths = scan_tsvs(dirs, tsv_glob, root, verbose=verbose)\n",
    "    triples: List[Triple] = []\n",
    "    triples += load_trials_json((root / json_rel).resolve(), verbose=verbose)\n",
    "    triples += load_pairs_from_tsvs(tsv_paths, verbose=verbose)\n",
    "    if not triples:\n",
    "        print(\"No pairs found in JSON/TSV inputs that match expected conditions.\")\n",
    "        return pd.DataFrame()\n",
    "    pair_to_conditions, pair_to_counts = build_index(triples, strip_ext)\n",
    "    return summarize_and_write(Path(outdir), pair_to_conditions, pair_to_counts)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raise SystemExit(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7451a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching from root: /Users/daisybuathatseephol/Documents/three_json_output\n",
      "\n",
      "ðŸ“ Found 2 'Testing' folder(s)\n",
      "ðŸ“ Found 1 'data files' folder(s)\n",
      "\n",
      "ðŸ“„ Found 1 JSON file(s)\n",
      "  - testing.json (in Testing/)\n",
      "\n",
      "ðŸ“Š Found 11 TSV/CSV file(s)\n",
      "  - trialdata_training2_firstdisplayonly.tsv (in data files/)\n",
      "  - trialdata_testing_firstdisplayonly.tsv (in data files/)\n",
      "  - fixdata_training1.tsv (in data files/)\n",
      "  - fix_testing.tsv (in data files/)\n",
      "  - training2_testing_dimensions.tsv (in data files/)\n",
      "  ... and 6 more\n",
      "\n",
      "============================================================\n",
      "PROCESSING JSON FILES\n",
      "============================================================\n",
      "\n",
      "ðŸ“„ Processing: testing.json\n",
      "  Format: List with 2304 items\n",
      "  âœ“ Found 2304 valid pairs\n",
      "\n",
      "============================================================\n",
      "PROCESSING TSV/CSV FILES\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Processing: trialdata_training2_firstdisplayonly.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: trialdata_testing_firstdisplayonly.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: fixdata_training1.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: fix_testing.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: training2_testing_dimensions.tsv\n",
      "  Format: TSV with 147 rows\n",
      "  Columns: ['trial', 'first_image file name', 'first_image dimensions (width, height)', 'second_image file name', 'second_image dimensions (width, height)']\n",
      "  Column mapping: img1=first_image_file_name, img2=second_image_file_name, cond=None\n",
      "  âœ“ Found 0 valid pairs\n",
      "  Sample row data:\n",
      "    first_image_file_name: 002fac9e85104f41a7843820e9e87ee3.jpg\n",
      "    second_image_file_name: nan\n",
      "\n",
      "ðŸ“Š Processing: fixdata_training2.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: trialdata_testing_seconddisplayonly.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: msgdata_training2.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: trialdata_training2_seconddisplayonly.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š Processing: training1_dimensions.tsv\n",
      "  Format: TSV with 41 rows\n",
      "  Columns: ['trial', 'first_image file name', 'first_image dimensions (width, height)']\n",
      "  Column mapping: img1=first_image_file_name, img2=None, cond=None\n",
      "  âš ï¸ Could not find required image columns\n",
      "     Available columns: ['trial', 'first_image_file_name', 'first_image_dimensions_(width,_height)']\n",
      "\n",
      "ðŸ“Š Processing: msgdata_testing.tsv\n",
      "  âŒ Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ”„ Analyzing 2304 total pairs...\n",
      "\n",
      "Pairs by source type:\n",
      "  JSON files: 2304\n",
      "  CSV files: 0\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š ANALYSIS RESULTS\n",
      "============================================================\n",
      "Total unique pairs: 72\n",
      "Pairs in 1 condition only: 72\n",
      "Pairs in 2 conditions: 0\n",
      "Pairs in all 3 conditions: 0\n",
      "\n",
      "ðŸ’¾ Results saved to: /Users/daisybuathatseephol/Documents/three_json_output/pair_analysis_results/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def analyze_image_pairs(root_path=\".\", json_folder=\"Testing\", data_folder=\"data files\", verbose=True):\n",
    "    \"\"\"\n",
    "    Find and analyze duplicate image pairs across viewing conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - root_path: Base directory to search from\n",
    "    - json_folder: Folder name containing JSON files (default: \"Testing\")\n",
    "    - data_folder: Folder name containing TSV/CSV files (default: \"data files\")\n",
    "    \"\"\"\n",
    "    root = Path(root_path).resolve()\n",
    "    print(f\"ðŸ” Searching from root: {root}\\n\")\n",
    "    \n",
    "    # Look for the specific folders\n",
    "    json_dirs = list(root.rglob(json_folder))\n",
    "    data_dirs = list(root.rglob(data_folder))\n",
    "    \n",
    "    print(f\"ðŸ“ Found {len(json_dirs)} '{json_folder}' folder(s)\")\n",
    "    print(f\"ðŸ“ Found {len(data_dirs)} '{data_folder}' folder(s)\")\n",
    "    \n",
    "    # Collect all JSON files from Testing folders\n",
    "    json_files = []\n",
    "    for jdir in json_dirs:\n",
    "        json_files.extend(list(jdir.glob(\"*.json\")))\n",
    "    \n",
    "    # Collect all TSV/CSV files from data files folders\n",
    "    data_files = []\n",
    "    for ddir in data_dirs:\n",
    "        data_files.extend(list(ddir.glob(\"*.tsv\")))\n",
    "        data_files.extend(list(ddir.glob(\"*.csv\")))\n",
    "    \n",
    "    # Also search in root and immediate subdirectories if folders not found\n",
    "    if not json_files:\n",
    "        print(f\"âš ï¸ No JSON files found in '{json_folder}' folders, searching everywhere...\")\n",
    "        json_files = list(root.rglob(\"*.json\"))\n",
    "    \n",
    "    if not data_files:\n",
    "        print(f\"âš ï¸ No TSV/CSV files found in '{data_folder}' folders, searching everywhere...\")\n",
    "        data_files = list(root.rglob(\"*.tsv\")) + list(root.rglob(\"*.csv\"))\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Found {len(json_files)} JSON file(s)\")\n",
    "    if json_files:\n",
    "        for f in json_files[:5]:\n",
    "            print(f\"  - {f.name} (in {f.parent.name}/)\")\n",
    "        if len(json_files) > 5:\n",
    "            print(f\"  ... and {len(json_files)-5} more\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Found {len(data_files)} TSV/CSV file(s)\")\n",
    "    if data_files:\n",
    "        for f in data_files[:5]:\n",
    "            print(f\"  - {f.name} (in {f.parent.name}/)\")\n",
    "        if len(data_files) > 5:\n",
    "            print(f\"  ... and {len(data_files)-5} more\")\n",
    "    \n",
    "    if not json_files and not data_files:\n",
    "        print(\"\\nâŒ No data files found!\")\n",
    "        print(\"Make sure you're running this from the right directory.\")\n",
    "        print(\"Current directory structure:\")\n",
    "        # Show directory structure\n",
    "        for p in sorted(root.iterdir())[:10]:\n",
    "            if p.is_dir():\n",
    "                print(f\"  ðŸ“ {p.name}/\")\n",
    "                for sp in sorted(p.iterdir())[:5]:\n",
    "                    print(f\"     - {sp.name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    all_pairs = []\n",
    "    \n",
    "    # Process JSON files\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING JSON FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        print(f\"\\nðŸ“„ Processing: {json_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            with open(json_file) as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle different JSON structures\n",
    "            if isinstance(data, list):\n",
    "                items = data\n",
    "                print(f\"  Format: List with {len(items)} items\")\n",
    "            elif isinstance(data, dict):\n",
    "                # Try various possible keys\n",
    "                items = data.get(\"trials\") or data.get(\"data\") or data.get(\"items\") or []\n",
    "                print(f\"  Format: Dict with {len(items)} items in '{list(data.keys())[:5]}'\")\n",
    "            else:\n",
    "                items = []\n",
    "            \n",
    "            # If still no items but dict has direct data\n",
    "            if not items and isinstance(data, dict):\n",
    "                items = [data]  # Treat the dict itself as a single item\n",
    "            \n",
    "            found_count = 0\n",
    "            sample_item = None\n",
    "            \n",
    "            for item in items:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                \n",
    "                if not sample_item:\n",
    "                    sample_item = item\n",
    "                \n",
    "                # Try ALL possible key names for images\n",
    "                img1 = (item.get(\"first_image\") or item.get(\"image1\") or \n",
    "                       item.get(\"left_image\") or item.get(\"filename1\") or\n",
    "                       item.get(\"image_left\") or item.get(\"leftimage\") or\n",
    "                       item.get(\"img1\") or item.get(\"image_1\") or\n",
    "                       item.get(\"imageFirst\") or item.get(\"first\") or\n",
    "                       item.get(\"image_a\") or item.get(\"stim1\"))\n",
    "                \n",
    "                img2 = (item.get(\"second_image\") or item.get(\"image2\") or \n",
    "                       item.get(\"right_image\") or item.get(\"filename2\") or\n",
    "                       item.get(\"image_right\") or item.get(\"rightimage\") or\n",
    "                       item.get(\"img2\") or item.get(\"image_2\") or\n",
    "                       item.get(\"imageSecond\") or item.get(\"second\") or\n",
    "                       item.get(\"image_b\") or item.get(\"stim2\"))\n",
    "                \n",
    "                # Try ALL possible key names for condition\n",
    "                condition = (item.get(\"viewing_condition\") or item.get(\"viewing\") or \n",
    "                           item.get(\"condition\") or item.get(\"view\") or \n",
    "                           item.get(\"viewing_cond\") or item.get(\"viewingCondition\") or\n",
    "                           item.get(\"view_condition\") or item.get(\"trial_type\") or \"\")\n",
    "                \n",
    "                if img1 and img2:\n",
    "                    condition = str(condition).strip().lower()\n",
    "                    # Also check for variations of condition names\n",
    "                    if condition in [\"full\", \"central\", \"peripheral\", \"f\", \"c\", \"p\"]:\n",
    "                        # Normalize single letters\n",
    "                        if condition == \"f\": condition = \"full\"\n",
    "                        elif condition == \"c\": condition = \"central\"\n",
    "                        elif condition == \"p\": condition = \"peripheral\"\n",
    "                        \n",
    "                        # Store as sorted pair for unordered comparison\n",
    "                        pair = tuple(sorted([str(img1).strip().lower(), str(img2).strip().lower()]))\n",
    "                        all_pairs.append({\n",
    "                            \"image1\": pair[0],\n",
    "                            \"image2\": pair[1],\n",
    "                            \"condition\": condition,\n",
    "                            \"source\": json_file.name,\n",
    "                            \"source_type\": \"json\"\n",
    "                        })\n",
    "                        found_count += 1\n",
    "            \n",
    "            print(f\"  âœ“ Found {found_count} valid pairs\")\n",
    "            \n",
    "            # Show sample if no pairs found\n",
    "            if found_count == 0 and sample_item:\n",
    "                print(f\"  âš ï¸ No valid pairs extracted. Sample item keys:\")\n",
    "                print(f\"     {list(sample_item.keys())[:15]}\")\n",
    "                if verbose:\n",
    "                    print(f\"  Sample values:\")\n",
    "                    for k, v in list(sample_item.items())[:5]:\n",
    "                        print(f\"     {k}: {str(v)[:50]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error: {e}\")\n",
    "    \n",
    "    # Process TSV/CSV files\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING TSV/CSV FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for data_file in data_files:\n",
    "        print(f\"\\nðŸ“Š Processing: {data_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Try reading as TSV first, then CSV\n",
    "            try:\n",
    "                df = pd.read_csv(data_file, sep='\\t')\n",
    "                print(f\"  Format: TSV with {len(df)} rows\")\n",
    "            except:\n",
    "                df = pd.read_csv(data_file)\n",
    "                print(f\"  Format: CSV with {len(df)} rows\")\n",
    "            \n",
    "            if df.empty:\n",
    "                print(f\"  âš ï¸ File is empty\")\n",
    "                continue\n",
    "            \n",
    "            # Show original columns\n",
    "            print(f\"  Columns: {list(df.columns)[:10]}{'...' if len(df.columns) > 10 else ''}\")\n",
    "            \n",
    "            # Normalize column names\n",
    "            df.columns = [col.strip().lower().replace(' ', '_').replace('-', '_') for col in df.columns]\n",
    "            \n",
    "            # Find image columns - be very flexible\n",
    "            img1_col = None\n",
    "            img2_col = None\n",
    "            cond_col = None\n",
    "            \n",
    "            # Look for first image column\n",
    "            for col in df.columns:\n",
    "                if any(x in col for x in [\"filename1\", \"first\", \"image1\", \"img1\", \"left\", \"image_1\", \"stim1\", \"image_a\"]):\n",
    "                    img1_col = col\n",
    "                    break\n",
    "            \n",
    "            # Look for second image column  \n",
    "            for col in df.columns:\n",
    "                if any(x in col for x in [\"filename2\", \"second\", \"image2\", \"img2\", \"right\", \"image_2\", \"stim2\", \"image_b\"]):\n",
    "                    img2_col = col\n",
    "                    break\n",
    "                    \n",
    "            # Look for condition column\n",
    "            for col in df.columns:\n",
    "                if any(x in col for x in [\"viewing\", \"condition\", \"view\", \"trial_type\"]):\n",
    "                    cond_col = col\n",
    "                    break\n",
    "            \n",
    "            print(f\"  Column mapping: img1={img1_col}, img2={img2_col}, cond={cond_col}\")\n",
    "            \n",
    "            found_count = 0\n",
    "            if img1_col and img2_col:\n",
    "                for idx, row in df.iterrows():\n",
    "                    img1 = str(row[img1_col]).strip() if pd.notna(row[img1_col]) else None\n",
    "                    img2 = str(row[img2_col]).strip() if pd.notna(row[img2_col]) else None\n",
    "                    \n",
    "                    # Skip if images are empty or 'nan'\n",
    "                    if not img1 or not img2 or img1.lower() == 'nan' or img2.lower() == 'nan':\n",
    "                        continue\n",
    "                    \n",
    "                    # Get condition from column or infer from filename\n",
    "                    condition = None\n",
    "                    if cond_col and pd.notna(row[cond_col]):\n",
    "                        condition = str(row[cond_col]).strip().lower()\n",
    "                    \n",
    "                    # If no condition column, try to infer from filename\n",
    "                    if not condition or condition not in [\"full\", \"central\", \"peripheral\", \"f\", \"c\", \"p\"]:\n",
    "                        fname_lower = data_file.name.lower()\n",
    "                        if \"full\" in fname_lower or \"_f_\" in fname_lower or \"_f.\" in fname_lower:\n",
    "                            condition = \"full\"\n",
    "                        elif \"central\" in fname_lower or \"_c_\" in fname_lower or \"_c.\" in fname_lower:\n",
    "                            condition = \"central\"\n",
    "                        elif \"peripheral\" in fname_lower or \"periph\" in fname_lower or \"_p_\" in fname_lower or \"_p.\" in fname_lower:\n",
    "                            condition = \"peripheral\"\n",
    "                    \n",
    "                    # Normalize single letter conditions\n",
    "                    if condition == \"f\": condition = \"full\"\n",
    "                    elif condition == \"c\": condition = \"central\"\n",
    "                    elif condition == \"p\": condition = \"peripheral\"\n",
    "                    \n",
    "                    if condition in [\"full\", \"central\", \"peripheral\"]:\n",
    "                        pair = tuple(sorted([img1.lower(), img2.lower()]))\n",
    "                        all_pairs.append({\n",
    "                            \"image1\": pair[0],\n",
    "                            \"image2\": pair[1],\n",
    "                            \"condition\": condition,\n",
    "                            \"source\": data_file.name,\n",
    "                            \"source_type\": \"csv\"\n",
    "                        })\n",
    "                        found_count += 1\n",
    "                \n",
    "                print(f\"  âœ“ Found {found_count} valid pairs\")\n",
    "                \n",
    "                if found_count == 0 and not df.empty:\n",
    "                    print(f\"  Sample row data:\")\n",
    "                    sample_row = df.iloc[0]\n",
    "                    if img1_col and img2_col:\n",
    "                        print(f\"    {img1_col}: {sample_row[img1_col]}\")\n",
    "                        print(f\"    {img2_col}: {sample_row[img2_col]}\")\n",
    "                        if cond_col:\n",
    "                            print(f\"    {cond_col}: {sample_row[cond_col]}\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ Could not find required image columns\")\n",
    "                print(f\"     Available columns: {df.columns.tolist()[:15]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error: {e}\")\n",
    "    \n",
    "    if not all_pairs:\n",
    "        print(\"\\nâŒ No valid pairs found across any files!\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Check that viewing conditions are: 'full', 'central', 'peripheral' (or 'f', 'c', 'p')\")\n",
    "        print(\"2. Check that image columns contain actual filenames\")\n",
    "        print(\"3. Try setting verbose=True to see more details\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create analysis\n",
    "    print(f\"\\nðŸ”„ Analyzing {len(all_pairs)} total pairs...\")\n",
    "    df_all = pd.DataFrame(all_pairs)\n",
    "    \n",
    "    # Show source breakdown\n",
    "    print(f\"\\nPairs by source type:\")\n",
    "    print(f\"  JSON files: {len(df_all[df_all['source_type'] == 'json'])}\")\n",
    "    print(f\"  CSV files: {len(df_all[df_all['source_type'] == 'csv'])}\")\n",
    "    \n",
    "    # Group by pair and count conditions\n",
    "    pair_analysis = df_all.groupby(['image1', 'image2']).agg({\n",
    "        'condition': lambda x: set(x),\n",
    "        'source': lambda x: list(x),\n",
    "        'source_type': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    pair_analysis.columns = ['image1', 'image2', 'conditions', 'sources', 'total_occurrences']\n",
    "    pair_analysis['n_conditions'] = pair_analysis['conditions'].apply(len)\n",
    "    \n",
    "    # Add binary flags for each condition\n",
    "    pair_analysis['in_full'] = pair_analysis['conditions'].apply(lambda x: 'full' in x).astype(int)\n",
    "    pair_analysis['in_central'] = pair_analysis['conditions'].apply(lambda x: 'central' in x).astype(int)\n",
    "    pair_analysis['in_peripheral'] = pair_analysis['conditions'].apply(lambda x: 'peripheral' in x).astype(int)\n",
    "    \n",
    "    # Convert conditions set to string\n",
    "    pair_analysis['conditions_str'] = pair_analysis['conditions'].apply(lambda x: ', '.join(sorted(x)))\n",
    "    \n",
    "    # Sort by number of conditions (descending)\n",
    "    pair_analysis = pair_analysis.sort_values(['n_conditions', 'image1', 'image2'], \n",
    "                                              ascending=[False, True, True])\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š ANALYSIS RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total unique pairs: {len(pair_analysis)}\")\n",
    "    print(f\"Pairs in 1 condition only: {sum(pair_analysis['n_conditions'] == 1)}\")\n",
    "    print(f\"Pairs in 2 conditions: {sum(pair_analysis['n_conditions'] == 2)}\")\n",
    "    print(f\"Pairs in all 3 conditions: {sum(pair_analysis['n_conditions'] == 3)}\")\n",
    "    \n",
    "    # Show duplicates\n",
    "    duplicates = pair_analysis[pair_analysis['n_conditions'] > 1]\n",
    "    if not duplicates.empty:\n",
    "        print(f\"\\nðŸ” DUPLICATE PAIRS (in multiple conditions): {len(duplicates)} pairs\")\n",
    "        \n",
    "        in_all_3 = duplicates[duplicates['n_conditions'] == 3]\n",
    "        if not in_all_3.empty:\n",
    "            print(f\"\\nâœ¨ Pairs in ALL 3 conditions ({len(in_all_3)} total):\")\n",
    "            for i, (_, row) in enumerate(in_all_3.head(5).iterrows(), 1):\n",
    "                print(f\"  {i}. {row['image1'][:40]}\")\n",
    "                print(f\"     {row['image2'][:40]}\")\n",
    "            if len(in_all_3) > 5:\n",
    "                print(f\"  ... and {len(in_all_3)-5} more\")\n",
    "    \n",
    "    # Save results\n",
    "    try:\n",
    "        output_dir = Path(\"pair_analysis_results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        pair_analysis.to_csv(output_dir / \"all_pairs.csv\", index=False)\n",
    "        if not duplicates.empty:\n",
    "            duplicates.to_csv(output_dir / \"duplicate_pairs.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Results saved to: {output_dir.resolve()}/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸ Could not save CSV files: {e}\")\n",
    "    \n",
    "    return pair_analysis\n",
    "\n",
    "# RUN THE ANALYSIS - adjust the folder names if needed\n",
    "df = analyze_image_pairs(\n",
    "    root_path=\".\",  # or use \"../..\" to go up directories\n",
    "    json_folder=\"Testing\",  # folder containing JSON files\n",
    "    data_folder=\"data files\",  # folder containing TSV/CSV files\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Show the duplicate pairs\n",
    "if not df.empty:\n",
    "    duplicates = df[df['n_conditions'] > 1]\n",
    "    if not duplicates.empty:\n",
    "        print(\"\\nðŸ“‹ All duplicate pairs:\")\n",
    "        display(duplicates[['image1', 'image2', 'conditions_str', 'n_conditions']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c9aa353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Diagnostic Analysis for: /Users/daisybuathatseephol/Documents/three_json_output\n",
      "\n",
      "Files found:\n",
      "  JSON: ['testing.json']\n",
      "  TSV/CSV: ['trialdata_training2_firstdisplayonly.tsv', 'trialdata_testing_firstdisplayonly.tsv', 'fixdata_training1.tsv', 'fix_testing.tsv', 'training2_testing_dimensions.tsv', 'fixdata_training2.tsv', 'trialdata_testing_seconddisplayonly.tsv', 'msgdata_training2.tsv', 'trialdata_training2_seconddisplayonly.tsv', 'training1_dimensions.tsv']\n",
      "\n",
      "============================================================\n",
      "CHECKING JSON FILE\n",
      "============================================================\n",
      "\n",
      "ðŸ“„ testing.json: 2304 items\n",
      "  Sample item keys: ['subject_id', 'trial_index', 'fix_index', 'fix_start_ms', 'fix_x', 'fix_y', 'fix_dur_ms', 'fix_pupil', 'saccade_dx', 'saccade_dy', 'saccade_amp', 'saccade_dir_deg', 'acc', 'subj_answer', 'paradigm', 'viewing_condition', 'eye_tracked', 'test_image_on_ms', 'test_image_fixation_idx', 'age', 'gender', 'max_calibration_error', 'avg_calibration_error', 'correct_response', 'first_image', 'second_image']\n",
      "  Conditions found:\n",
      "    central: 768 pairs\n",
      "      Sample pairs:\n",
      "        - f39e8c35b562470eb229890c2c547210.jpg\n",
      "          e00efbb3124b43f684903b0230e16252.jpg\n",
      "        - 7f5e4bf12f9f4e159081162cf88a7fb7.jpg\n",
      "          5ee126f8ae6640758d5481e2689bdb81.jpg\n",
      "    full: 768 pairs\n",
      "      Sample pairs:\n",
      "        - 28ae8fcc3c214dc5b785225113b5d5ed.jpg\n",
      "          15dcb6731dda4a3fb0996108c8ef8d50.jpg\n",
      "        - 2c4e653610eb497da50f7420cff5c3a2.jpg\n",
      "          3e263a25c12942b6a691c1e3f8b8c0d0.jpg\n",
      "    peripheral: 768 pairs\n",
      "      Sample pairs:\n",
      "        - 8f8709d10e3b4a7d9132023c629b81f7.jpg\n",
      "          5592a6d794c24c73849b83794d5b6a4e.jpg\n",
      "        - 573e9073f3ed44b094a3ffbbe8fed4db.jpg\n",
      "          571b534494664f83b7f796bde052bf49.jpg\n",
      "\n",
      "============================================================\n",
      "CHECKING TSV/CSV FILES\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š trialdata_training2_firstdisplayonly.tsv\n",
      "  Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š trialdata_testing_firstdisplayonly.tsv\n",
      "  Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š fixdata_training1.tsv\n",
      "  Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š fix_testing.tsv\n",
      "  Error: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "ðŸ“Š training2_testing_dimensions.tsv\n",
      "  Shape: (147, 5)\n",
      "  Columns: ['trial', 'first_image file name', 'first_image dimensions (width, height)', 'second_image file name', 'second_image dimensions (width, height)']\n",
      "  Mapped: img1=first_image_file_name, img2=second_image_file_name, cond=None\n",
      "  Sample rows:\n",
      "    Row 0:\n",
      "      img1: 002fac9e85104f41a7843820e9e87ee3.jpg\n",
      "      img2: NA\n",
      "      condition: NA\n",
      "    Row 1:\n",
      "      img1: 01f6d2022368434aacc21db2a6ba4fc9.jpg\n",
      "      img2: NA\n",
      "      condition: NA\n",
      "    Row 2:\n",
      "      img1: 0260cc5cc34344afb50cfaeda417d65b.jpg\n",
      "      img2: NA\n",
      "      condition: NA\n",
      "  Conditions in this file: {}\n",
      "\n",
      "============================================================\n",
      "COMPARISON: JSON vs TSV/CSV\n",
      "============================================================\n",
      "\n",
      "Pairs per condition:\n",
      "\n",
      "CENTRAL:\n",
      "  JSON pairs: 24\n",
      "  TSV pairs: 0\n",
      "\n",
      "FULL:\n",
      "  JSON pairs: 24\n",
      "  TSV pairs: 0\n",
      "\n",
      "PERIPHERAL:\n",
      "  JSON pairs: 24\n",
      "  TSV pairs: 0\n",
      "\n",
      "============================================================\n",
      "CHECKING FOR DUPLICATES WITHIN EACH SOURCE\n",
      "============================================================\n",
      "\n",
      "JSON duplicates (pairs in multiple conditions): 0\n",
      "\n",
      "============================================================\n",
      "ðŸ’¡ DIAGNOSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Possible issues to check:\n",
      "1. Image filenames might have different formats between JSON and TSV\n",
      "2. File extensions might be included in one source but not the other\n",
      "3. Path information might be included in one source but not the other\n",
      "4. Case sensitivity issues (uppercase vs lowercase)\n",
      "5. The TSV files might not have been processed correctly\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def diagnose_pair_matching(root_path=\".\", json_folder=\"Testing\", data_folder=\"data files\"):\n",
    "    \"\"\"\n",
    "    Diagnose why pairs aren't matching across files\n",
    "    \"\"\"\n",
    "    root = Path(root_path).resolve()\n",
    "    print(f\"ðŸ” Diagnostic Analysis for: {root}\\n\")\n",
    "    \n",
    "    # Find folders\n",
    "    json_dirs = list(root.rglob(json_folder))\n",
    "    data_dirs = list(root.rglob(data_folder))\n",
    "    \n",
    "    # Collect files\n",
    "    json_files = []\n",
    "    for jdir in json_dirs:\n",
    "        json_files.extend(list(jdir.glob(\"*.json\")))\n",
    "    \n",
    "    data_files = []\n",
    "    for ddir in data_dirs:\n",
    "        data_files.extend(list(ddir.glob(\"*.tsv\")))\n",
    "        data_files.extend(list(ddir.glob(\"*.csv\")))\n",
    "    \n",
    "    print(f\"Files found:\")\n",
    "    print(f\"  JSON: {[f.name for f in json_files]}\")\n",
    "    print(f\"  TSV/CSV: {[f.name for f in data_files[:10]]}\")\n",
    "    \n",
    "    # Store all pairs with their sources\n",
    "    json_pairs = {}  # condition -> set of pairs\n",
    "    tsv_pairs = {}   # condition -> set of pairs\n",
    "    \n",
    "    # Process JSON\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHECKING JSON FILE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        items = data if isinstance(data, list) else []\n",
    "        print(f\"\\nðŸ“„ {json_file.name}: {len(items)} items\")\n",
    "        \n",
    "        # Check first few items\n",
    "        if items:\n",
    "            print(f\"  Sample item keys: {list(items[0].keys())}\")\n",
    "            \n",
    "            # Count by condition\n",
    "            conditions_found = defaultdict(int)\n",
    "            sample_pairs = defaultdict(list)\n",
    "            \n",
    "            for item in items:\n",
    "                # Get images - try multiple keys\n",
    "                img1 = (item.get(\"first_image\") or item.get(\"image1\") or \n",
    "                       item.get(\"filename1\") or item.get(\"left_image\"))\n",
    "                img2 = (item.get(\"second_image\") or item.get(\"image2\") or \n",
    "                       item.get(\"filename2\") or item.get(\"right_image\"))\n",
    "                cond = (item.get(\"viewing_condition\") or item.get(\"viewing\") or \n",
    "                       item.get(\"condition\") or \"\")\n",
    "                \n",
    "                if img1 and img2:\n",
    "                    cond = str(cond).strip().lower()\n",
    "                    if cond in [\"full\", \"central\", \"peripheral\"]:\n",
    "                        conditions_found[cond] += 1\n",
    "                        \n",
    "                        # Store normalized pair\n",
    "                        pair = tuple(sorted([str(img1).strip().lower(), str(img2).strip().lower()]))\n",
    "                        \n",
    "                        if cond not in json_pairs:\n",
    "                            json_pairs[cond] = set()\n",
    "                        json_pairs[cond].add(pair)\n",
    "                        \n",
    "                        # Keep sample\n",
    "                        if len(sample_pairs[cond]) < 3:\n",
    "                            sample_pairs[cond].append((img1, img2))\n",
    "            \n",
    "            print(f\"  Conditions found:\")\n",
    "            for cond, count in conditions_found.items():\n",
    "                print(f\"    {cond}: {count} pairs\")\n",
    "                if sample_pairs[cond]:\n",
    "                    print(f\"      Sample pairs:\")\n",
    "                    for img1, img2 in sample_pairs[cond][:2]:\n",
    "                        print(f\"        - {img1[:40]}\")\n",
    "                        print(f\"          {img2[:40]}\")\n",
    "    \n",
    "    # Process TSV/CSV files\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHECKING TSV/CSV FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for data_file in data_files[:5]:  # Check first 5 files\n",
    "        print(f\"\\nðŸ“Š {data_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read file\n",
    "            try:\n",
    "                df = pd.read_csv(data_file, sep='\\t')\n",
    "            except:\n",
    "                df = pd.read_csv(data_file)\n",
    "            \n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Columns: {list(df.columns)[:10]}\")\n",
    "            \n",
    "            # Normalize columns\n",
    "            df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "            \n",
    "            # Find image columns\n",
    "            img1_col = None\n",
    "            img2_col = None\n",
    "            cond_col = None\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if not img1_col and any(x in col for x in [\"filename1\", \"first\", \"image1\", \"left\"]):\n",
    "                    img1_col = col\n",
    "                if not img2_col and any(x in col for x in [\"filename2\", \"second\", \"image2\", \"right\"]):\n",
    "                    img2_col = col\n",
    "                if not cond_col and any(x in col for x in [\"viewing\", \"condition\"]):\n",
    "                    cond_col = col\n",
    "            \n",
    "            print(f\"  Mapped: img1={img1_col}, img2={img2_col}, cond={cond_col}\")\n",
    "            \n",
    "            if img1_col and img2_col:\n",
    "                # Show sample data\n",
    "                sample = df.head(3)\n",
    "                print(f\"  Sample rows:\")\n",
    "                for idx, row in sample.iterrows():\n",
    "                    img1 = str(row[img1_col]) if pd.notna(row[img1_col]) else \"NA\"\n",
    "                    img2 = str(row[img2_col]) if pd.notna(row[img2_col]) else \"NA\"\n",
    "                    cond_val = str(row[cond_col]) if cond_col and pd.notna(row[cond_col]) else \"NA\"\n",
    "                    \n",
    "                    print(f\"    Row {idx}:\")\n",
    "                    print(f\"      img1: {img1[:50]}\")\n",
    "                    print(f\"      img2: {img2[:50]}\")\n",
    "                    print(f\"      condition: {cond_val}\")\n",
    "                \n",
    "                # Count conditions\n",
    "                conditions_in_file = defaultdict(int)\n",
    "                for _, row in df.iterrows():\n",
    "                    img1 = str(row[img1_col]).strip() if pd.notna(row[img1_col]) else None\n",
    "                    img2 = str(row[img2_col]).strip() if pd.notna(row[img2_col]) else None\n",
    "                    \n",
    "                    if not img1 or not img2 or img1.lower() == 'nan' or img2.lower() == 'nan':\n",
    "                        continue\n",
    "                    \n",
    "                    # Get condition\n",
    "                    if cond_col and pd.notna(row[cond_col]):\n",
    "                        cond = str(row[cond_col]).strip().lower()\n",
    "                    else:\n",
    "                        # Infer from filename\n",
    "                        fname = data_file.name.lower()\n",
    "                        if \"full\" in fname:\n",
    "                            cond = \"full\"\n",
    "                        elif \"central\" in fname:\n",
    "                            cond = \"central\"\n",
    "                        elif \"peripheral\" in fname or \"periph\" in fname:\n",
    "                            cond = \"peripheral\"\n",
    "                        else:\n",
    "                            cond = None\n",
    "                    \n",
    "                    if cond in [\"full\", \"central\", \"peripheral\"]:\n",
    "                        conditions_in_file[cond] += 1\n",
    "                        \n",
    "                        # Store normalized pair\n",
    "                        pair = tuple(sorted([img1.lower(), img2.lower()]))\n",
    "                        if cond not in tsv_pairs:\n",
    "                            tsv_pairs[cond] = set()\n",
    "                        tsv_pairs[cond].add(pair)\n",
    "                \n",
    "                print(f\"  Conditions in this file: {dict(conditions_in_file)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Compare JSON vs TSV pairs\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON: JSON vs TSV/CSV\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nPairs per condition:\")\n",
    "    all_conditions = set(json_pairs.keys()) | set(tsv_pairs.keys())\n",
    "    \n",
    "    for cond in sorted(all_conditions):\n",
    "        json_count = len(json_pairs.get(cond, set()))\n",
    "        tsv_count = len(tsv_pairs.get(cond, set()))\n",
    "        print(f\"\\n{cond.upper()}:\")\n",
    "        print(f\"  JSON pairs: {json_count}\")\n",
    "        print(f\"  TSV pairs: {tsv_count}\")\n",
    "        \n",
    "        if json_count > 0 and tsv_count > 0:\n",
    "            # Check for overlaps\n",
    "            json_set = json_pairs[cond]\n",
    "            tsv_set = tsv_pairs[cond]\n",
    "            \n",
    "            overlap = json_set & tsv_set\n",
    "            json_only = json_set - tsv_set\n",
    "            tsv_only = tsv_set - json_set\n",
    "            \n",
    "            print(f\"  Overlapping pairs: {len(overlap)}\")\n",
    "            print(f\"  JSON-only pairs: {len(json_only)}\")\n",
    "            print(f\"  TSV-only pairs: {len(tsv_only)}\")\n",
    "            \n",
    "            if len(overlap) == 0 and json_count > 0 and tsv_count > 0:\n",
    "                print(f\"\\n  âš ï¸ NO OVERLAP! Checking why...\")\n",
    "                \n",
    "                # Show samples from each\n",
    "                print(f\"  Sample JSON pairs (first 3):\")\n",
    "                for pair in list(json_set)[:3]:\n",
    "                    print(f\"    - {pair[0][:40]}\")\n",
    "                    print(f\"      {pair[1][:40]}\")\n",
    "                \n",
    "                print(f\"  Sample TSV pairs (first 3):\")\n",
    "                for pair in list(tsv_set)[:3]:\n",
    "                    print(f\"    - {pair[0][:40]}\")\n",
    "                    print(f\"      {pair[1][:40]}\")\n",
    "    \n",
    "    # Check for duplicates across conditions within each source\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHECKING FOR DUPLICATES WITHIN EACH SOURCE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check JSON duplicates\n",
    "    all_json_pairs = []\n",
    "    for cond, pairs in json_pairs.items():\n",
    "        for pair in pairs:\n",
    "            all_json_pairs.append((pair, cond))\n",
    "    \n",
    "    pair_conditions = defaultdict(set)\n",
    "    for pair, cond in all_json_pairs:\n",
    "        pair_conditions[pair].add(cond)\n",
    "    \n",
    "    duplicates = {pair: conds for pair, conds in pair_conditions.items() if len(conds) > 1}\n",
    "    \n",
    "    print(f\"\\nJSON duplicates (pairs in multiple conditions): {len(duplicates)}\")\n",
    "    if duplicates:\n",
    "        for i, (pair, conds) in enumerate(list(duplicates.items())[:5], 1):\n",
    "            print(f\"  {i}. Pair in conditions: {', '.join(sorted(conds))}\")\n",
    "            print(f\"     {pair[0][:40]}\")\n",
    "            print(f\"     {pair[1][:40]}\")\n",
    "    \n",
    "    return json_pairs, tsv_pairs\n",
    "\n",
    "# Run the diagnostic\n",
    "json_pairs, tsv_pairs = diagnose_pair_matching(\".\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¡ DIAGNOSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPossible issues to check:\")\n",
    "print(\"1. Image filenames might have different formats between JSON and TSV\")\n",
    "print(\"2. File extensions might be included in one source but not the other\")\n",
    "print(\"3. Path information might be included in one source but not the other\")\n",
    "print(\"4. Case sensitivity issues (uppercase vs lowercase)\")\n",
    "print(\"5. The TSV files might not have been processed correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d65b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Analyzing pairs from: /Users/daisybuathatseephol/Documents/three_json_output\n",
      "\n",
      "ðŸ“„ Reading: /Users/daisybuathatseephol/Documents/three_json_output/Testing/testing.json\n",
      "\n",
      "Total items in JSON: 2304\n",
      "\n",
      "Valid pairs extracted: 2304\n",
      "Pairs by condition:\n",
      "  full: 768\n",
      "  central: 768\n",
      "  peripheral: 768\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š DUPLICATE ANALYSIS RESULTS\n",
      "============================================================\n",
      "\n",
      "Total unique pairs: 72\n",
      "Pairs in 1 condition only: 0\n",
      "Pairs in 2 conditions: 0\n",
      "Pairs in all 3 conditions: 0\n",
      "\n",
      "Overlaps between condition pairs:\n",
      "  Full âˆ© Central: 0\n",
      "  Full âˆ© Peripheral: 0\n",
      "  Central âˆ© Peripheral: 0\n",
      "  All three: 0\n",
      "\n",
      "ðŸ” DUPLICATE PAIRS (in multiple conditions): 72 pairs\n",
      "\n",
      "ðŸ’¾ Results saved to pair_analysis_results/\n",
      "\n",
      "ðŸ“‹ First 10 pairs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img1_original</th>\n",
       "      <th>img2_original</th>\n",
       "      <th>conditions_str</th>\n",
       "      <th>n_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002fac9e85104f41a7843820e9e87ee3.jpg</td>\n",
       "      <td>30e447f9321a4c74880915cc77f54950.jpg</td>\n",
       "      <td>full, full, full, full, full, full, full, full...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6b94d252b54242748bfb2c611bd3368f.jpg</td>\n",
       "      <td>01f6d2022368434aacc21db2a6ba4fc9.jpg</td>\n",
       "      <td>peripheral, peripheral, peripheral, peripheral...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374cad12d807421b9e8295785d6bc44f.jpg</td>\n",
       "      <td>0260cc5cc34344afb50cfaeda417d65b.jpg</td>\n",
       "      <td>full, full, full, full, full, full, full, full...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1098187526484c62b20eb0f59faec6ef.jpg</td>\n",
       "      <td>0693816789364e83bc559e435816ec8f.jpg</td>\n",
       "      <td>peripheral, peripheral, peripheral, peripheral...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78765bd55e9a4ec7a9c5f5590a36b386.jpg</td>\n",
       "      <td>07150a02be164be4bd0e325cb2428430.jpg</td>\n",
       "      <td>full, full, full, full, full, full, full, full...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6f50c2b038564cd19a0873bf396f1549.jpg</td>\n",
       "      <td>0c5a770e3f57463f84a9897407744692.jpg</td>\n",
       "      <td>peripheral, peripheral, peripheral, peripheral...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15ef3e209a84417282278db5a1e3b412.jpg</td>\n",
       "      <td>0e68f17fae9949888cd2017b9b5a6754.jpg</td>\n",
       "      <td>central, central, central, central, central, c...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0e68f17fae9949888cd2017b9b5a6754.jpg</td>\n",
       "      <td>1e014f55dac948ccb2e9053666e123b4.jpg</td>\n",
       "      <td>full, full, full, full, full, full, full, full...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8b20a875626e420fa0abf64466663c6e.jpg</td>\n",
       "      <td>0f4c34f941f243c5b5f19c25924a9202.jpg</td>\n",
       "      <td>peripheral, peripheral, peripheral, peripheral...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11b33ba71fc34d88b6a2fd4183ec3cea.jpg</td>\n",
       "      <td>1a8b5b4beb2844bfa7d6d7dd163a829f.jpg</td>\n",
       "      <td>central, central, central, central, central, c...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          img1_original                         img2_original  \\\n",
       "0  002fac9e85104f41a7843820e9e87ee3.jpg  30e447f9321a4c74880915cc77f54950.jpg   \n",
       "1  6b94d252b54242748bfb2c611bd3368f.jpg  01f6d2022368434aacc21db2a6ba4fc9.jpg   \n",
       "2  374cad12d807421b9e8295785d6bc44f.jpg  0260cc5cc34344afb50cfaeda417d65b.jpg   \n",
       "3  1098187526484c62b20eb0f59faec6ef.jpg  0693816789364e83bc559e435816ec8f.jpg   \n",
       "4  78765bd55e9a4ec7a9c5f5590a36b386.jpg  07150a02be164be4bd0e325cb2428430.jpg   \n",
       "5  6f50c2b038564cd19a0873bf396f1549.jpg  0c5a770e3f57463f84a9897407744692.jpg   \n",
       "6  15ef3e209a84417282278db5a1e3b412.jpg  0e68f17fae9949888cd2017b9b5a6754.jpg   \n",
       "7  0e68f17fae9949888cd2017b9b5a6754.jpg  1e014f55dac948ccb2e9053666e123b4.jpg   \n",
       "8  8b20a875626e420fa0abf64466663c6e.jpg  0f4c34f941f243c5b5f19c25924a9202.jpg   \n",
       "9  11b33ba71fc34d88b6a2fd4183ec3cea.jpg  1a8b5b4beb2844bfa7d6d7dd163a829f.jpg   \n",
       "\n",
       "                                      conditions_str  n_conditions  \n",
       "0  full, full, full, full, full, full, full, full...            32  \n",
       "1  peripheral, peripheral, peripheral, peripheral...            32  \n",
       "2  full, full, full, full, full, full, full, full...            32  \n",
       "3  peripheral, peripheral, peripheral, peripheral...            32  \n",
       "4  full, full, full, full, full, full, full, full...            32  \n",
       "5  peripheral, peripheral, peripheral, peripheral...            32  \n",
       "6  central, central, central, central, central, c...            32  \n",
       "7  full, full, full, full, full, full, full, full...            32  \n",
       "8  peripheral, peripheral, peripheral, peripheral...            32  \n",
       "9  central, central, central, central, central, c...            32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def find_duplicate_pairs(root_path=\".\"):\n",
    "    \"\"\"\n",
    "    Find duplicate pairs across viewing conditions from the JSON file.\n",
    "    Since TSV files have issues, we'll focus on the JSON data.\n",
    "    \"\"\"\n",
    "    root = Path(root_path).resolve()\n",
    "    print(f\"ðŸ” Analyzing pairs from: {root}\\n\")\n",
    "    \n",
    "    # Find the JSON file\n",
    "    json_files = list(root.rglob(\"testing.json\"))\n",
    "    if not json_files:\n",
    "        print(\"âŒ testing.json not found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    json_file = json_files[0]\n",
    "    print(f\"ðŸ“„ Reading: {json_file}\\n\")\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Total items in JSON: {len(data)}\\n\")\n",
    "    \n",
    "    # Process all pairs\n",
    "    all_pairs = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Get images\n",
    "        img1 = item.get(\"first_image\", \"\")\n",
    "        img2 = item.get(\"second_image\", \"\")\n",
    "        condition = item.get(\"viewing_condition\", \"\")\n",
    "        \n",
    "        if img1 and img2 and condition:\n",
    "            # Remove .jpg extension and normalize\n",
    "            img1_clean = str(img1).replace('.jpg', '').strip().lower()\n",
    "            img2_clean = str(img2).replace('.jpg', '').strip().lower()\n",
    "            condition_clean = str(condition).strip().lower()\n",
    "            \n",
    "            if condition_clean in [\"full\", \"central\", \"peripheral\"]:\n",
    "                # Create sorted pair (for unordered comparison)\n",
    "                pair = tuple(sorted([img1_clean, img2_clean]))\n",
    "                \n",
    "                all_pairs.append({\n",
    "                    \"img1\": pair[0],\n",
    "                    \"img2\": pair[1],\n",
    "                    \"condition\": condition_clean,\n",
    "                    \"img1_original\": img1,\n",
    "                    \"img2_original\": img2\n",
    "                })\n",
    "    \n",
    "    if not all_pairs:\n",
    "        print(\"âŒ No valid pairs found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_pairs)\n",
    "    \n",
    "    print(f\"Valid pairs extracted: {len(df)}\")\n",
    "    print(f\"Pairs by condition:\")\n",
    "    for cond in [\"full\", \"central\", \"peripheral\"]:\n",
    "        count = len(df[df['condition'] == cond])\n",
    "        print(f\"  {cond}: {count}\")\n",
    "    \n",
    "    # Find duplicates - group by pair and see which appear in multiple conditions\n",
    "    pair_analysis = df.groupby(['img1', 'img2']).agg({\n",
    "        'condition': lambda x: sorted(list(x)),\n",
    "        'img1_original': 'first',\n",
    "        'img2_original': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    pair_analysis['n_conditions'] = pair_analysis['condition'].apply(len)\n",
    "    pair_analysis['conditions_str'] = pair_analysis['condition'].apply(lambda x: ', '.join(x))\n",
    "    \n",
    "    # Add flags for each condition\n",
    "    pair_analysis['in_full'] = pair_analysis['condition'].apply(lambda x: 'full' in x).astype(int)\n",
    "    pair_analysis['in_central'] = pair_analysis['condition'].apply(lambda x: 'central' in x).astype(int)\n",
    "    pair_analysis['in_peripheral'] = pair_analysis['condition'].apply(lambda x: 'peripheral' in x).astype(int)\n",
    "    \n",
    "    # Sort by number of conditions\n",
    "    pair_analysis = pair_analysis.sort_values(['n_conditions', 'img1'], ascending=[False, True])\n",
    "    \n",
    "    # Get unique pairs per condition\n",
    "    unique_per_condition = {}\n",
    "    for cond in [\"full\", \"central\", \"peripheral\"]:\n",
    "        cond_pairs = set(df[df['condition'] == cond][['img1', 'img2']].apply(tuple, axis=1))\n",
    "        unique_per_condition[cond] = cond_pairs\n",
    "    \n",
    "    # Find overlaps\n",
    "    full_central = unique_per_condition['full'] & unique_per_condition['central']\n",
    "    full_peripheral = unique_per_condition['full'] & unique_per_condition['peripheral']\n",
    "    central_peripheral = unique_per_condition['central'] & unique_per_condition['peripheral']\n",
    "    all_three = unique_per_condition['full'] & unique_per_condition['central'] & unique_per_condition['peripheral']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š DUPLICATE ANALYSIS RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTotal unique pairs: {len(pair_analysis)}\")\n",
    "    print(f\"Pairs in 1 condition only: {sum(pair_analysis['n_conditions'] == 1)}\")\n",
    "    print(f\"Pairs in 2 conditions: {sum(pair_analysis['n_conditions'] == 2)}\")\n",
    "    print(f\"Pairs in all 3 conditions: {sum(pair_analysis['n_conditions'] == 3)}\")\n",
    "    \n",
    "    print(f\"\\nOverlaps between condition pairs:\")\n",
    "    print(f\"  Full âˆ© Central: {len(full_central)}\")\n",
    "    print(f\"  Full âˆ© Peripheral: {len(full_peripheral)}\")\n",
    "    print(f\"  Central âˆ© Peripheral: {len(central_peripheral)}\")\n",
    "    print(f\"  All three: {len(all_three)}\")\n",
    "    \n",
    "    # Show duplicates\n",
    "    duplicates = pair_analysis[pair_analysis['n_conditions'] > 1]\n",
    "    \n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"\\nðŸ” DUPLICATE PAIRS (in multiple conditions): {len(duplicates)} pairs\")\n",
    "        \n",
    "        # Pairs in all 3\n",
    "        in_all_3 = duplicates[duplicates['n_conditions'] == 3]\n",
    "        if len(in_all_3) > 0:\n",
    "            print(f\"\\nâœ¨ Pairs in ALL 3 conditions: {len(in_all_3)}\")\n",
    "            for i, (_, row) in enumerate(in_all_3.head(5).iterrows(), 1):\n",
    "                print(f\"\\n  {i}. Pair:\")\n",
    "                print(f\"     Image 1: {row['img1_original']}\")\n",
    "                print(f\"     Image 2: {row['img2_original']}\")\n",
    "        \n",
    "        # Pairs in exactly 2\n",
    "        in_2 = duplicates[duplicates['n_conditions'] == 2]\n",
    "        if len(in_2) > 0:\n",
    "            print(f\"\\nðŸ“ Pairs in exactly 2 conditions: {len(in_2)}\")\n",
    "            \n",
    "            # Break down by which 2 conditions\n",
    "            fc = in_2[(in_2['in_full'] == 1) & (in_2['in_central'] == 1) & (in_2['in_peripheral'] == 0)]\n",
    "            fp = in_2[(in_2['in_full'] == 1) & (in_2['in_peripheral'] == 1) & (in_2['in_central'] == 0)]\n",
    "            cp = in_2[(in_2['in_central'] == 1) & (in_2['in_peripheral'] == 1) & (in_2['in_full'] == 0)]\n",
    "            \n",
    "            print(f\"  Full + Central: {len(fc)}\")\n",
    "            print(f\"  Full + Peripheral: {len(fp)}\")\n",
    "            print(f\"  Central + Peripheral: {len(cp)}\")\n",
    "            \n",
    "            if len(in_2) > 0:\n",
    "                print(f\"\\n  Examples:\")\n",
    "                for i, (_, row) in enumerate(in_2.head(3).iterrows(), 1):\n",
    "                    print(f\"  {i}. {row['conditions_str']}:\")\n",
    "                    print(f\"     {row['img1_original']}\")\n",
    "                    print(f\"     {row['img2_original']}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… No duplicate pairs found across conditions!\")\n",
    "        print(\"Each image pair appears in only one viewing condition.\")\n",
    "    \n",
    "    # Save results\n",
    "    try:\n",
    "        output_dir = Path(\"pair_analysis_results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save all pairs\n",
    "        pair_analysis.to_csv(output_dir / \"all_pairs_analysis.csv\", index=False)\n",
    "        \n",
    "        # Save duplicates if any\n",
    "        if len(duplicates) > 0:\n",
    "            duplicates.to_csv(output_dir / \"duplicate_pairs.csv\", index=False)\n",
    "            print(f\"\\nðŸ’¾ Results saved to {output_dir}/\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open(output_dir / \"summary.txt\", \"w\") as f:\n",
    "            f.write(f\"Total unique pairs: {len(pair_analysis)}\\n\")\n",
    "            f.write(f\"Pairs in 1 condition: {sum(pair_analysis['n_conditions'] == 1)}\\n\")\n",
    "            f.write(f\"Pairs in 2 conditions: {sum(pair_analysis['n_conditions'] == 2)}\\n\")\n",
    "            f.write(f\"Pairs in 3 conditions: {sum(pair_analysis['n_conditions'] == 3)}\\n\")\n",
    "            f.write(f\"\\nOverlaps:\\n\")\n",
    "            f.write(f\"Full âˆ© Central: {len(full_central)}\\n\")\n",
    "            f.write(f\"Full âˆ© Peripheral: {len(full_peripheral)}\\n\")\n",
    "            f.write(f\"Central âˆ© Peripheral: {len(central_peripheral)}\\n\")\n",
    "            f.write(f\"All three: {len(all_three)}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not save files: {e}\")\n",
    "    \n",
    "    return pair_analysis\n",
    "\n",
    "# Run the analysis\n",
    "df = find_duplicate_pairs(\".\")\n",
    "\n",
    "# Show the dataframe\n",
    "if not df.empty:\n",
    "    print(\"\\nðŸ“‹ First 10 pairs:\")\n",
    "    display(df[['img1_original', 'img2_original', 'conditions_str', 'n_conditions']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdgaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
